{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### From RNN to Neural Machine Translation (NMT)\n",
    "<br><br>\n",
    "Prof. Iacopo Masi and Prof. Stefano Faralli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.colheader_justify', 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "#plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "font = {'family' : 'Times',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "\n",
    "# Aux functions\n",
    "\n",
    "def plot_grid(Xs, Ys, axs=None):\n",
    "    ''' Aux function to plot a grid'''\n",
    "    t = np.arange(Xs.size) # define progression of int for indexing colormap\n",
    "    if axs:\n",
    "        axs.plot(0, 0, marker='*', color='r', linestyle='none') #plot origin\n",
    "        axs.scatter(Xs,Ys, c=t, cmap='jet', marker='.') # scatter x vs y\n",
    "        axs.axis('scaled') # axis scaled\n",
    "    else:\n",
    "        plt.plot(0, 0, marker='*', color='r', linestyle='none') #plot origin\n",
    "        plt.scatter(Xs,Ys, c=t, cmap='jet', marker='.') # scatter x vs y\n",
    "        plt.axis('scaled') # axis scaled\n",
    "        \n",
    "def linear_map(A, Xs, Ys):\n",
    "    '''Map src points with A'''\n",
    "    # [NxN,NxN] -> NxNx2 # add 3-rd axis, like adding another layer\n",
    "    src = np.stack((Xs,Ys), axis=Xs.ndim)\n",
    "    # flatten first two dimension\n",
    "    # (NN)x2\n",
    "    src_r = src.reshape(-1,src.shape[-1]) #ask reshape to keep last dimension and adjust the rest\n",
    "    # 2x2 @ 2x(NN)\n",
    "    dst = A @ src_r.T # 2xNN\n",
    "    #(NN)x2 and then reshape as NxNx2\n",
    "    dst = (dst.T).reshape(src.shape)\n",
    "    # Access X and Y\n",
    "    return dst[...,0], dst[...,1]\n",
    "\n",
    "\n",
    "def plot_points(ax, Xs, Ys, col='red', unit=None, linestyle='solid'):\n",
    "    '''Plots points'''\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, which='both')\n",
    "    ax.axhline(y=0, color='gray', linestyle=\"--\")\n",
    "    ax.axvline(x=0, color='gray',  linestyle=\"--\")\n",
    "    ax.plot(Xs, Ys, color=col)\n",
    "    if unit is None:\n",
    "        plotVectors(ax, [[0,1],[1,0]], ['gray']*2, alpha=1, linestyle=linestyle)\n",
    "    else:\n",
    "        plotVectors(ax, unit, [col]*2, alpha=1, linestyle=linestyle)\n",
    "\n",
    "def plotVectors(ax, vecs, cols, alpha=1, linestyle='solid'):\n",
    "    '''Plot set of vectors.'''\n",
    "    for i in range(len(vecs)):\n",
    "        x = np.concatenate([[0,0], vecs[i]])\n",
    "        ax.quiver([x[0]],\n",
    "                   [x[1]],\n",
    "                   [x[2]],\n",
    "                   [x[3]],\n",
    "                   angles='xy', scale_units='xy', scale=1, color=cols[i],\n",
    "                   alpha=alpha, linestyle=linestyle, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## My own latex definitions\n",
    "\n",
    "$$\\def\\mbf#1{\\mathbf{#1}}$$\n",
    "$$\\def\\bmf#1{\\boldsymbol{#1}}$$\n",
    "$$\\def\\bx{\\mbf{x}}$$\n",
    "$$\\def\\bxt#1{\\mbf{x}_{\\text{#1}}}$$\n",
    "$$\\def\\bv{\\mbf{v}}$$\n",
    "$$\\def\\bz{\\mbf{z}}$$\n",
    "$$\\def\\bmu{\\bmf{\\mu}}$$\n",
    "$$\\def\\bsigma{\\bmf{\\Sigma}}$$\n",
    "$$\\def\\Rd#1{\\in \\mathbb{R}^{#1}}$$\n",
    "$$\\def\\chain#1#2{\\frac{\\partial #1}{\\partial #2}}$$\n",
    "$$\\def\\loss{\\mathcal{L}}$$\n",
    "$$\\def\\params{\\bmf{\\theta}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's lecture\n",
    "## - Recap on RNN\n",
    "### - Stacked, Bidirectional RNN\n",
    "### - Long short-Term Memory Networks (LSTM)\n",
    "### - Introduction to Self-Attention and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This lecture material is taken from\n",
    "üìò **Chapter 9 Jurafsky Book**\n",
    "\n",
    "üìò **Chapter 6.3 Eisenstein Book**\n",
    "- [Stanford Slide LSTM](http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture06-fancy-rnn.pdf)\n",
    "- [Stanford Lecture LSTM](https://www.youtube.com/watch?v=0LixFSa7yts&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&index=6)\n",
    "- [Stanford Notes on RNN and LSTM](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf)\n",
    "- [Andrej Karpathy Lecture on LSTM](https://www.youtube.com/watch?v=yCC09vCHzF8)\n",
    "- [Andrej Karpathy Slides on LSTM](http://cs231n.stanford.edu/slides/2022/lecture_10_ruohan.pdf)\n",
    "\n",
    "Another resource with code is [[d2l.ai] Modern RNN](https://d2l.ai/chapter_recurrent-modern/index.html)\n",
    "\n",
    "LSTM: [colah.github.io](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) | \n",
    "[Illustrated Guide to LSTM](https://www.youtube.com/watch?v=8HyCNIVRbSU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning for Sequence Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stacked Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN so far\n",
    "\n",
    "<br><div align='center'><img src=\"figs/stacked_rnn_01.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Stacked RNN (or Multi-layer RNN)\n",
    "\n",
    "- RNNs are already \"deep\" on one dimension, **the time dimension**---they unroll over many timesteps.\n",
    "\n",
    "- We can also make them ‚Äúdeep‚Äù in **another dimension** (the representation dimension)! We can do so by applying multiple RNNs ‚Äì this is a multi-layer RNN (stacked RNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This allows the network to compute more complex representations \n",
    "\n",
    "- The lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Stacked RNN (or Multi-layer RNN)\n",
    "\n",
    "The hidden states from RNN layer $i$ are the inputs to RNN layer $i+1$\n",
    "\n",
    "<br><div align='center'><img src=\"figs/stacked_rnn_02.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Stacked RNN (or Multi-layer RNN)\n",
    "\n",
    "\n",
    "- **Multi-layer or stacked RNNs** allow a network to compute **more complex representations**: they work better than just have one layer of high-dimensional encodings!\n",
    "\n",
    "- High-performing RNNs are usually multi-layer BUT NOT as deep as convolutional or feed-forward networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In a 2017 paper, Britz et al. find that for **Neural Machine Translation**, 2 to 4 layers is best for the encoder RNN, and 4 layers is best for the decoder RNN:\n",
    "    - Often 2 layers is a lot better than 1, and 3 might be a little better than 2\n",
    "    -  Usually, skip-connections/dense-connections are needed to train deeper RNNs (e.g., 8 layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bidirectional RNN: Motivation\n",
    "\n",
    "**Task:** Sentiment Classification\n",
    "\n",
    "We can regard the hidden state $\\mbf{h}_4$ of `terribly` as a representation of the word given prior context this sentence. We call this a _contextual representation_.\n",
    "Note that the sentence denotes `positivity` but `terribly` is usually used as negative word.\n",
    "\n",
    "<div align='center'><img src=\"figs/bidirectional_01.png\" width='25%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bidirectional RNN: Motivation\n",
    "\n",
    "**Task:** Sentiment Classification\n",
    "\n",
    "The pooling of all hidden states may cancel then contribution of the hidden state of `exciting` making the model less effective.\n",
    "\n",
    "**It would be nice if we could condition terribly on what comes \"in the future\" (right)**\n",
    "<div align='center'><img src=\"figs/bidirectional_01.png\" width='25%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bidirectional RNN: Motivation\n",
    "\n",
    "**Task:** Sentiment Classification\n",
    "\n",
    "**Depending on the task, we have the text at our hand so we can go back at the end of text, we can go right etc.**\n",
    "\n",
    "**Bottom line: We can move in the text as we want.**\n",
    "\n",
    "<div align='center'><img src=\"figs/bidirectional_01.png\" width='20%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bidirectional RNN: Motivation\n",
    "\n",
    "**Task:** Sentiment Classification\n",
    "\n",
    "**Idea:** we learn the RNN in reverse order, but what do we do with the previous RNN? ü§î\n",
    "\n",
    "<div align='center'><img src=\"figs/bidirectional_02.png\" width='25%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bidirectional RNN\n",
    "\n",
    "**Task:** Sentiment Classification\n",
    "\n",
    "**Idea:** We use both of the hidden layers.\n",
    "1. A set of hidden layers goes from left $\\rightarrow$ right\n",
    "2. Another set of hidden layers goes left $\\leftarrow$ right (new)\n",
    "3. **Fusion:** We somehow \"pool\" the final representation (usually `concat`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bidirectional RNN\n",
    "<br><div align='center'><img src=\"figs/bidirectional_03.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bidirectional RNN\n",
    "\n",
    "In general for each side of the RNN, you have different weights.\n",
    "\n",
    "The representation is thus:\n",
    "\n",
    "$$ \\overrightarrow{\\mbf{h}}(t) = \\text{RNN}_{\\text{FW}}\\big(\\overrightarrow{\\mbf{h}}(t-1), \\mbf{x}(t)\\big)$$\n",
    "$$ \\overleftarrow{\\mbf{h}}(t) = \\text{RNN}_{\\text{BW}}\\big(\\overleftarrow{\\mbf{h}}(t+1), \\mbf{x}(t)\\big)$$\n",
    "$$ \\mbf{h}(t) = [\\overrightarrow{\\mbf{h}}(t);\\overleftarrow{\\mbf{h}}(t)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br><div align='center'><img src=\"figs/bidirectional_03.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bidirectional RNN\n",
    "\n",
    "Note: bidirectional RNNs are only applicable if you have access to the **entire input sequence**\n",
    "\n",
    "- They are NOT applicable to Language Modeling, because in LM you only have left context available.\n",
    "- If you do have entire input sequence (e.g., for any kind of encoding), **bidirectionality is powerful** _(you should use it by default!)_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, **BERT (Bidirectional Encoder Representations from Transformers)** is a powerful pretrained contextual representation system built on **bidirectionality**.\n",
    "- You will learn more about transformers, including BERT, soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From simple RNN to LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Remember Back Propagation Through Time (BPTT)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT)\n",
    "\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\underbrace{\\chain{\\loss}{\\loss}}_1\\underbrace{\\chain{\\loss}{\\loss_3}}_{1/T=1/3}\\underbrace{\\chain{\\loss_3}{\\mbf{y}_3}}_{\\mathbb{R}^{1\\times|V|}}\\underbrace{\\chain{\\mbf{y}_3}{\\mbf{h}_3}}_{\\mathbb{R}^{|V|\\times h}}\\underbrace{\\chain{\\mbf{h}_3}{\\mbf{W}_h}}_{\\mathbb{R}^{h}\\times(\\mathbb{R}^{h}\\times \\mathbb{R}^{h})}$$\n",
    " \n",
    " <div align='center'><img src=\"figs/Slide58.png\" width='50%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why LSTM? Vanishing Gradient Problem\n",
    "\n",
    "If the recursive chain is too long, the produce of matrices $\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\chain{\\mbf{h}_2}{\\mbf{h}_1}...$ can make the **gradient to vanish** especially if using tanh activation function.\n",
    "\n",
    "Think as multiply $a\\cdot b\\cdot b\\cdot b\\cdot b\\cdot b ...$ where $0<b<1$ but for matrices. \n",
    "\n",
    "**At the end the norm of the gradient will be so small that will get to zero numerically!**\n",
    "\n",
    "$$\\chain{\\mbf{h}_3}{\\mbf{W}_h}\\Big\\vert_{\\text{all time steps}}=\\chain{\\mbf{h}_3}{\\mbf{W}_h}+\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\chain{\\mbf{h}_2}{\\mbf{W}_h}+\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\chain{\\mbf{h}_2}{\\mbf{h}_1}\\chain{\\mbf{h}_1}{\\mbf{W}_h}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN to model Sequences\n",
    "\n",
    "\n",
    "‚úÖ Improvements over window-based LM \n",
    "- Can process any length input\n",
    "- Computation for step $t$ can **(in theory)** use information from many steps back\n",
    "- Bounded model size respect to context size\n",
    "- Same weights applied on every timestep, so there is symmetry in how inputs are processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# RNN to model Sequences\n",
    "\n",
    "‚ùå Remaining problems:\n",
    "- Recurrent computation is slow (unroll in time)\n",
    "- In practice, **difficult to access information from many steps back**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # RNN Gradient Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Gradient Flow\n",
    "<br><div align='center'><img src=\"figs/rnn_grad_flow/rnn_grad_flow_01.png\" width='60%' ></div>\n",
    "    \n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Gradient Flow\n",
    "<br><div align='center'><img src=\"figs/rnn_grad_flow/rnn_grad_flow_02.png\" width='60%' ></div>\n",
    "    \n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Gradient Flow\n",
    "<br><div align='center'><img src=\"figs/rnn_grad_flow/rnn_grad_flow_03.png\" width='60%' ></div>\n",
    "    \n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Gradient Flow\n",
    "<br><div align='center'><img src=\"figs/rnn_grad_flow/rnn_grad_flow_04.png\" width='60%' ></div>\n",
    "    \n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Gradient Flow\n",
    "<br><div align='center'><img src=\"figs/rnn_grad_flow/rnn_grad_flow_05.png\" width='60%' ></div>\n",
    "    \n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Gradient Flow\n",
    "<br><div align='center'><img src=\"figs/rnn_grad_flow/rnn_grad_flow_06.png\" width='60%' ></div>\n",
    "    \n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Gradient Flow\n",
    "<br><div align='center'><img src=\"figs/rnn_grad_flow/rnn_grad_flow_07.png\" width='60%' ></div>\n",
    "    \n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Gradient Flow\n",
    "<br><div align='center'><img src=\"figs/rnn_grad_flow/rnn_grad_flow_08.png\" width='60%' ></div>\n",
    "    \n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Gradient Flow\n",
    "<br><div align='center'><img src=\"figs/rnn_grad_flow/rnn_grad_flow_09.png\" width='60%' ></div>\n",
    "    \n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Gradient Flow\n",
    "<br><div align='center'><img src=\"figs/rnn_grad_flow/rnn_grad_flow_10.png\" width='60%' ></div>\n",
    "    \n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Gradient Flow\n",
    "<br><div align='center'><img src=\"figs/rnn_grad_flow/rnn_grad_flow_11.png\" width='60%' ></div>\n",
    "    \n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Gradient Flow\n",
    "<br><div align='center'><img src=\"figs/rnn_grad_flow/rnn_grad_flow_12.png\" width='60%' ></div>\n",
    "    \n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Change RNN architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# from simple RNN to Long Short-Term Memory RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Terminology\n",
    "\n",
    "<div align='center'><img src=\"figs/rnn_category.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Long Short-Term Memory RNN (LSTM)\n",
    "\n",
    "We have to modify the **Elman Unit** in something that can mitigate back-propagation vanishing gradients when the recurrence is **\"plain\"**:\n",
    "<br><br><br>\n",
    "$$\n",
    "\\mbf{h}_t=\\tanh \\Biggl(\\mbf{W}\\left(\\begin{array}{c}\n",
    "\\mbf{h}_{t-1} \\\\\n",
    "\\mbf{x}_t\n",
    "\\end{array}\\right)\\Biggr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM Madness üò±\n",
    "\n",
    "<div align='center'><img src=\"figs/lstm_madness.png\" width='60%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM Motivation\n",
    "\n",
    "- While gradient clipping helps with exploding gradients,  handling **vanishing gradients appears  to require a more elaborate solution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LSTMs: each ordinary recurrent node is replaced by a **memory cell**. Each memory cell contains an **internal state** $\\mbf{c}_t$, i.e., a node with a self-connected recurrent edge of fixed weight 1\n",
    "- This ensures that the gradient can pass across many time steps without vanishing or exploding (LONG dependency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why the name LSTM?\n",
    "\n",
    "The term \"long short-term memory\" comes from the following intuition: \n",
    "- Simple RNN have **long-term memory** in the form of weights. The weights change slowly during training,  encoding general knowledge about the data. \n",
    "- They also have **short-short term memory** in the form of ephemeral activations, which pass from each node to successive nodes. \n",
    "- The LSTM model introduces an intermediate type of storage via **the memory cell**. **A memory cell is a composite unit,  built from simpler nodes in a specific connectivity pattern, with the novel inclusion of multiplicative nodes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Best way to understand LSTM: a conveyor\n",
    "**RNN:** compute and recurr\n",
    "\n",
    "**LSTM:** compute, _let me see what I have to add/forget from the memory_, then recurr\n",
    "\n",
    "<div align='center'><img src=\"figs/rnn_lstm.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Best way to understand LSTM: a conveyor\n",
    "\n",
    "**LSTM:** have these highways (bottom lines) that forward information from the past\n",
    "\n",
    "<br><div align='center'><img src=\"figs/rnn_lstm_02.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# From RNN to LSTM Units\n",
    "\n",
    "<br><div align='center'><img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" width='60%' ></div>\n",
    "\n",
    "<small>Taken from [colah.github.io](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Breaking down LSTM\n",
    "\n",
    "The hidden states of the LSTM are now 2:\n",
    "1. the usual hidden state $\\mbf{h}_t$ like simple RNN\n",
    "2. another state  $\\mbf{c}_t$ which is called **memory cell state vector** $\\leftarrow$ this is the highway!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM Unit Input\n",
    "\n",
    "At each time step $t$, we have as input:\n",
    "- $\\mbf{x}_{t}$ (input from data)\n",
    "- $\\mbf{c}_{t-1}$ (previous memory state)\n",
    "- $\\mbf{h}_{t-1}$ (previous hidden state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# LSTM Unit Output\n",
    "\n",
    "At each time step $t$, we have to compute:\n",
    "- $\\mbf{c}_{t}$ (next memory state)\n",
    "- $\\mbf{h}_{t}$ (next hidden state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Breaking down LSTM\n",
    "\n",
    "Given some **gates** (\"controls\"), we want to compute the next state:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mbf{c}_t & = \\operatorname{function}\\Big(\\mbf{c}_{t-1}, \\operatorname{function}(\\mbf{h}_{t-1},\\mbf{x}_{t-1})  ;\\operatorname{\\mathbf{gates}}\\Big)\\\\\n",
    "\\mbf{h}_t & = \\operatorname{function}\\Big(\\mbf{c}_t;\\operatorname{\\mathbf{gates}}\\Big)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What are the \"controls\"? They are Gates!\n",
    "\n",
    "Gates (\"controls\") change the inductive bias of the architecture, allowing the network to control the information that flows.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{\\mathbf{gates}} & = \\operatorname{function}\\Big(\\mbf{h}_{t-1},\\mbf{x}_{t-1}\\Big)\\\\\n",
    "\\mbf{c}_t & = \\operatorname{function}\\Big(\\mbf{c}_{t-1}, \\operatorname{function}(\\mbf{h}_{t-1},\\mbf{x}_{t-1})  ;\\operatorname{\\mathbf{gates}}\\Big)\\\\\n",
    "\\mbf{h}_t & = \\operatorname{function}\\Big(\\mbf{c}_t;\\operatorname{\\mathbf{gates}}\\Big)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $\\operatorname{\\mathbf{gates}} = \\operatorname{function}\\Big(\\mbf{h}_{t-1},\\mbf{x}_{t-1}\\Big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What are the \"controls\"? They are Gates!\n",
    "\n",
    "The LSTM \"controls\" the information in the memory (as a sort of RAM memory) using various **gates**.\n",
    "Think **gates** as actions performed on the memory:\n",
    "- **F**orget about the past in the cell memory state $\\longrightarrow$ **soft binary gate**\n",
    "- **I**nput new information in the cell memory state $\\longrightarrow$ **soft binary gate**\n",
    "- **O**utput information to retain in the next hidden state $\\longrightarrow$ **soft binary gate**\n",
    "\n",
    "Given that we are in üáÆüáπ, to recall just remember **FIO**. ü´†\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\operatorname{\\textbf{gates}} & = \\operatorname{function}\\Big(\\mbf{h}_{t-1},\\mbf{x}_{t-1}\\Big)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gates\n",
    "\n",
    "Gates end with **sigmoid** and act as a **soft binary mask** to control how much information should flow.\n",
    "\n",
    "Gates are function of $\\Big(\\mbf{h}_{t-1},\\mbf{x}_{t-1}\\Big)$\n",
    "<div align='center'><img src=\"figs/lstm_switch.png?1\" width='20%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gates\n",
    "\n",
    "Gates end with sigmoid and act as a **soft binary mask** to control how much information should flow.\n",
    "\n",
    "Gates are function of $\\Big(\\mbf{h}_{t-1},\\mbf{x}_{t-1}\\Big)$\n",
    "<div align='center'><img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png\" width='10%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gates\n",
    "\n",
    "A few details and terminology:\n",
    "- $\\sigma$ is the sigmoid function---the one you find also in logistic regression LR). Remember that $\\sigma$ gives you an output in $[0...1]$.\n",
    "    - you can think $\\sigma$ as a probability as in LR or, as in here, as **soft-masking bounded [0,1]**\n",
    "    - $\\sigma$ tells you how much information you want to make flow.\n",
    "- $\\odot$ is the **Hadamard product** which is element-wise multiplication of tensors.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left(\\begin{array}{l}\n",
    "i \\\\\n",
    "f \\\\\n",
    "o \n",
    "\\end{array}\\right) & =\\left(\\begin{array}{c}\n",
    "\\sigma \\\\\n",
    "\\sigma \\\\\n",
    "\\sigma \\\\\n",
    "\\end{array}\\right) \\mbf{W}\\left(\\begin{array}{c}\n",
    "\\mbf{h}_{t-1} \\\\\n",
    "\\mbf{x}_t\n",
    "\\end{array}\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gates\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left(\\begin{array}{l}\n",
    "\\text{input} \\\\\n",
    "\\text{forget} \\\\\n",
    "\\text{output} \n",
    "\\end{array}\\right) & =\\left(\\begin{array}{c}\n",
    "\\sigma \\\\\n",
    "\\sigma \\\\\n",
    "\\sigma \\\\\n",
    "\\end{array}\\right) \\mbf{W}\\left(\\begin{array}{c}\n",
    "\\mbf{h}_{t-1} \\\\\n",
    "\\mbf{x}_t\n",
    "\\end{array}\\right) + \\mbf{b} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forget Gate\n",
    "\n",
    "<div align='center'><img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gates: alternative equation still same concept\n",
    "\n",
    "Sometimes you can find it written as below but is more complex. In our case $\\mbf{W}=[\\mathbf{W}_{xi};\\mathbf{W}_{hi};\\mathbf{W}_{xf};\\mathbf{W}_{hf};\\mathbf{W}_{xo};\\mathbf{W}_{ho};]$ concat of matrices, same for biases.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\\n",
    "\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f),\\\\\n",
    "\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o),\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Candidate update for $\\mbf{c}_t$: $\\tilde{\\mbf{c}}_t$\n",
    "\n",
    "We can compute it in the similar way we do for **gates** but still with its own parameters\n",
    "\n",
    "$\\mbf{c}_t = \\operatorname{function}\\Big(\\mbf{c}_{t-1}, \\underbrace{\\operatorname{function}(\\mbf{h}_{t-1},\\mbf{x}_{t-1})}_{\\text{candidate update}~~\\tilde{\\mbf{c}}_t}; \\underbrace{\\operatorname{\\mathbf{gates}}}_{\\text{you know this!}}\\Big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gates + Candidate update $\\tilde{\\mbf{c}}_t$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left(\\begin{array}{l}\n",
    "\\text{input} \\\\\n",
    "\\text{forget} \\\\\n",
    "\\text{output} \\\\\n",
    "\\tilde{\\mbf{c}}_t\n",
    "\\end{array}\\right) & =\\left(\\begin{array}{c}\n",
    "\\sigma \\\\\n",
    "\\sigma \\\\\n",
    "\\sigma \\\\\n",
    "\\tanh\\\\\n",
    "\\end{array}\\right) \\mbf{W}\\left(\\begin{array}{c}\n",
    "\\mbf{h}_{t-1} \\\\\n",
    "\\mbf{x}_t\n",
    "\\end{array}\\right) + \\mbf{b} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Update the memory cell state: forget or add input\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Update the memory cell state: forget or add input\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Parametric model for LSTM Unit\n",
    "\n",
    "The parametric model of an LSTM comprises of a matrix $\\mbf{W}$ of dimensionality $4D \\times 2D$ where $D$ is the dimension of the hidden state $\\mbf{h}$.\n",
    "\n",
    "- $2D$ because we take as input both $\\mbf{h}_{t-1}$ and $\\mbf{x}_{t}$\n",
    "- $4D$ because we output 4 vector of dimensionality $D$: forget, input, output and candidate update $\\tilde{\\mbf{c}}_t$\n",
    "\n",
    "The bias is optional and is $4D$. \n",
    "\n",
    "\n",
    "This assume dim($\\mbf{h})$ = dim($\\mbf{x}$) but they could differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $\\mbf{c}_t = \\operatorname{function}\\Big(\\mbf{c}_{t-1}, \\operatorname{function}(\\mbf{h}_{t-1},\\mbf{x}_{t-1})  ;\\operatorname{\\mathbf{controls}}\\Big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Update the memory cell state\n",
    "\n",
    "\n",
    "$$ \\mbf{c}_t = f \\odot \\mbf{c}_{t-1}+i \\odot \\tilde{\\mbf{c}}_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Update the memory cell state\n",
    "\n",
    "\n",
    "$$ \\mbf{c}_t = \\underbrace{f \\odot \\mbf{c}_{t-1}}_{\\text{forget the past}}+\\underbrace{i \\odot \\tilde{\\mbf{c}}_t}_{\\text{input new information}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Update the memory cell state: forget or add input\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Update the hidden state\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mbf{c}_t & =f \\odot \\mbf{c}_{t-1}+i \\odot \\tilde{\\mbf{c}}_t \\\\\n",
    "\\mbf{h}_t & =o \\odot \\tanh \\left(\\mbf{c}_t\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Update the hidden state\n",
    "\n",
    "<div align='center'><img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Putting all together\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left(\\begin{array}{l}\n",
    "i \\\\\n",
    "f \\\\\n",
    "o \\\\\n",
    "\\tilde{\\mbf{c}}_t\n",
    "\\end{array}\\right) & =\\left(\\begin{array}{c}\n",
    "\\sigma \\\\\n",
    "\\sigma \\\\\n",
    "\\sigma \\\\\n",
    "\\tanh\n",
    "\\end{array}\\right) \\mbf{W}\\left(\\begin{array}{c}\n",
    "\\mbf{h}_{t-1} \\\\\n",
    "\\mbf{x}_t\n",
    "\\end{array}\\right) + \\mbf{b} \\\\\n",
    "\\mbf{c}_t & =f \\odot \\mbf{c}_{t-1}+i \\odot \\tilde{\\mbf{c}}_t \\\\\n",
    "\\mbf{h}_t & =o \\odot \\tanh \\left(\\mbf{c}_t\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Putting all together\n",
    "\n",
    "<div align='center'><img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Simple RNN vs LSTM\n",
    "\n",
    "\\begin{align}\n",
    "\\mbf{h}_t=\\tanh \\Biggl(\\mbf{W}\\left(\\begin{array}{c}\n",
    "\\mbf{h}_{t-1} \\\\\n",
    "\\mbf{x}_t\n",
    "\\end{array}\\right)\\Biggr)\n",
    "&& \\qquad \n",
    "\\begin{aligned}\n",
    "\\left(\\begin{array}{l}\n",
    "i \\\\\n",
    "f \\\\\n",
    "o \\\\\n",
    "\\tilde{\\mbf{c}}\n",
    "\\end{array}\\right) & =\\left(\\begin{array}{c}\n",
    "\\sigma \\\\\n",
    "\\sigma \\\\\n",
    "\\sigma \\\\\n",
    "\\tanh\n",
    "\\end{array}\\right) \\mbf{W}\\left(\\begin{array}{c}\n",
    "\\mbf{h}_{t-1} \\\\\n",
    "\\mbf{x}_t\n",
    "\\end{array}\\right) + \\mbf{b}\\\\\n",
    "\\mbf{c}_t & =f \\odot \\mbf{c}_{t-1}+i \\odot \\tilde{\\mbf{c}} \\\\\n",
    "\\mbf{h}_t & =o \\odot \\tanh \\left(\\mbf{c}_t\\right)\n",
    "\\end{aligned}\n",
    "\\end{align}\n",
    "\n",
    "<small>Hochreiter and Schmidhuber, ‚ÄúLong Short Term Memory‚Äù, Neural Computation 1997<br>   \n",
    "Gers, Schmidhuber, and Cummins, 2000. Learning to Forget: Continual Prediction with LSTM</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN vs LSTM Gradients\n",
    "\n",
    "\n",
    "```python\n",
    "H = 5  # dim of hidden staate\n",
    "T = 50 # steps of unrolling the RNN\n",
    "Whh = np.random.randn(H, H)\n",
    "hs, ss = {}, {}\n",
    "hs[-1] = np.random.randn(H)\n",
    "# forward pass of the RNN ignoring input\n",
    "for t in range(T):\n",
    "    ss[t] = np.dot(Whh, hs[t-1])\n",
    "    hs[t] = np.maximum(0,ss[t])\n",
    "# backward pass of the RNN\n",
    "dhs, dss = {}, {}\n",
    "dhs[T-1] = np.random.randn(H) # we inject random gradients (similar to having a loss)\n",
    "for t in reversed(range(T)):\n",
    "    dss[t]=(hs[t-1] > 0)*dhs[t]  #backprop through the non-linearity\n",
    "    dhs[t-1] = np.dot(Whh.T,dss[t]) #backprop into previous hidden state\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "<small>Code by A. Karpathy</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN vs LSTM Gradients\n",
    "\n",
    "\n",
    "```python \n",
    "dhs[0]= np.dot(Whh.T, dss[1]) # where dss[1]=(hs[0] > 0)*dhs[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python \n",
    "dhs[0]= np.dot(Whh.T, (hs[0] > 0)*dhs[1]) # where dhs[1]=np.dot(Whh.T, (hs[1] > 0)*dhs[2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python \n",
    "dhs[0]= np.dot(Whh.T, (hs[0] > 0)*np.dot(Whh.T, (hs[1] > 0)*dhs[2])) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN vs LSTM Gradients\n",
    "\n",
    "RNN vs LSTM gradients on the input weight matrix\n",
    "\n",
    "**Error is generated at 128th step and propagated back. No error from other steps.**\n",
    "At the beginning of training. Weights sampled from Normal Distribution in (-0.1, 0.1). \n",
    "\n",
    "<small>[Taken from http://imgur.com/gallery/vaNahKE](http://imgur.com/gallery/vaNahKE)</small>\n",
    "\n",
    "<div align='center'><video controls loop src=\"figs/RNN-vs-LSTM-gradients.mp4\"/></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM Variants\n",
    "\n",
    "<div align='center'><img src=\"figs/rnn_variants.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM and Neural Architectural Search\n",
    "\n",
    "<div align='center'><img src=\"figs/rnn_nas.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM Grad Flow\n",
    "\n",
    "<div align='center'><img src=\"figs/rnn_lstm_grad_flow.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Does this configuration recall you of another famous architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM vs ResNet\n",
    "\n",
    "<br><div align='center'><img src=\"figs/lstm_resnet.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How does LSTM solve vanishing gradients?\n",
    "\n",
    "- The LSTM architecture makes it **much easier** for an RNN to **preserve information over many timesteps**\n",
    "    - e.g., if the **forget gate is set to 1** for a cell dimension and **the input gate\n",
    "    set to 0**, then the **information of that cell is preserved indefinitely**.\n",
    "    - In contrast, it‚Äôs harder for a vanilla RNN to learn a recurrent weight\n",
    "    matrix Wh that preserves info in the hidden state\n",
    "    - In practice, you get about <u>**100 timesteps rather than about 7**</u>\n",
    "- LSTMs do not guarantee that there is no vanishing/exploding gradients but they do provide an easier way for the model to learn long-distance dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM: Real-world success\n",
    "\n",
    "- In 2013‚Äì2015, LSTMs started achieving state-of-the-art results\n",
    "    - Successful tasks include **handwriting recognition, speech recognition, machine\n",
    "translation, parsing, and image captioning**, as well as language models\n",
    "    - LSTMs became the dominant approach for most NLP tasks\n",
    "    \n",
    "<small>Source: \"Findings of the 2016 Conference on Machine Translation (WMT16)\", Bojar et al. 2016, http://www.statmt.org/wmt16/pdf/W16-2301.pdf</small>\n",
    "\n",
    "<small>Source: \"Findings of the 2018 Conference on Machine Translation (WMT18)\", Bojar et al. 2018, http://www.statmt.org/wmt18/pdf/WMT028.pdf</small>\n",
    "\n",
    "<small>Source: \"Findings of the 2019 Conference on Machine Translation (WMT19)\", Barrault et al. 2019, http://www.statmt.org/wmt18/pdf/WMT028.pdf</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Situation now\n",
    "- Now (2019‚Äì2023), **Transformers** have become dominant for all tasks\n",
    "    - For example, in WMT (a Machine Translation conference + competition):\n",
    "    - In WMT **2014, there were 0 neural machine translation systems (!)**\n",
    "    - In WMT **2016**, the summary report contains **‚ÄúRNN‚Äù 44 times** (and these systems won)\n",
    "    - In WMT **2019** **‚ÄúRNN‚Äù 7 times**, **‚ÄùTransformer‚Äù 105 times**\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practical Application with LSTM\n",
    "\n",
    "<small>Tutorial taken from https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Implementing LSTM from scratch with Pytorch\n",
    "\n",
    "```python\n",
    "class LSTMScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n",
    "        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n",
    "        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n",
    "        self.W_xc, self.W_hc, self.b_c = triple()  # Input node\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Implementing LSTM from scratch with Pytorch\n",
    "\n",
    "```python\n",
    "def forward(self, inputs, H_C=None):\n",
    "    # inputs is #seq_size x dimension\n",
    "    if H_C is None:\n",
    "        # Initial state with shape: (batch_size, num_hiddens)\n",
    "        H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                      device=inputs.device)\n",
    "        C = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                      device=inputs.device)\n",
    "    else:\n",
    "        H, C = H_C\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = torch.sigmoid(torch.matmul(X, self.W_xi) +\n",
    "                        torch.matmul(H, self.W_hi) + self.b_i)\n",
    "        F = torch.sigmoid(torch.matmul(X, self.W_xf) +\n",
    "                        torch.matmul(H, self.W_hf) + self.b_f)\n",
    "        O = torch.sigmoid(torch.matmul(X, self.W_xo) +\n",
    "                        torch.matmul(H, self.W_ho) + self.b_o)\n",
    "        C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +\n",
    "                           torch.matmul(H, self.W_hc) + self.b_c)\n",
    "        C = F * C + I * C_tilde\n",
    "        H = O * torch.tanh(C)\n",
    "        outputs.append(H)\n",
    "    return outputs, (H, C)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Implementing LSTM using Pytorch\n",
    "\n",
    "- As previously, the hyperparameter `num_hiddens` dictates the number of hidden units. \n",
    "\n",
    "- We initialize weights following a Gaussian distribution with 0.01 standard deviation, and we set the biases to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# [Torch  Text](https://pytorch.org/text/stable/index.html)\n",
    "\n",
    "<br><div align='center'><img src=\"figs/torchtext.png\" width='80%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [LSTM Pytorch API](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#lstm)\n",
    "\n",
    "<div align='center'><img src=\"figs/lstm.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [LSTM Pytorch API](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#lstm)\n",
    "\n",
    "<div align='center'><img src=\"figs/lstm_02.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [LSTM Pytorch API](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#lstm)\n",
    "\n",
    "<div align='center'><img src=\"figs/lstm_01.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [LSTM Pytorch API](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#lstm)\n",
    "\n",
    "<div align='center'><img src=\"figs/lstm_params.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f89f142b390>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# we define an LSTM model with\n",
    "# input_size a 10D vector\n",
    "# hidden_size a 20D vector\n",
    "# num_layers means 2 stacked LSTM\n",
    "lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# we define an LSTM model with\n",
    "# input_size a 3D vector\n",
    "# hidden_size a 3D vector\n",
    "# num_layers means 2 stacked LSTM\n",
    "input_size, hidden_size, batch_size, = 3, 2, 1\n",
    "lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let us make up a single training sequence\n",
    "\n",
    "### This is very useful in deep learning to see if your model digests input correctly\n",
    "### Very common to test the network on random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "inputs = [torch.randn(1, input_size) for _ in range(5)]  # make a sequence of length 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Now we will do \"forward pass\" manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, batch_size, hidden_size),\n",
    "          torch.randn(1, batch_size, hidden_size))\n",
    "for i in inputs: # loop over sequences\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden) # i becomes 1x1x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# We can also let Pytorch handle everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Batch\n",
    "\n",
    "Remember that in practice the input is batched. As input to the RNN you feed a 3D tensor $\\mbf{X}$ of size: \n",
    "\n",
    "$$ \\mbf{X} = (\\text{seq_len, batch, feature}) $$\n",
    "\n",
    "where:\n",
    "- `seq_len` is the axis to index the sequence\n",
    "- `batch` is the axis to index which sequence in the batch\n",
    "- `feature` is the axis to index the feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Batch\n",
    "\n",
    "\n",
    "\n",
    "$$ \\mbf{X} = (\\text{seq_len, batch, feature}) $$\n",
    "\n",
    "$\\mbf{X}[0][0][:]$ means the feature of the **first element** in the **first sequence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can do the entire sequence all at once.\n",
    "1. first value `out` returned by LSTM is all of the hidden states throughout the sequence.\n",
    "2. the second `hidden` is just the most recent hidden state\n",
    "\n",
    "Let's make the training data a batch now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "\n",
    "# Add the extra 2nd dimension\n",
    "inputs_batch = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "# torch.cat(inputs) make 5 item lits of 3D vector a 5x3 tensor\n",
    "# .view(len(inputs), 1, -1) makes the tensor 5x3 a 5x1x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "inputs_batch = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "torch.cat(inputs) # make 5 item lits of 3D vector a 5x3 tensor\n",
    ".view(len(inputs), 1, -1) #makes the tensor 5x3 a 5x1x3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Note we still have a single seq in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Now we forward in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM Units\n",
    "\n",
    "<br><div align='center'><img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" width='60%' ></div>\n",
    "\n",
    "<small>Taken from [colah.github.io](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hidden = (torch.randn(1, batch_size, hidden_size),\n",
    "          torch.randn(1, batch_size, hidden_size))  #clean out hidden state\n",
    "out, hidden = lstm(inputs_batch, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1099, -0.1292]],\n",
      "\n",
      "        [[ 0.0916, -0.1633]],\n",
      "\n",
      "        [[ 0.2786, -0.2308]],\n",
      "\n",
      "        [[ 0.1764, -0.1675]],\n",
      "\n",
      "        [[ 0.0410, -0.1014]]], grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "(tensor([[[ 0.0410, -0.1014]]], grad_fn=<StackBackward0>), tensor([[[ 0.0480, -0.5339]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "variables": {
     "type(out), type(hidden)": "<p><strong>NameError</strong>: name &#39;out&#39; is not defined</p>\n"
    }
   },
   "source": [
    "# All hidden states and last hidden state\n",
    "```python\n",
    "type(out), type(hidden)\n",
    "```\n",
    "{{type(out), type(hidden)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "variables": {
     "out.shape": "<p><strong>NameError</strong>: name &#39;out&#39; is not defined</p>\n"
    }
   },
   "source": [
    "# All hidden states `out` shape\n",
    "\n",
    "`out.shape`\n",
    "\n",
    "{{out.shape}}\n",
    "\n",
    "5 elements in the sequence, only 1 sequence, of dimensionality 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "variables": {
     "out[-1].detach().numpy(), hidden[0].detach().numpy() ": "<p><strong>NameError</strong>: name &#39;out&#39; is not defined</p>\n"
    }
   },
   "source": [
    "# `hidden` contains the hidden state $\\mbf{h}$ and the cell state $\\mbf{c}$\n",
    "```python\n",
    "out[-1], hidden[0] # are the same thing\n",
    "```\n",
    "{{out[-1].detach().numpy(), hidden[0].detach().numpy() }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let us now batchify the data\n",
    "### Batch size is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "inputs_batch = torch.randn(5, batch_size, 3)\n",
    "hidden = (torch.randn(1, batch_size, hidden_size), \n",
    "          torch.randn(1, batch_size, hidden_size))  # clean out hidden state\n",
    "out, (H, C) = lstm(inputs_batch, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Question: what is the size now of `out`, `H` and `C`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 2]), torch.Size([1, 10, 2]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape, C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# POS tagging with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this section, we will use an LSTM to get part of speech tags. We will\n",
    "not use Viterbi or Forward-Backward or anything like that.\n",
    "\n",
    "The model is as follows: let our input sentence be\n",
    "$w_1, \\dots, w_M$, where $w_i \\in V$, our vocab. \n",
    "\n",
    "Also, let\n",
    "$T$ be our tag set, and $y_i$ the tag of word $w_i$.\n",
    "Denote our prediction of the tag of word $w_i$ by\n",
    "$\\hat{y}_i$.\n",
    "\n",
    "This is a structure prediction, model, where our output is a sequence\n",
    "$\\hat{y}_1, \\dots, \\hat{y}_M$, where $\\hat{y}_i \\in T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# There is NO POS tagging anymore yet seq2seq modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To do the prediction, pass an LSTM over the sentence. Denote the hidden\n",
    "state at timestep $i$ as $\\mbf{h}_i$. Also, assign each tag a\n",
    "unique index. Then our prediction rule for $\\hat{y}_i$ is\n",
    "\n",
    "$$\\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(A\\mbf{h}_i + \\mbf{b}))_j$$\n",
    "\n",
    "That is, take the log softmax of the affine map of the hidden state,\n",
    "and the predicted tag is the tag that has the maximum value in this\n",
    "vector. Note this implies immediately that the dimensionality of the\n",
    "target space of $A$ is $|T|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    # Tags are: DET - determiner; NN - noun; V - verb\n",
    "    # For example, the word \"The\" is a determiner\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Now to each word we assign a unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for seq_x, seq_y in training_data:\n",
    "    for word in seq_x:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Now to each label we assign a unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
    "ix_to_tag =  { v:k for k,v in tag_to_ix.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# From text to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM + Input embedding + Output projection\n",
    "<br><div align='center'><img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" width='60%' ></div>\n",
    "\n",
    "<small>Taken from [colah.github.io](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 3\n",
    "HIDDEN_DIM = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        # NOTE that we could have used LSTM Pytorch API to implement this\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4133, -0.8714, -1.0838],\n",
      "        [-1.3909, -0.8884, -1.0793],\n",
      "        [-1.5004, -0.8467, -1.0552],\n",
      "        [-1.4326, -0.8832, -1.0560],\n",
      "        [-1.6684, -0.7974, -1.0189]])\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    seq_x = training_data[0][0] # 1st training take the input seq\n",
    "    inputs = prepare_sequence(seq_x, word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores) # num. of words (5) x num. of tags (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Fitting part (training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data: # 5 words --> 3 classes\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward() # get the gradients over params\n",
    "        optimizer.step() # update the params\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data: # 5 words --> 3 classes\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward() # get the gradients over params\n",
    "        optimizer.step() # update the params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let's see now the score after training (on the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0521, -3.6669, -3.6814],\n",
      "        [-3.8780, -0.3398, -1.3189],\n",
      "        [-3.0486, -0.5128, -1.0391],\n",
      "        [-0.0356, -4.1671, -3.9390],\n",
      "        [-4.4822, -0.2243, -1.6628]])\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['The', 'dog', 'ate', 'the', 'apple'], ['DET', 'NN', 'V', 'DET', 'NN'])\n",
      "predicted  DET -> NN -> NN -> DET -> NN\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0])\n",
    "print(\"predicted \", \" -> \".join([ix_to_tag[i.item()] for i in torch.argmax(tag_scores, dim=1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Remember: for evaluation you have to validate on the valid/test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Homework\n",
    "\n",
    "1. Study and reproduce [NLP From Scratch: Classifying Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial)\n",
    "2. Study and reproduce [NLP From Scratch: Generating Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### Neural Machine Translation (NMT)\n",
    "<br><br>\n",
    "Prof. Iacopo Masi and Prof. Stefano Faralli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's lecture\n",
    "## - What is Machine Translation\n",
    "## - Neural Machine Translation (NMT)\n",
    "## - Beam Search\n",
    "## - How to eval NMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This lecture material is taken from\n",
    "üìò **Chapter 10 Jurafsky Book**\n",
    "\n",
    "üìò **Chapter 18 Eisenstein Book**\n",
    "- [Stanford Slide NMT](http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture07-final-project.pdf)\n",
    "- [Stanford Lecture NMT](https://www.youtube.com/watch?v=wzfWHP6SXxY&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&index=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# [The Giant Language Model Test Room - GLTR](http://gltr.io/)\n",
    "\n",
    "<br><div align='center'><img src=\"figs/gltr.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Translation\n",
    "\n",
    "Machine Translation (MT) is the task of translating a sentence $\\mbf{x}$ from one language (the source language) to a sentence $\\mbf{y}$ in another language (the target language).\n",
    "\n",
    "$$ \\mbf{x} \\qquad \\text{L'homme est n√© libre, et partout il est dans les fers} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Translation\n",
    "\n",
    "Machine Translation (MT) is the task of translating a sentence x from one language (the source language) to a sentence y in another language (the target language).\n",
    "\n",
    "$$ \\mbf{x} \\qquad \\text{L'homme est n√© libre, et partout il est dans les fers} $$\n",
    "\n",
    "$$ \\mbf{y} \\qquad  \\text{Man is born free, but everywhere he is in chains}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The early history of MT: 1950s\n",
    "- Machine translation research began in the early 1950s on machines less\n",
    "powerful than high school calculators (before term ‚ÄúA.I.‚Äù coined!)\n",
    "- Concurrent with foundational work on automata, formal languages,\n",
    "probabilities, and information theory\n",
    "- MT heavily funded by military, but basically just simple rule-based\n",
    "systems doing word substitution\n",
    "- Human language is more complicated than that, and varies more across\n",
    "languages!\n",
    "- Little understanding of natural language syntax, semantics, pragmatics\n",
    "- Problem soon appeared **intractable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 1990s-2010s: Statistical Machine Translation\n",
    "\n",
    "Core idea: Learn a probabilistic model from data\n",
    "- Suppose we‚Äôre translating French ‚Üí English.\n",
    "- We want to find best English sentence y, given French sentence x\n",
    "\n",
    "$$ \\arg\\max_{y} p(y|x)$$\n",
    "\n",
    "- Use Bayes Rule to break this down into two components to be learned separately:\n",
    "$$ = \\arg\\max_{y} p(x|y)p(y)$$\n",
    "\n",
    "$p(y) \\longrightarrow$ Models how to write good English **(fluency)**. Learned from monolingual data\n",
    "\n",
    "$p(x|y)\\longrightarrow$ Translation Model: Models how words and phrases should be translated **(fidelity)**.\n",
    "Learned from aligned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Translation is not trivial to model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# There is not a clear alignment nor one-to-one mapping\n",
    "<br><div align='center'><img src=\"figs/machine_trans_01.png\" width='60%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Different symbols\n",
    "<br><div align='center'><img src=\"figs/machine_trans_03.png\" width='60%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Google Translate over time\n",
    "<br><div align='center'><img src=\"figs/machine_trans_02.png\" width='60%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 1990s-2010s: Statistical Machine Translation (SMT)\n",
    "\n",
    "SMT was a huge research field\n",
    "- The best systems were extremely complex\n",
    "- Hundreds of important details\n",
    "    - Systems had many separately-designed subcomponents\n",
    "- Lots of feature engineering\n",
    "    - Need to design features to capture particular language phenomena\n",
    "    - Required compiling and maintaining extra resources\n",
    "    - Lots of human effort to maintain\n",
    "    - Repeated effort for each language pair!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Then came Neural MT [circa 2014]\n",
    "\n",
    "<br><div align='center'><img src=\"figs/asteroidi.jpg\" width='60%' ></div>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Neural Machine Translation (NMT) is a way to do Machine Translation with **a single\n",
    "end-to-end neural network.**\n",
    "\n",
    "The neural network architecture is called a **sequence-to-sequence model (aka seq2seq)** and it involves **two RNNs**.\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/encoder-decoder.svg\" width='60%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Here, the encoder RNN will take a **variable-length sequence** as input and transform it into a **fixed-shape hidden state**. Later, we will introduce attention mechanisms, which allow us to access encoded inputs without having to compress the entire input into a single fixed-length representation\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/seq2seq.svg\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence is versatile!\n",
    "\n",
    "- The general notion here is an **encoder-decoder** model\n",
    "    - One neural network takes input and produces a neural representation\n",
    "    - Another network produces output based on that neural representation\n",
    "    - If the input and output are sequences, we call it a `seq2seq` model\n",
    "- Sequence-to-sequence is useful for **more than just MT**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NLP tasks $\\approx$ seq2seq:\n",
    "- Summarization (long text ‚Üí short text)\n",
    "- Dialogue (previous utterances ‚Üí next utterance)\n",
    "- Parsing (input text ‚Üí output parsed as sequence)\n",
    "- Code generation (natural language ‚Üí Python code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/python_NLP.png\" width='100%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NTM is a Conditional LM\n",
    "\n",
    "The sequence-to-sequence model is an example of a **Conditional Language Model**\n",
    "- **Language Model** because the decoder is predicting the next word of the target sentence $y$\n",
    "- **Conditional** because its predictions are also conditioned on the source sentence $x$\n",
    "\n",
    "NMT calculates $p(y|x)$ where $y$ is the target sentence and $x$ is the input sentence. \n",
    "Generate likely samples of $y$ given that as input I had $x$.\n",
    "\n",
    "$$p(y|x) = p(y_1|x)p(y_2|y_1,x)p(y_3|y_1,y_2,x) \\ldots p(y_t|y_1,y_2,\\ldots,y_{t-1},x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NTM is a Conditional LM\n",
    "\n",
    "\n",
    "**Question: How to train an NMT system?**\n",
    "- (Easy) Answer: Get a big parallel corpus‚Ä¶\n",
    "- But there is now exciting work on ‚Äúunsupervised NMT‚Äù, data augmentation, etc.\n",
    "\n",
    "<div align='center'><img src=\"figs/nmt_conditioning.png\" width='60%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training NMT\n",
    "\n",
    "<div align='center'><img src=\"figs/nmt_conditioning_02.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training NMT\n",
    "\n",
    "<div align='center'><img src=\"figs/nmt_conditioning_03.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-layer Deep encoder-decoder Machine Translation Net\n",
    "<br><div align='center'><img src=\"figs/nmt_deep.png\" width='80%' ></div>\n",
    "\n",
    "<small>[Sutskever et al. 2014; Luong et al. 2015]</small>\n",
    "<br> <small>Slide from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we generate a sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Generating is also called \"decoding\"\n",
    "# Decoding happens at inference [test] time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Remember something already seen about decoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$p(w_{t-1}|w_t = \\text{natural}) = 1 \\cdot 0.9\\cdot0.95\\cdot0.65\\cdot0.2 = 0.11115$\n",
    "\n",
    "Loss is $-\\log\\big(p(w_{t-1}|w_t = \\text{natural})\\big) = -\\log(0.11115)$\n",
    "\n",
    "<div align='center'><img src=\"figs/hsoftmax_3g.png?2\" width='35%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Decoding\n",
    "\n",
    "**Important:** In inference with do not have the label!\n",
    "\n",
    "1. Exhaustive search [too complex]\n",
    "2. **Greedy search** (at each branch take the branch at maximum probability) [too greedy and deterministic] $\\longleftarrow$\n",
    "3. Beam search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Decoding: Greedy decoding\n",
    "\n",
    "- We saw how to generate (or \"decode\") the target sentence by taking `argmax` on each step of the decoder.\n",
    "- This is greedy decoding (take most probable word on each step)\n",
    "<br><div align='center'><img src=\"figs/greedy_decoding.png\" width='45%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Decoding: Probabilistic Decoding\n",
    "\n",
    "- Instead of doing `argmax` we sample from the probability at each layer. \n",
    "- This makes the algorithm randomized in case we want to generate multiple, similar sentences.\n",
    "<br><div align='center'><img src=\"figs/nmt_conditioning_04.png\" width='35%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The problem with Greedy Decoding\n",
    "\n",
    "Greedy decoding has no way to undo decisions! Once a decision is taken, is taken!\n",
    "- Input: `il a m‚Äôentart√©` $\\longrightarrow$ `he hit me with a pie`\n",
    "\n",
    "Step 1: `he __________`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Step 2: `he hit_______`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Step 3: `he hit a_____` _(wrong prediction,  no way of going back)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Greedy is suboptimal:** at each local step, you just choose the maximum, without seeing the entire distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Decoding\n",
    "\n",
    "**Important:** In inference with do not have the label!\n",
    "\n",
    "1. **Exhaustive search** [too complex] $\\longleftarrow$\n",
    "2. Greedy search (at each branch take the branch at maximum probability) [too greedy] \n",
    "3. Beam search (we will cover later on) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exhaustive Search\n",
    "\n",
    "Ideally, we want to find a (length $T$) translation/decoding $y$ that maximizes:\n",
    "\n",
    "$$ p(y|x) = p(y_1 | x) p(y_2 | y_1, x) p(y_3 | y_2,y_1, x) \n",
    "\\ldots(y_T | y_{T-1},\\ldots,y_1, x) = \\prod_{i=1}^T p(y_T|y_{T-1},\\ldots,y_1,x)$$\n",
    "\n",
    "We could try computing **all possible sequences y** and take globally the most likely.\n",
    "- This means that on each step $t$ of the decoder, we are tracking $V^t$ possible partial translations, where $V$ is vocab size\n",
    "- This $\\mathcal{O}(|V|^T)$ is **far too expensive.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Decoding\n",
    "\n",
    "\n",
    "1. Exhaustive search [too complex]\n",
    "2. Greedy search (at each branch take the branch at maximum probability) [too greedy] \n",
    "3. <u>**Beam search**</u> $\\longleftarrow$ ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Beam search decoding üî¶\n",
    "\n",
    "**Core idea:** take a trade-off approach between local (greedy) and global (exhaustive). On each step of decoder, keep track of the $k$ **most probable partial translations** (which we call **hypotheses**)\n",
    "\n",
    "A **hypothesis** $\\{y_1,\\ldots,y_t\\}$ has a score which is its log probability:\n",
    "\n",
    "$\\text{score}\\{y_1,\\ldots,y_t\\} = \\log p(y_1,\\ldots,y_t|x) = \\sum_{i=1}^t  \\log p(y_i|y_1,\\ldots,y_{i-1},x) $\n",
    "\n",
    "- Scores are all negative (negative means low prob.), and higher score is better (high prob.)\n",
    "- We search for high-scoring hypotheses, **tracking top-k on each step**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Beam search decoding üî¶\n",
    "\n",
    "Beam search is not guaranteed to find optimal solution but much more efficient than exhaustive search.\n",
    "To better understand how **beam search** works we use a **search tree**.\n",
    "\n",
    "<div align='center'><img src=\"figs/beam_search_01.png\" width='65%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Beam search decoding üî¶\n",
    "This **fixed-size beam width memory footprint** $k$ is called the **beam width**, on the metaphor of a flashlight beam that can be parameterized to be wider or narrower. In practice $k$ is 5 or 10.\n",
    "\n",
    "<div align='center'><img src=\"figs/beam_search_02.png\" width='45%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Beam search decoding üî¶\n",
    "\n",
    "<div align='center'><img src=\"figs/beam_search_03.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Beam search decoding üî¶: stopping criterion\n",
    "\n",
    "In greedy decoding, usually we decode until the model produces an `<END>` token\n",
    " - For example: `<START> he hit me with a pie <END>`\n",
    "\n",
    "In beam search decoding, different hypotheses may produce `<END>` tokens on\n",
    "different timesteps:\n",
    " - When a hypothesis produces `<END>`, that hypothesis is complete.\n",
    " - Place it aside and continue exploring other hypotheses via beam search.\n",
    "\n",
    "Usually we continue beam search until:\n",
    " - We reach **timestep T (where T is some pre-defined cutoff)**, OR\n",
    " - We have at least $n$ completed hypotheses (where $n$ is pre-defined cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Beam search decoding üî¶: finishing up\n",
    "\n",
    "We have our list of completed hypotheses.\n",
    "- How to select top one?\n",
    "\n",
    "- Each hypothesis $\\{y_1,\\ldots,y_n\\}$ on our list has a score:\n",
    "\n",
    "$$ score(y_1,\\ldots,y_n) = \\log p(y_1,\\ldots,y_n|x) = \\sum_{i=i}^T \\log p(y_i|y_1,\\ldots,y_{i-1},x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Can you spot a problem with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Shorter Sentences may have higher scores!\n",
    "\n",
    "**Fix: Normalize by length. Use this to select top one instead**\n",
    "\n",
    "$$\\frac{1}{T}\\sum_{i=i}^T \\log p(y_i|y_1,\\ldots,y_{i-1},x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How do we evaluate Machine Translation?\n",
    "\n",
    "**BLEU (Bilingual Evaluation Understudy)**\n",
    "\n",
    "**BLEU** compares the machine-written translation to one or several human-written translation(s), and **computes a similarity score** based on:\n",
    "- Geometric mean of **n-gram precision** (usually for 1, 2, 3 and 4-grams)\n",
    "- Plus a penalty for too-short system translations\n",
    "\n",
    "BLEU is useful but imperfect:\n",
    "- There are many valid ways to translate a sentence\n",
    "- So a good translation can get a poor BLEU score because it has low n-gram overlap with the human translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BLEU\n",
    "<div align='center'><img src=\"figs/bleu.png\" width='45%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NMT Progress\n",
    "\n",
    "<div align='center'><img src=\"figs/nmt_progress.png\" width='50%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Advantages of NMT ‚úÖ\n",
    "\n",
    "Compared to SMT, NMT has many **advantages**: \n",
    "\n",
    "- Better performance\n",
    "    - More fluent\n",
    "    - Better use of context\n",
    "    - Better use of phrase similarities\n",
    "    \n",
    "- A single neural network to be optimized end-to-end\n",
    "    - No subcomponents to be individually optimized\n",
    "    \n",
    "- Requires much less human engineering effort\n",
    "    - No feature engineering\n",
    "    - Same method for all language pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Disadvantages of NMT ‚ùå\n",
    "\n",
    "Compared to SMT: \n",
    "\n",
    "- NMT is **less interpretable**\n",
    "    - Hard to debug\n",
    "    - Why this translation came up?\n",
    "    \n",
    "- NMT is **difficult to control**\n",
    "    - For example, cannot easily specify rules or guidelines for translation\n",
    "    - Safety concerns!\n",
    "    - Invention of content not in source\n",
    "    - Systematic gender biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NMT: the first big success story of NLP Deep Learning\n",
    "\n",
    "Neural Machine Translation went from a **fringe research attempt** in 2014 to the leading\n",
    "**standard method** in 2016\n",
    "\n",
    "- 2014: First seq2seq paper published [Sutskever et al. 2014]\n",
    "- 2016: Google Translate switches from SMT to NMT ‚Äì and by 2018 everyone has.\n",
    "<div align='center'><img src=\"figs/companies.png\" width='50%' ></div>\n",
    "- SMT systems, built by hundreds of engineers over many years, outperformed by NMT systems trained by small groups of engineers in a few months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Using NMT to introduce Attention üßê"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NTM is a Conditional LM\n",
    "\n",
    "\n",
    "Do you see any problem with this architecture?\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_00.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why attention? Sequence-to-sequence: the bottleneck problem\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_01.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why attention? Sequence-to-sequence: the bottleneck problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attention üßê\n",
    "\n",
    "Attention provides a **solution to the bottleneck problem.**\n",
    "\n",
    "**Core idea:** on each step of the decoder, <u>**use direct connection to the encoder to focus\n",
    "on a particular part**</u> of the source sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence with attention\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_02.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence with attention\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_03.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence with attention\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_04.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence with attention\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_05.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence with attention\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_06.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence with attention\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_07.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence with attention\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_08.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence with attention\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_09.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence with attention\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_10.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence with attention\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_11.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence with attention\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_12.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence-to-sequence with attention\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_13.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attention with Equations\n",
    "\n",
    "- We have encoder hidden states $\\mbf{h}_1,\\ldots,\\mbf{h}_N \\in \\mathbb{R}^h$\n",
    "- **On timestep $t$**, we have decoder hidden state $\\mbf{s}_t \\in \\mathbb{R}^h$\n",
    "- We get the attention scores for this step $\\mbf{e}^t$:\n",
    "\n",
    "$$ \\mbf{e}^t = [ \\mbf{s}_t^{\\top}\\mbf{h}_1,\\ldots,\\mbf{s}_t^{\\top}\\mbf{h}_N] \\in \\mathbb{R}^N$$\n",
    "\n",
    "- We take **softmax** to get the attention distribution for this step (this is a probability distribution and sums to 1)\n",
    "$$ \\alpha^t = \\operatorname{softmax}(\\mbf{e}^t) $$\n",
    "\n",
    "- We use $\\alpha^t$ to take a weighted sum of the encoder hidden states to get the attention output $\\mbf{a}_t$:\n",
    " $$ \\mbf{a}_t = \\sum_{i=1}^N \\alpha_i\\mbf{h}_i \\in \\mathbb{R}^h $$\n",
    "- Finally we concatenate the attention output $\\mbf{a}_t$ with the decoder hidden state $\\mbf{s}_t$ and proceed as in the non-attention seq2seq model: $$[\\mbf{a}_t,\\mbf{s}_t] \\in \\mathbb{R}^{2h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attention üßê is great! ü¶æ\n",
    "\n",
    "Attention provides **some interpretability**:\n",
    "- By inspecting attention distribution, we see what the decoder was focusing on\n",
    "- We get (soft) alignment for free!\n",
    "- This is cool because we never explicitly trained an alignment system\n",
    "- The network just learned alignment by itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/attention_14.png\" width='60%' ></div>\n",
    "<small>Picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# There are _several_ attention variants\n",
    "- We have some **values** $h_1,\\ldots, h_n \\in \\mathbb{R}^{D1}$ and query $\\mbf{s} \\in \\mathbb{R}^{D2}$\n",
    "- Attention always involves:\n",
    "    1. Computing the attention scores $\\mbf{e} \\in \\mathbb{R}^{N}$\n",
    "    2. Taking `softmax` to get attention distribution $\\alpha$:\n",
    "     $$ \\alpha = \\operatorname{softmax}(\\mbf{e}) \\in \\mathbb{R}^{N}$$\n",
    "    3. Using attention distribution to take weighted sum of values:\n",
    "     $$\\mbf{a} = \\sum_{i=1}^N \\alpha_i\\mbf{h}_i  \\in \\mathbb{R}^{D1}  $$\n",
    "    4. thus obtaining the **attention output** $\\mbf{a}$ (sometimes called the **context vector**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attention variants\n",
    "\n",
    "\n",
    "There are several ways you can compute $\\mbf{e} \\in \\mathbb{R}^{N}$ from $h_1,\\ldots, h_n \\in \\mathbb{R}^{D1}$ and $\\mbf{s} \\in \\mathbb{R}^{D2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Dot product attention\n",
    "\n",
    "Basic dot-product attention: $\\mbf{e}_i = \\mbf{s}^{\\top}\\mbf{h}_i $. Note: this assumes $D1=D2$ This is the version we saw earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Bilinear attention\n",
    "\n",
    "Bilinear attention: $\\mbf{e}_i = \\mbf{s}^{\\top}\\mbf{W}\\mbf{h}_i $. $\\mbf{W} \\in \\mathbb{R}^{D1\\times D2}$ is a weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Reduced Rank\n",
    "\n",
    "As bilinear but low rank: $\\mbf{e}_i = \\mbf{s}^{\\top}(\\mbf{U}^{\\top}\\mbf{V})\\mbf{h}_i =(\\mbf{U}\\mbf{s})^{\\top}(\\mbf{V}\\mbf{h}_i)$. For low rank matrices $\\mbf{U} \\in \\mathbb{R}^{k \\times D2}$ and $\\mbf{V} \\in \\mathbb{R}^{k \\times D1}$ with $k \\ll D1,D2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attention is a _general_ Deep Learning technique\n",
    "\n",
    "We have seen that attention is a great way to improve the sequence-to-sequence model for Machine Translation.\n",
    "- However: **You can use attention in many architectures** (not just seq2seq) and **many tasks** (not just MT)\n",
    "\n",
    "More general definition of attention:\n",
    " - Given a set of vector **values**, and a vector **query**, attention is a technique to compute\n",
    "a <u>weighted sum of the values, dependent on the query</u>\n",
    "\n",
    "- We sometimes say that **the query attends to the values**\n",
    "- For example, in the `seq2seq + attention model`, each decoder hidden state (`query`) attends to all the encoder hidden states (`values`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Intuition about Attention üßê\n",
    "\n",
    "- The weighted sum is a **selective summary** of the information contained in the values, where the query determines which values to focus on.\n",
    "- Attention is a way to obtain a **fixed-size representation** of an arbitrary set of representations (the values), **dependent on some other representation** (the query).\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_13.png\" width='60%' ></div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "key",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "rise": {
   "autolaunch": true,
   "overlay": "<div class='myheader'>Natural Language Processing<img src='../sapienza_logo.png'/></div>",
   "transition": "linear"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Summary",
   "toc_cell": false,
   "toc_position": {
    "height": "47px",
    "left": "1143px",
    "top": "173px",
    "width": "210.344px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
