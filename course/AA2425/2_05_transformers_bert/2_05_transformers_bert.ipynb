{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### Transformers\n",
    "\n",
    "<br><br>\n",
    "Prof. Iacopo Masi and Prof. Stefano Faralli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.colheader_justify', 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "#plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "font = {'family' : 'Times',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "\n",
    "# Aux functions\n",
    "\n",
    "def plot_grid(Xs, Ys, axs=None):\n",
    "    ''' Aux function to plot a grid'''\n",
    "    t = np.arange(Xs.size) # define progression of int for indexing colormap\n",
    "    if axs:\n",
    "        axs.plot(0, 0, marker='*', color='r', linestyle='none') #plot origin\n",
    "        axs.scatter(Xs,Ys, c=t, cmap='jet', marker='.') # scatter x vs y\n",
    "        axs.axis('scaled') # axis scaled\n",
    "    else:\n",
    "        plt.plot(0, 0, marker='*', color='r', linestyle='none') #plot origin\n",
    "        plt.scatter(Xs,Ys, c=t, cmap='jet', marker='.') # scatter x vs y\n",
    "        plt.axis('scaled') # axis scaled\n",
    "        \n",
    "def linear_map(A, Xs, Ys):\n",
    "    '''Map src points with A'''\n",
    "    # [NxN,NxN] -> NxNx2 # add 3-rd axis, like adding another layer\n",
    "    src = np.stack((Xs,Ys), axis=Xs.ndim)\n",
    "    # flatten first two dimension\n",
    "    # (NN)x2\n",
    "    src_r = src.reshape(-1,src.shape[-1]) #ask reshape to keep last dimension and adjust the rest\n",
    "    # 2x2 @ 2x(NN)\n",
    "    dst = A @ src_r.T # 2xNN\n",
    "    #(NN)x2 and then reshape as NxNx2\n",
    "    dst = (dst.T).reshape(src.shape)\n",
    "    # Access X and Y\n",
    "    return dst[...,0], dst[...,1]\n",
    "\n",
    "\n",
    "def plot_points(ax, Xs, Ys, col='red', unit=None, linestyle='solid'):\n",
    "    '''Plots points'''\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, which='both')\n",
    "    ax.axhline(y=0, color='gray', linestyle=\"--\")\n",
    "    ax.axvline(x=0, color='gray',  linestyle=\"--\")\n",
    "    ax.plot(Xs, Ys, color=col)\n",
    "    if unit is None:\n",
    "        plotVectors(ax, [[0,1],[1,0]], ['gray']*2, alpha=1, linestyle=linestyle)\n",
    "    else:\n",
    "        plotVectors(ax, unit, [col]*2, alpha=1, linestyle=linestyle)\n",
    "\n",
    "def plotVectors(ax, vecs, cols, alpha=1, linestyle='solid'):\n",
    "    '''Plot set of vectors.'''\n",
    "    for i in range(len(vecs)):\n",
    "        x = np.concatenate([[0,0], vecs[i]])\n",
    "        ax.quiver([x[0]],\n",
    "                   [x[1]],\n",
    "                   [x[2]],\n",
    "                   [x[3]],\n",
    "                   angles='xy', scale_units='xy', scale=1, color=cols[i],\n",
    "                   alpha=alpha, linestyle=linestyle, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align='center'><img src='https://www.dottorgadget.it/news/wp-content/uploads/2022/07/transformers-1984-optimus-prime.gif' width='15%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## My own latex definitions\n",
    "\n",
    "$$\\def\\mbf#1{\\mathbf{#1}}$$\n",
    "$$\\def\\bmf#1{\\boldsymbol{#1}}$$\n",
    "$$\\def\\bx{\\mbf{x}}$$\n",
    "$$\\def\\bxt#1{\\mbf{x}_{\\text{#1}}}$$\n",
    "$$\\def\\bv{\\mbf{v}}$$\n",
    "$$\\def\\bz{\\mbf{z}}$$\n",
    "$$\\def\\bmu{\\bmf{\\mu}}$$\n",
    "$$\\def\\bsigma{\\bmf{\\Sigma}}$$\n",
    "$$\\def\\Rd#1{\\in \\mathbb{R}^{#1}}$$\n",
    "$$\\def\\chain#1#2{\\frac{\\partial #1}{\\partial #2}}$$\n",
    "$$\\def\\loss{\\mathcal{L}}$$\n",
    "$$\\def\\params{\\bmf{\\theta}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's lecture\n",
    "## - Limitations of RNN\n",
    "## - Self and Cross-Attention\n",
    "## - The Transformers Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This lecture material is taken from\n",
    "üìò **Chapter 9, 10, 11 Jurafsky Book**\n",
    "\n",
    "üìò **Chapter 6.3 Eisenstein Book**\n",
    "- [Stanford Slide Transformers](http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture08-transformers.pdf)\n",
    "- [Stanford Lecture Transformers](https://www.youtube.com/watch?v=ptuGllU5SQQ&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&index=9&themeRefresh=1)\n",
    "- [Stanford Notes on Transformers](http://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)\n",
    "\n",
    "Another resource with code is [[d2l.ai] Attention and Transformers](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)\n",
    "\n",
    "Illustrated Transformer [jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Our last lecture: NLP  ‚ù§Ô∏è RNN\n",
    "\n",
    "- For what we have seen so far the state-of-the-arti: **bidirectional LSTM**: (for example, the source sentence in a translation) **This was circa 2016**\n",
    "- Define your input (sequence)and use an **LSTM** to reduce it (classification) or generate (sequence generation).\n",
    "- Notably we have seen that we can use **attention** üßê to avoid encode/decoder bottleneck and go back to see previous part in the text.\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_13.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2Ô∏è‚É£0Ô∏è‚É£1Ô∏è‚É£7Ô∏è‚É£   ‚û°Ô∏è‚û°Ô∏è 2Ô∏è‚É£0Ô∏è‚É£2Ô∏è‚É£3Ô∏è‚É£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Limitations of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Limitations of RNN\n",
    "\n",
    "<div align='center'><img src=\"figs/intro_transf.png\" width='90%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Limitations of RNN\n",
    "\n",
    "### Linear interaction distance\n",
    "### Intrinsic Lack of Parallelizability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Limitations of RNN: Linear interaction distance\n",
    "\n",
    "- RNNs are unrolled ‚Äúleft-to-right‚Äù.\n",
    "- This encodes **linear locality:** a useful heuristic!\n",
    "- Nearby words often affect each other‚Äôs meanings\n",
    "\n",
    "\n",
    "- **Problem:** RNNs take $\\mathcal{O}(T)$ steps for\n",
    "distant word pairs to interact where $T$ is the sequence length.\n",
    "\n",
    "<div align='center'><img src=\"figs/short_seq.png?1\" width='10%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Limitations of RNN: Linear interaction distance\n",
    "\n",
    "- RNNs are unrolled ‚Äúleft-to-right‚Äù.\n",
    "- This encodes **linear locality:** a useful heuristic!\n",
    "- Nearby words often affect each other‚Äôs meanings\n",
    "- **Problem:** RNNs take $\\mathcal{O}(T)$ steps for\n",
    "distant word pairs to interact where $T$ is the sequence length.\n",
    "\n",
    "<div align='center'><img src=\"figs/long_seq.png?1\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Limitations of RNN: Linear interaction distance\n",
    "\n",
    "$\\mathcal{O}(T)$ steps for distant word pairs to interact means:\n",
    "- Hard to learn **long-distance dependencies (because of vanishing gradient problems!)**\n",
    "- Linear order of words is **‚Äúbaked in‚Äù**; we already know linear order is not the right way to think about sentences...\n",
    "\n",
    "<div align='center'><img src=\"figs/long_seq.png?1\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Limitations of RNN: Lack of Parallelizability\n",
    "\n",
    "Forward and backward passes have $\\mathcal{O}(T)$ **unparallelizable operations**\n",
    "- GPUs can perform a bunch of independent computations at once!\n",
    "- But **future RNN hidden states can not be computed in full before past RNN hidden states have been computed**\n",
    "- Inhibits training on very large datasets!\n",
    "<div align='center'><img src=\"figs/rnn_time_dependency.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# If not recurrence, then what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What about window-based classifer?\n",
    "\n",
    "Word window models **aggregate local contexts**\n",
    "- Also known as 1D convolution! On images, this works very well, aka 2D convolution!\n",
    "- **Number of unparallelizable operations does not increase with sequence length!**\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/local_context.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What about window-based classifer?\n",
    "\n",
    "The hidden state at layer 2, at time position $t=2$ will \"see\" hidden states at layer 1 at positions 1,2,3.\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/local_context_02.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# OK, good but how we make long-distance dependencies?\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/local_context_02.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ideas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Long-distance dependencies with local context [Receptive Field]\n",
    "\n",
    "Stacking word window layers in depth allows interaction between farther words\n",
    "- Maximum Interaction distance = sequence length / window size\n",
    "- (But if your sequences are too long, you‚Äôll just ignore long-distance context)\n",
    "\n",
    "<br/><br/>\n",
    "<div align='center'><img src=\"figs/local_context_03.png?2\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Overall take message so far: \n",
    "# - parallelizability in time is a must (RNN do not have that)\n",
    "# - we can \"pass on\" parallelizability over depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What about Attention? üßê\n",
    "\n",
    "Attention treats each word‚Äôs representation as a **query** to access and incorporate information from a **set of values**.\n",
    "\n",
    "We saw attention from **the decoder to the encoder**; today: attention with a single sentence.\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/attention_13.png\" width='50%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What about Attention? üßê\n",
    "\n",
    "- Number of unparallelizable operations does not increase with sequence length.\n",
    "- Maximum interaction distance: $\\mathcal{O}(1)$, since all words interact at every layer!\n",
    "- All words attend to all words in previous layer (most arrows are omitted)\n",
    "\n",
    "<br><br>\n",
    "<div align='center'><img src=\"figs/self_attention_01.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br><div align='center'><img src=\"../2_04_from_rnn_to_nmt/figs/asteroidi.jpg\" width='60%' ></div>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/all_you_need.png\" width='60%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A few words on query, key, values\n",
    "\n",
    "So far all the networks we reviewed crucially relied on the **input being of a well-defined size.**\n",
    "\n",
    "- **[VISION]** The images in ImageNet are of size $224 \\times 224$ pixels and CNNs are specifically tuned to this size. \n",
    "\n",
    "- **[NLP]** **the input size for RNNs is well defined and fixed**. Variable size is addressed by sequentially processing one token at a time, or by specially designed convolution kernels.\n",
    "\n",
    "In particular, for **long sequences it becomes quite difficult to keep track** of everything that has already been generated or even viewed by the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Query, key, values terminology is from databases\n",
    "\n",
    "In their simplest form they are collections of keys ($k$) and values ($v$). For instance, our database $\\mathcal{D}$ might consist of tuples.\n",
    "\n",
    " `{(\"key\", \"value\")`\n",
    "\n",
    " `{(\"Zhang\", \"Aston\"), (\"Lipton\", \"Zachary\"), (\"Li\", \"Mu\"), (\"Smola\", \"Alex\"), (\"Hu\", \"Rachel\"), (\"Werness\", \"Brent\")}`\n",
    "\n",
    "- query ($q$) for \"Li\"  $\\longrightarrow$ \"Mu\". Note query matches the key and returns the value.\n",
    "- In case `(\"Li\", \"Mu\")` was not a record in $\\mathcal{D}$, there would be no valid answer!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we also allowed for **approximate matches**, we would retrieve `(\"Lipton\", \"Zachary\")` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Query, key, values terminology is from databases\n",
    "\n",
    "* We can design queries $q$ that operate on ($k$,$v$) pairs in such a manner as to be valid regardless of the  database size. \n",
    "* The same query can receive different answers, according to the contents of the database. \n",
    "* The \"code\" being executed to operate on a large state space (the database) can be quite simple (e.g., exact match, approximate match, top-$k$). \n",
    "* There is no need to compress or simplify the database to make the operations effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Lookup table vs Soft-average Lookup table\n",
    "\n",
    "<div align='center'><img src=\"figs/self_attention_02.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Self-Attention: soft, averaging lookup table\n",
    "\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/attention-output.svg\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Self-Attention\n",
    "\n",
    "<div align='center'><img src=\"figs/self_attention_03.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Self-Attention: soft, averaging lookup table\n",
    "\n",
    "Denote by $\\mathcal{D} \\stackrel{\\mathrm{def}}{=} \\{(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots (\\mathbf{k}_m, \\mathbf{v}_m)\\}$ a database of $m$ tuples of *keys* and *values*. Moreover, denote by $\\mathbf{q}$ a *query*. Then we can define the *attention* over $\\mathcal{D}$ as\n",
    "\n",
    "$$\\mathrm{Attention}(\\mathbf{q}, \\mathcal{D}) \\stackrel{\\mathrm{def}}{=} \\sum_{i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i,$$\n",
    "\n",
    "where $\\alpha(\\mathbf{q}, \\mathbf{k}_i) \\in \\mathbb{R}$ ($i = 1, \\ldots, m$) are **scalar attention weights**. The operation itself is typically referred to as *attention pooling*. \n",
    "\n",
    "\n",
    "The name *attention* derives from the fact that the operation pays particular attention to the terms for which the weight $\\alpha$ is significant (i.e., large). As such, the attention over $\\mathcal{D}$ generates a linear combination of values contained in the database. In fact, this contains the above example as a special case where all but one weight is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Self-Attention Properties\n",
    "* The weights $\\alpha(\\mathbf{q}, \\mathbf{k}_i)$ are nonnegative. In this case the output of the attention mechanism is contained in the convex cone spanned by the values $\\mathbf{v}_i$. \n",
    "* The weights $\\alpha(\\mathbf{q}, \\mathbf{k}_i)$ form a convex combination, i.e., $\\sum_i \\alpha(\\mathbf{q}, \\mathbf{k}_i) = 1$ and $\\alpha(\\mathbf{q}, \\mathbf{k}_i) \\geq 0$ for all $i$. This is the most common setting in deep learning. \n",
    "* Exactly one of the weights $\\alpha(\\mathbf{q}, \\mathbf{k}_i)$ is $1$, while all others are $0$. This is akin to a traditional database query. \n",
    "* All weights are equal, i.e., $\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\frac{1}{m}$ for all $i$. This amounts to averaging across the entire database, also called average pooling in deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Softmax normalization\n",
    "A common strategy to ensure that the weights sum up to $1$ is to normalize them via \n",
    "\n",
    "$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\frac{\\alpha(\\mathbf{q}, \\mathbf{k}_i)}{{\\sum_j} \\alpha(\\mathbf{q}, \\mathbf{k}_j)}.$$\n",
    "\n",
    "In particular, to ensure that the weights are also nonnegative, one can resort to exponentiation. This means that we can now pick *any* function  $a(\\mathbf{q}, \\mathbf{k})$ and then apply the softmax operation used for multinomial models to it via\n",
    "\n",
    "$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\frac{\\exp\\big(a(\\mathbf{q}, \\mathbf{k}_i)\\big)}{\\sum_j \\exp\\big(a(\\mathbf{q}, \\mathbf{k}_j)\\big)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Self-Attention\n",
    "\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/qkv.svg\" width='50%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# So far, <u>nothing is learnable</u>, just linear combination\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Before going into the learning part, let us make a connection with classic machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attention Pooling via Nadaraya-Watson Regression [1964]\n",
    "\n",
    "\n",
    "$$f(\\mathbf{x}) = \\sum_i \\mathbf{y}_i \\underbrace{\\frac{\\alpha(\\mathbf{x}, \\mathbf{x}_i)}{\\sum_j \\alpha(\\mathbf{x}, \\mathbf{x}_j)}}_{\\text{attention}}.$$\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/output_attention-pooling_d5e6b2_63_0.svg\" width='80%' ></div>\n",
    "\n",
    "<small>[Taken from d2l.ai](https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html)</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attention Pooling via Nadaraya-Watson Regression\n",
    "\n",
    "\n",
    "$$f(\\mathbf{q}) = \\sum_i \\mathbf{v}_i \\underbrace{\\frac{\\alpha(\\mathbf{q}, \\mathbf{k}_i)}{\\sum_j \\alpha(\\mathbf{q}, \\mathbf{k}_j)}}_{\\text{attention}}.$$\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/output_attention-pooling_d5e6b2_78_0.svg\" width='80%' ></div>\n",
    "\n",
    "<small>[Taken from d2l.ai](https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html)</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attention Pooling via Nadaraya-Watson Regression\n",
    "\n",
    "The way you compare $\\alpha(\\mathbf{q}, \\mathbf{k}_i)$ gives rise at different shape in the regression (control the range and smoothness).\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\alpha(\\mathbf{q}, \\mathbf{k}) & = \\exp\\left(-\\frac{1}{2} \\|\\mathbf{q} - \\mathbf{k}\\|^2 \\right) && \\mathrm{Gaussian} \\\\\n",
    "\\alpha(\\mathbf{q}, \\mathbf{k}) & = 1 \\text{ if } \\|\\mathbf{q} - \\mathbf{k}\\| \\leq 1 && \\mathrm{Boxcar} \\\\\n",
    "\\alpha(\\mathbf{q}, \\mathbf{k}) & = 1 && \\mathrm{constant} \\\\\n",
    "\\alpha(\\mathbf{q}, \\mathbf{k}) & = \\mathop{\\mathrm{max}}\\left(0, 1 - \\|\\mathbf{q} - \\mathbf{k}\\|\\right) && \\mathrm{Epanechikov}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attention Pooling via Nadaraya-Watson Regression\n",
    "\n",
    "```python\n",
    "def nadaraya_watson(x_train, y_train, x_val, kernel):\n",
    "    dists = x_train.reshape((-1, 1)) - x_val.reshape((1, -1))\n",
    "    # Each column/row corresponds to each query/key\n",
    "    k = kernel(dists).type(torch.float32)\n",
    "    # Normalization over keys for each query\n",
    "    attention_w = k / k.sum(0)\n",
    "    y_hat = y_train@attention_w\n",
    "    return y_hat, attention_w\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attention Weights\n",
    "<br>\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/output_attention-pooling_d5e6b2_63_0.svg\" width='80%' ></div>\n",
    "\n",
    "<div align='center'>Note that, besides the constant kernel, they are all similar. \n",
    "<br>Why not stick to Gaussian kernel and tune its bandwidth?<img src=\"https://d2l.ai/_images/output_attention-pooling_d5e6b2_78_0.svg\" width='80%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let's tune the bandwidth parameter in the [Gaussian] kernel\n",
    "$$ \\alpha(\\mathbf{q}, \\mathbf{k}) = \\exp\\left(-\\frac{1}{2 \\sigma^2} \\|\\mathbf{q} - \\mathbf{k}\\|^2 \\right) $$\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/output_attention-pooling_d5e6b2_93_0.svg\" width='80%' ></div>\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/output_attention-pooling_d5e6b2_108_0.svg\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Instead of tuning the bandwidth why not learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learnable self-attention\n",
    "\n",
    "Let $\\{w_1,\\ldots,w_n \\}$ be a sequence of words in vocabulary $V$, like \n",
    "\n",
    "```Iacopo made his daughter food```.\n",
    "\n",
    "For each word token $w_i$ , let $\\mbf{x}_i = \\mbf{E}{w_i}$, where $\\mbf{E} \\in \\mathbb{R}^{d\\times |ùëâ|}$ is an embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Learnable self-attention\n",
    "\n",
    "1. We transform each word embedding $\\mbf{x}_i$ with <u>**learnable</u> weight matrices** $\\mbf{Q},\\mbf{K},\\mbf{V} \\in \\mathbb{R}^{d\\times d}$\n",
    "\n",
    " $$ \\mbf{q}_i=\\mbf{Q}\\mbf{x}_i \\qquad  \\mbf{k}_i=\\mbf{K}\\mbf{x}_i \\qquad \\mbf{v}_i=\\mbf{V}\\mbf{x}_i \\qquad$$\n",
    "2. Compute pairwise similarities between keys and queries; normalize with softmax (across keys):\n",
    "\n",
    "$$ e_{ij} = \\mbf{q}_i^{\\top}\\mbf{k}_j \\qquad \\bmf{\\alpha}_{ij}=\\frac{\\exp(e_{ij})}{\\sum_j \\exp(e_{ij}) } $$\n",
    "\n",
    "3. Compute output for each word as weighted sum of values:\n",
    "$$ \\mbf{o}_i = \\sum_{j} \\bmf{\\alpha}_{ij}\\mbf{v}_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Computation with two word tokens\n",
    "\n",
    "<div align='center'><img src=\"http://jalammar.github.io/images/t/transformer_self_attention_vectors.png\" width='60%' ></div>\n",
    "\n",
    "<small>[Taken from illustrated-transformer](http://jalammar.github.io/illustrated-transformer/)</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Computation with two word tokens\n",
    "\n",
    "<div align='center'><img src=\"http://jalammar.github.io/images/t/transformer_self_attention_score.png\" width='60%' ></div>\n",
    "\n",
    "<small>[Taken from illustrated-transformer](http://jalammar.github.io/illustrated-transformer/)</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Computation with two word tokens\n",
    "\n",
    "<div align='center'><img src=\"http://jalammar.github.io/images/t/self-attention_softmax.png\" width='60%' ></div>\n",
    "\n",
    "<small>[Taken from illustrated-transformer](http://jalammar.github.io/illustrated-transformer/)</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Computation with two word tokens\n",
    "\n",
    "<div align='center'><img src=\"http://jalammar.github.io/images/t/self-attention-output.png\" width='50%' ></div>\n",
    "\n",
    "<small>[Taken from illustrated-transformer](http://jalammar.github.io/illustrated-transformer/)</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Computation with more than 2 word tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/self_attention_04.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/self_attention_05.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/self_attention_06.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention can be easily made into tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation.png\" width='50%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attention with tensor\n",
    "\n",
    "Let us assume that $\\mbf{X} = [\\mbf{x}_1,\\ldots,\\mbf{x}_n] \\in \\mathbb{R}^{n\\times d}$ is a matrix of concatenated input vectors. We are going to write all in function of the input $\\mbf{X}$. Note also that:\n",
    "- $\\underbrace{\\mbf{X}}_{n\\times d}\\underbrace{\\mbf{K}}_{d\\times d} \\in \\mathbb{R}^{n \\times d}$, same hold for $\\mbf{X}\\mbf{Q} \\in \\mathbb{R}^{n \\times d}$ and $\\mbf{X}\\mbf{V} \\in \\mathbb{R}^{n \\times d}$ \n",
    "- The output is defined as $\\operatorname{softmax}(\\mbf{X}\\mbf{Q}(\\mbf{X}\\mbf{K})^{\\top})\\cdot\\mbf{X}\\mbf{V}$\n",
    "\n",
    "<div align='center'><img src=\"figs/self_attention_07.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Self-Attention with tensor\n",
    "\n",
    "Let us assume that $\\mbf{X} = [\\mbf{x}_1,\\ldots,\\mbf{x}_n] \\in \\mathbb{R}^{n\\times d}$ is a matrix of concatenated input vectors. We are going to write all in function of the input $\\mbf{X}$. Note also that:\n",
    "- $\\underbrace{\\mbf{X}}_{n\\times d}\\underbrace{\\mbf{K}}_{d\\times d} \\in \\mathbb{R}^{n \\times d}$, same hold for $\\mbf{X}\\mbf{Q} \\in \\mathbb{R}^{n \\times d}$ and $\\mbf{X}\\mbf{V} \\in \\mathbb{R}^{n \\times d}$ \n",
    "- The output is defined as $\\operatorname{softmax}(\\mbf{X}\\mbf{Q}(\\mbf{X}\\mbf{K})^{\\top})\\cdot\\mbf{X}\\mbf{V}  $\n",
    "\n",
    "<div align='center'><img src=\"figs/self_attention_08.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Are we done with Self-Attention?\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/self_attention_09.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# No! Still many things to fix üò™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Self-attention as a NLP block: Fix 1) No notion of order!\n",
    "\n",
    "Self-attention, as of now, <u>works on **sets**</u>, does not have a **notion of order**; thus is permutation equivariant (i.e. permutations of the inputs give same permutation of the output).\n",
    "\n",
    "`Iacopo made his daughter food` $=$ ` his daughter made Iacopo food`\n",
    "\n",
    "RNN had order encoded **implicitly** in the representation Iacopo $\\rightarrow$ made $\\rightarrow$ his $\\rightarrow$ daughter \n",
    "\n",
    "Why not putting the **word index explicit in the representation**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| Iacopo | made | his | daughter | food |\n",
    "|:------:|:----:|:---:|:--------:|:----:|\n",
    "|    0   |   1  |  2  |     3    |   4  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Self-attention as a NLP block: Fix 1) No notion of order!\n",
    "\n",
    "| **x** | Iacopo | made | his | daughter | food |\n",
    "|-------|:------:|:----:|:---:|:--------:|:----:|\n",
    "| **i** |    0   |   1  |  2  |     3    |   4  |\n",
    "\n",
    "With $\\mbf{x}$ we encode $i$ as well. The **sequence index** is mapped to a vector with a function. Let's call  $\\mbf{p}$ the output of this function given $i$.\n",
    "\n",
    "$$ \\mbf{\\tilde{x}}_i = \\mbf{x}_i + \\mbf{p}_i $$\n",
    "\n",
    "So we **add signal of the index in the feature itself** (we could also concat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Fix 1) No notion of order!\n",
    "The dominant approach for preserving  information about the order of tokens is to represent this to the model \n",
    "as an additional input associated  with each token.  These inputs are called **positional encodings** and they can either be:\n",
    "1. learned or \n",
    "2. fixed a priori.\n",
    "\n",
    "We now describe a simple scheme for fixed positional encodings based on sine and cosine functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*pS2_ywtYRO7hIRoj0XpHBQ.png\" width='55%' ></div>\n",
    "\n",
    "[Taken from towardsdatascience.com](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*R3U8xOrxuYRYNLe961n7bg.png\" width='55%' ></div>\n",
    "\n",
    "[Taken from towardsdatascience.com](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Positional Encoding\n",
    "\n",
    "### Absolute Positional Information\n",
    "\n",
    "To see how the monotonically decreased frequency\n",
    "along the encoding dimension relates to absolute positional information,\n",
    "let's print out **the binary representations** of $0, 1, \\ldots, 7$.\n",
    "As we can see, the lowest bit, the second-lowest bit, \n",
    "and the third-lowest bit alternate on every number, \n",
    "every two numbers, and every four numbers, respectively.\n",
    "\n",
    "```python\n",
    "0 in binary is 000\n",
    "1 in binary is 001\n",
    "2 in binary is 010\n",
    "3 in binary is 011\n",
    "4 in binary is 100\n",
    "5 in binary is 101\n",
    "6 in binary is 110\n",
    "7 in binary is 111\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Positional Encoding\n",
    "\n",
    "Suppose that the input representation \n",
    "$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ \n",
    "contains the $d$-dimensional embeddings \n",
    "for $n$ tokens of a sequence.\n",
    "The positional encoding outputs\n",
    "$\\mathbf{X} + \\mathbf{P}$\n",
    "using a positional embedding matrix \n",
    "$\\mathbf{P} \\in \\mathbb{R}^{n \\times d}$ of the same shape,\n",
    "whose element on the $i^\\mathrm{th}$ row \n",
    "and the $(2j)^\\mathrm{th}$\n",
    "or the $(2j + 1)^\\mathrm{th}$ column is\n",
    "\n",
    "$$\\begin{aligned} p_{i, 2j} &= \\sin\\left(\\frac{i}{10000^{2j/d}}\\right),\\\\p_{i, 2j+1} &= \\cos\\left(\\frac{i}{10000^{2j/d}}\\right).\\end{aligned}$$\n",
    "\n",
    "At first glance,\n",
    "this trigonometric-function\n",
    "design looks weird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Positional Encoding\n",
    "\n",
    "In the positional embedding matrix $\\mathbf{P}$,\n",
    "**rows correspond to positions within a sequence\n",
    "and columns represent different positional encoding dimensions**.\n",
    "In the example below,\n",
    "we can see that\n",
    "the $6^{\\mathrm{th}}$ and the $7^{\\mathrm{th}}$\n",
    "columns of the positional embedding matrix \n",
    "have a higher frequency than \n",
    "the $8^{\\mathrm{th}}$ and the $9^{\\mathrm{th}}$\n",
    "columns.\n",
    "The offset between \n",
    "the $6^{\\mathrm{th}}$ and the $7^{\\mathrm{th}}$ (same for the $8^{\\mathrm{th}}$ and the $9^{\\mathrm{th}}$) columns\n",
    "is due to the alternation of sine and cosine functions.\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/output_self-attention-and-positional-encoding_ce9eb6_48_0.svg\" width='60%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Positional Encoding - Absolute Positional Info\n",
    "\n",
    "In binary representations, a higher bit has a lower frequency than a lower bit. Similarly, as demonstrated in the heat map below, the positional encoding decreases frequencies along the encoding dimension by using trigonometric functions. Since the outputs are float numbers, such continuous representations are more space-efficient than binary representations.\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/output_self-attention-and-positional-encoding_ce9eb6_78_0.svg\" width='25%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Positional Encoding - Relative Positional Info\n",
    "\n",
    "Besides capturing absolute positional information,\n",
    "the above positional encoding\n",
    "also allows\n",
    "a model to easily learn to attend by relative positions.\n",
    "This is because\n",
    "for any fixed position offset $\\delta$,\n",
    "the positional encoding at position $i + \\delta$\n",
    "can be represented by a linear projection\n",
    "of that at position $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This projection can be explained\n",
    "mathematically.\n",
    "Denoting\n",
    "$\\omega_j = 1/10000^{2j/d}$,\n",
    "any pair of $(p_{i, 2j}, p_{i, 2j+1})$ \n",
    "in equation below\n",
    "can \n",
    "be linearly projected to $(p_{i+\\delta, 2j}, p_{i+\\delta, 2j+1})$\n",
    "for any fixed offset $\\delta$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\begin{bmatrix} \\cos(\\delta \\omega_j) & \\sin(\\delta \\omega_j) \\\\  -\\sin(\\delta \\omega_j) & \\cos(\\delta \\omega_j) \\\\ \\end{bmatrix}\n",
    "\\begin{bmatrix} p_{i, 2j} \\\\  p_{i, 2j+1} \\\\ \\end{bmatrix}\\\\\n",
    "=&\\begin{bmatrix} \\cos(\\delta \\omega_j) \\sin(i \\omega_j) + \\sin(\\delta \\omega_j) \\cos(i \\omega_j) \\\\  -\\sin(\\delta \\omega_j) \\sin(i \\omega_j) + \\cos(\\delta \\omega_j) \\cos(i \\omega_j) \\\\ \\end{bmatrix}\\\\\n",
    "=&\\begin{bmatrix} \\sin\\left((i+\\delta) \\omega_j\\right) \\\\  \\cos\\left((i+\\delta) \\omega_j\\right) \\\\ \\end{bmatrix}\\\\\n",
    "=& \n",
    "\\begin{bmatrix} p_{i+\\delta, 2j} \\\\  p_{i+\\delta, 2j+1} \\\\ \\end{bmatrix},\n",
    "\\end{aligned}$$\n",
    "\n",
    "where the $2\\times 2$ projection matrix does not depend on any position index $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Self-Attention so far\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/self_attention_10.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Self-attention as a NLP block: Fix 2) No nonlinearities! Just weighted average.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fix 2) Adding non-linearity\n",
    "\n",
    "- Note that there are no elementwise\n",
    "nonlinearities in self-attention;\n",
    "stacking more self-attention layers\n",
    "just re-averages value vectors\n",
    "\n",
    "- Easy fix: **add a feed-forward network (MLP)**\n",
    "to \"post-process\" each output vector.\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mbf{m}_i = &\\text{MLP}(\\mbf{output}_i)=\\\\\n",
    "= & \\mbf{W}_2\\operatorname{ReLu}\\big(\\mbf{W}_1\\mbf{output}_i+ \\mbf{b}_1 \\big)+ \\mbf{b}_2 \n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#   \n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/self_attention_11.png\" width='100%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Positionwise Feed-Forward Networks (MLP)\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/self_attention_11.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Self-attention as a NLP block: Fix 3) We can look in the future! \n",
    "## <u> Only used in the Decoder </u>\n",
    "\n",
    "With RNN, this was not possible but with Self-attention we need to ensure we **do not ‚Äúlook at the future‚Äù** when predicting a sequence for:\n",
    "- Machine Translation\n",
    "- Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Masking Self-attention\n",
    "\n",
    "- To use self-attention in\n",
    "decoders, we need to ensure\n",
    "we can‚Äôt peek at the future\n",
    "\n",
    "- At every timestep, we could\n",
    "change the set of keys and\n",
    "queries to include only past\n",
    "words. (Inefficient!)\n",
    "\n",
    "- To enable parallelization, we\n",
    "mask out attention to future\n",
    "words by setting attention\n",
    "scores to $-\\infty$\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mbf{e}_{ij} = \\begin{cases}\n",
    "      \\mbf{q}_i^{\\top}\\mbf{k}_j & j \\leq i\\\\\n",
    "      -\\infty & j > i\\\\\n",
    "    \\end{cases}\\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#   \n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/self_attention_12.png\" width='100%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Transformer Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align='center'><img src='https://www.dottorgadget.it/news/wp-content/uploads/2022/07/transformers-1984-optimus-prime.gif' width='15%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Transformer\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/transformer.svg\" width='35%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Missing pieces\n",
    "\n",
    "## 1) Multi-Head Attention\n",
    "### 2a) Residual Connection\n",
    "### 2b) Layer Normalization\n",
    "### 2c) Scaled Dot-Product\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/transformer.svg\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The world is \"multi-modal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Self-Attention\n",
    "\n",
    "<div align='center'><img src=\"figs/self_attention_03.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-head self-attention\n",
    "<br>\n",
    "<div align='center'><img src=\"https://dmdave.com/wp-content/uploads/2019/03/demonic-hydra.jpg\" width='60%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-head self-attention\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/self_attention_13.png\" width='90%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-head self-attention\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/multi-head.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Self-Attention with tensor\n",
    "\n",
    "Let us assume that $\\mbf{X} = [\\mbf{x}_1,\\ldots,\\mbf{x}_n] \\in \\mathbb{R}^{n\\times d}$ is a matrix of concatenated input vectors. We are going to write all in function of the input $\\mbf{X}$. Note also that:\n",
    "- $\\underbrace{\\mbf{X}}_{n\\times d}\\underbrace{\\mbf{K}}_{d\\times d} \\in \\mathbb{R}^{n \\times d}$, same hold for $\\mbf{X}\\mbf{Q} \\in \\mathbb{R}^{n \\times d}$ and $\\mbf{X}\\mbf{V} \\in \\mathbb{R}^{n \\times d}$ \n",
    "- The output is defined as $\\operatorname{softmax}(\\mbf{X}\\mbf{Q}(\\mbf{X}\\mbf{K})^{\\top})\\cdot\\mbf{X}\\mbf{V}  $\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/self_attention_08.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi Head Self-Attention with tensor\n",
    "\n",
    "Instead of a single set of weights, we have now **$h$** weights $\\{\\mbf{Q}_l, \\mbf{K}_l, \\mbf{V}_l\\}_{l=1}^h$ but to keep the same computational cost we cut the output dimensionality.\n",
    "\n",
    "- $\\mbf{Q}_l,\\mbf{K}_l, \\mbf{V}_l \\in \\mathbb{R}^{d\\times \\frac{d}{h}}$ where $l \\in  \\{1,\\ldots,h \\}$.\n",
    "- The output$_l$ is defined as $\\operatorname{softmax}(\\mbf{X}\\mbf{Q}_l(\\mbf{X}\\mbf{K}_l)^{\\top})\\cdot\\mbf{X}\\mbf{V}_l   \\in \\mathbb{R}^{d\\times \\frac{d}{h}}$\n",
    "- The final output is concatenation of all output processed by a linear projection\n",
    " $$ \\text{output} = \\mbf{Y} [\\text{output}_1;\\ldots; \\text{output}_h]$$\n",
    "\n",
    "<div align='center'><img src=\"figs/multi-head_00.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi Head Self-Attention with tensor\n",
    "\n",
    "Instead of a single set of weights, we have now **$h$** weights $\\{\\mbf{Q}_l, \\mbf{K}_l, \\mbf{V}_l\\}_{l=1}^h$ but to keep the same computational cost we cut the output dimensionality.\n",
    "\n",
    "- $\\mbf{Q}_l,\\mbf{K}_l, \\mbf{V}_l \\in \\mathbb{R}^{d\\times \\frac{d}{h}}$ where $l$ ranges from $1,\\ldots,h$.\n",
    "- The output$_l$ is defined as $\\operatorname{softmax}(\\mbf{X}\\mbf{Q}_l(\\mbf{X}\\mbf{K}_l)^{\\top})\\cdot\\mbf{X}\\mbf{V}_l   \\in \\mathbb{R}^{d\\times \\frac{d}{h}}$\n",
    "- The final output is concatenation of all output processed by a linear projection\n",
    " $$ \\text{output} = \\mbf{Y} [\\text{output}_1;\\ldots; \\text{output}_h]$$\n",
    "\n",
    "### <u>Computationally is the same thing as before with the same #parameters except for $\\mbf{Y}$.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 1) Multi Head Self-Attention \n",
    "<br>\n",
    "<div align='center'><img src=\"figs/multi-head_01.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 2a) Residual Connection [He et al., 2016]\n",
    "\n",
    "Residual connections are a powerful mechanism to allow gradients to flow better in your model. Sometimes they are also called skip connections. They have been popular in computer vision [He et al., 2016]\n",
    "\n",
    " $$\\mbf{x}^i = \\operatorname{Layer}\\big(\\mbf{x}^{i-1}\\big)+\\mbf{x}^{i-1}  $$\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/residual_connection.png\" width='30%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 2a) Residual Connection\n",
    "\n",
    "$$\\mbf{x}^i = \\operatorname{Layer}\\big(\\mbf{x}^{i-1}\\big)+\\mbf{x}^{i-1}  $$\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/residual_connection.png\" width='70%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/residual_connection_01.png\" width='100%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 2b) Layer Normalization [Ba et al., 2016]\n",
    "\n",
    "$\\mbf{x} \\in \\mathbb{R}^{d}$ be a word embedding then:\n",
    " $$ \\mbf{x}^{\\prime} = \\frac{\\mbf{x}-\\mu}{\\sigma^2+\\epsilon}\\cdot \\mbf{\\gamma}+\\mbf{\\beta}$$\n",
    "\n",
    "Note that $\\mu,\\sigma$ are scalar while $\\mbf{\\gamma}$,$\\mbf{\\beta}$ vectors. $~~~~~\\downarrow$\n",
    "\n",
    "<div align='center'><img src=\"figs/layer_norm.png\" width='100%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 2c) Scaled Dot Product\n",
    "Last, we need to keep the order of magnitude of the arguments in the exponential function under control. Assume that all the elements of the query $\\mathbf{q} \\in \\mathbb{R}^d$ and the key $\\mathbf{k}_i \\in \\mathbb{R}^d$ are independent and identically drawn random variables **with zero mean and unit variance**. The dot product between both vectors has zero mean and a variance of $d$. To ensure that the variance of the dot product still remains one regardless of vector length, we use the **scaled dot-product attention** scoring function. That is, we rescale the dot-product by $1/\\sqrt{d}$. \n",
    "\n",
    "$$ a(\\mathbf{q}, \\mathbf{k}_i) = \\frac{\\mathbf{q}^\\top \\mathbf{k}_i}{\\sqrt{d}}.$$\n",
    "\n",
    "[More info here](https://github.com/BAI-Yeqi/Statistical-Properties-of-Dot-Product/blob/master/proof.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Transformer\n",
    "\n",
    "<div align='center'><img src=\"figs/paper_00.png?1\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Transformer\n",
    "\n",
    "<div align='center'><img src=\"figs/paper_01.png\" width='38%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Cross-Attention\n",
    "\n",
    "- Self-attention, query, key and values come from the same **source**\n",
    "- In the decoder, we have attention that looks more like what we saw in NMT.\n",
    "- $\\{ \\mbf{h}_1,\\ldots,\\mbf{h}_T \\}$ last output from the encoder\n",
    "- $\\{ \\mbf{z}_1,\\ldots,\\mbf{z}_T \\}$ is the input of the decoder [this can be input representation of input to next layer]\n",
    "\n",
    "**Keys and Values are drawn from the encoder (like a memory)**\n",
    "\n",
    "- $\\underbrace{\\mbf{k}_i = \\mbf{K}\\mbf{h}_i \\quad \\mbf{v}_i = \\mbf{V}\\mbf{h}_i}_{\\text{encoder}} \\quad \\underbrace{\\mbf{q}_i = \\mbf{Q}\\mbf{z}_i}_{\\text{decoder}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Cross-Attention with tensor\n",
    "\n",
    "$\\mbf{H} = [\\mbf{h}_1,\\ldots,\\mbf{h}_n] \\in \\mathbb{R}^{n\\times d}$ is a matrix of concatenated last encodings.\n",
    "\n",
    "$\\mbf{D} = [\\mbf{d}_1,\\ldots,\\mbf{d}_n] \\in \\mathbb{R}^{n\\times d}$ is a matrix of concatenated input in the decoder.\n",
    "\n",
    "The output is defined as $\\operatorname{softmax}(\\mbf{D}\\mbf{Q}(\\mbf{H}\\mbf{K})^{\\top})\\cdot\\mbf{H}\\mbf{V}  $\n",
    "\n",
    "<div align='center'><img src=\"figs/corss-attention.png\" width='50%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transformers Decoding\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=\"http://jalammar.github.io/images/t/transformer_decoding_1.gif\" width='80%' >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transformers Decoding\n",
    "\n",
    "<div align='center'><img src=\"http://jalammar.github.io/images/t/transformer_decoding_2.gif\" width='80%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Final Linear Layer\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"http://jalammar.github.io/images/t/transformer_decoder_output_softmax.png\" width='50%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Translation Performance\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/performance_01.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Text Summarization\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/performance_00.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### Contextual Embedding, Subword model, \n",
    "### BERT, Transfer Learning (Pre-training)\n",
    "\n",
    "<br><br>\n",
    "Prof. Iacopo Masi and Prof. Stefano Faralli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's lecture\n",
    "### - Few words on subwords modeling (Byte pairing)\n",
    "## - GPT, BERT\n",
    "## - Transfer Learning (Pre-training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This lecture material is taken from\n",
    "üìò **Chapter 9, 10, 11 Jurafsky Book**\n",
    "\n",
    "üìò **Chapter 6.3 Eisenstein Book**\n",
    "- [Stanford Slide Transformers](http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture08-transformers.pdf)\n",
    "- [Stanford Lecture Transformers](https://www.youtube.com/watch?v=ptuGllU5SQQ&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&index=9&themeRefresh=1)\n",
    "- [Stanford Notes on Transformers](http://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)\n",
    "\n",
    "Another resource with code is [[d2l.ai] Attention and Transformers](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)\n",
    "\n",
    "Illustrated Transformer [jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Subword Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Word structure and subword models\n",
    "\n",
    "Let us take a look at the assumptions we have made about a language's vocabulary.\n",
    "\n",
    "We assume a fixed vocab of tens of thousands of words, built from the training set.\n",
    "All novel words seen at test time are mapped to a single **`UNK`**\n",
    "\n",
    "| type       | word         | V mapping     | embedding         |\n",
    "|------------|--------------|---------------|-------------------|\n",
    "| common     | hat          | hat (index)   | $\\mbf{e}_{hat}$   |\n",
    "| common     | learn        | learn (index) | $\\mbf{e}_{learn}$ |\n",
    "| variations | taaaasty     | UNK (index)   | $\\mbf{e}_{UNK}$   |\n",
    "| typo       | laern        | UNK (index)   | $\\mbf{e}_{UNK}$   |\n",
    "| new word   | Transformify | UNK (index)   | $\\mbf{e}_{UNK}$   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Word structure and subword models: Words Morphology\n",
    "\n",
    "Finite vocabulary assumptions make **even less sense in many COMPLEX languages**.\n",
    "- Many languages exhibit **complex morphology,** or word structure.\n",
    "- The effect is more word types, each occurring fewer times\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/verbs.png\" width='50%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Subword modeling: we give up the assumption of single word token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Byte Pair Encoding [2015]\n",
    "\n",
    "To allow for **variable-length subwords** in a **fixed-size vocabulary**, we can apply a compression algorithm called byte pair encoding (BPE) to extract subwords. BPE is in the middle between:\n",
    "\n",
    "1. Break down all words into characters (the vocabulary is the set of chars). Model has to make a big effort to learn words.\n",
    "2. Usual way of splitting the text in word token: too rigid, as soon as we get an non frequent unknown word we map it to `UNK`.\n",
    "\n",
    "BPE starts from assumption 1. and slowly builds towards point 2. We learn the Vocabulary using a greedy approach starting from 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Byte Pair Encoding\n",
    "\n",
    "Simple, effective strategy for defining a subword vocabulary.\n",
    "\n",
    "1. Start with a **vocabulary containing only characters and an ‚Äúend-of-word‚Äù symbol**.\n",
    "2. Using a corpus of text, find the most common adjacent characters ‚Äúa,b‚Äù; add ‚Äúab‚Äù as a subword.\n",
    "3. Replace instances of the character pair with the new subword; repeat until desired vocab size.\n",
    "\n",
    "Originally used in NLP for machine translation; now a similar method (**WordPiece**) is used in pretrained\n",
    "models.\n",
    "\n",
    "**BPE used in GPT-2 and RoBERTA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Byte Pair Encoding\n",
    "| type       | word         | V mapping     |      embedding     |\n",
    "|------------|--------------|---------------|:------------------:|\n",
    "| common     | hat          | hat (index)   |   $\\mbf{e}_{hat}$  |\n",
    "| common     | learn        | learn (index) |  $\\mbf{e}_{learn}$ |\n",
    "| variations | taaaasty     | taa aaa sty   | $\\mbf{e}_i ~i=1..3$ |\n",
    "| typo       | laern        | la ern        | $\\mbf{e}_i ~i=1..2$ |\n",
    "| new word   | Transformify | Transform ify | $\\mbf{e}_i ~i=1..2$ |\n",
    "\n",
    "[Taken from d2l.ai](https://d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html#byte-pair-encoding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Byte Pair Encoding\n",
    "\n",
    "We start from: \n",
    "\n",
    "```python\n",
    "symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "           '_', '[UNK]']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Byte Pair Encoding\n",
    "\n",
    "From a corpus, we get raw frequency tokens (here only 4 for simplicity).\n",
    "\n",
    "```python\n",
    "raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\n",
    "```\n",
    "\n",
    "From this we break each word in chars `_` is needed to remember where words where ending.\n",
    "\n",
    "```python\n",
    "{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Merging process by most frequent adjacent subwords\n",
    "\n",
    "\n",
    "```python\n",
    "merge #1: ('t', 'a')\n",
    "merge #2: ('ta', 'l')\n",
    "merge #3: ('tal', 'l')\n",
    "merge #4: ('f', 'a')\n",
    "merge #5: ('fa', 's')\n",
    "merge #6: ('fas', 't')\n",
    "merge #7: ('e', 'r')\n",
    "merge #8: ('er', '_')\n",
    "merge #9: ('tall', '_')\n",
    "merge #10: ('fast', '_')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# English syntax and rules emerge\n",
    "\n",
    "Faster and taller are broken as:\n",
    "\n",
    "```python\n",
    "['fast_', 'fast er_', 'tall_', 'tall er_']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Out of Vocabulary\n",
    "\n",
    "We have to model:\n",
    "```python\n",
    "tokens = ['tallest_', 'fatter_']\n",
    "```\n",
    "\n",
    "what happens with **BPE**?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "['tall e s t _', 'fa t t er_']\n",
    "```\n",
    "\n",
    "able to recover `tall` and also `fa` and `er` but fragmenting `e` `s` `t` etc. **Using a big corpus can fix this.**\n",
    "\n",
    "[Taken from d2l.ai](https://d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html#byte-pair-encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contextual Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Motivating word meaning and context\n",
    "\n",
    "Stated by J. R. Firth (1957) as: \n",
    "\n",
    "> \"You shall know a word by the company it keeps\"\n",
    "\n",
    "Distributional statistics have a striking ability to capture lexical semantic relationships such as **analogies**.\n",
    "\n",
    "J. R. Firth (1935) also stated:\n",
    "\n",
    "> \"... the complete meaning of a word is always contextual,\n",
    "and no study of meaning apart from a complete context\n",
    "can be taken seriously.\"\n",
    "\n",
    "Consider `I record the record:` the two instances of `record` mean different things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Where we were: pretrained word embeddings\n",
    "\n",
    "Circa 2017:\n",
    "- Start with pretrained word embeddings (no\n",
    "context!)\n",
    "- Learn how to incorporate context in an LSTM or Transformer while training on the task\n",
    "\n",
    "Issues:\n",
    "- The training data we have for **our downstream task** (like question answering) **must be sufficient** to teach all contextual aspects of language\n",
    "- Most of the parameters in our network are **randomly initialized!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Pretrained word embeddings\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/pre_training.png\" width='100%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Where we are: pretrained whole model\n",
    "\n",
    "In modern NLP:\n",
    "- All (or almost all) parameters in NLP\n",
    "networks are initialized via **pretraining**.\n",
    "- Pretraining methods hide parts of the input\n",
    "from the model, and train the model to\n",
    "reconstruct those parts\n",
    "\n",
    "This has been exceptionally effective at\n",
    "building strong:\n",
    "- **representations of language**\n",
    "- **parameter initializations** for strong NLP\n",
    "models.\n",
    "- **Probability distributions** over language that we can sample from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Pretrained model\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/pre_training_01.png\" width='90%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pretraining ‚ù§Ô∏è Self-Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What can we learn from reconstructing the input?\n",
    "\n",
    "<br><br><br>\n",
    "$$ \\text{Sapienza University is located in __________, Italy}$$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What can we learn from reconstructing the input?\n",
    "\n",
    "<br><br><br>\n",
    "$$ \\text{I put ___ fork down on the table.}$$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What can we learn from reconstructing the input?\n",
    "\n",
    "<br><br><br>\n",
    "$$ \\text{The woman walked across the street,\n",
    "checking for traffic over ___ shoulder.}$$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What can we learn from reconstructing the input?\n",
    "\n",
    "<br><br><br>\n",
    "$$ \\text{I went to the ocean to see the fish, turtles, seals, and _____.}$$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What can we learn from reconstructing the input?\n",
    "\n",
    "<br><br><br>\n",
    "$$\\text{Overall, the value I got from the two hours watching\n",
    "it was the sum total of the popcorn and the drink.}$$\n",
    "\n",
    "$$\\text{The movie was ______.}$$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What can we learn from reconstructing the input?\n",
    "\n",
    "<br><br><br>\n",
    "$$\\text{Iroh went into the kitchen to make some tea.}$$\n",
    "$$\\text{Standing next to Iroh, Zuko pondered his destiny.}$$\n",
    "$$\\text{Zuko left the ______.}$$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Transformer (Encoder and Decoder)\n",
    "\n",
    "<div align='center'><img src=\"figs/paper_01.png\" width='35%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pretraining through language modeling (LM)\n",
    "\n",
    "Recall the language modeling task:\n",
    "- Model $p_{\\theta}(w_t | w_1,\\ldots,w_{t-1})$, the probability\n",
    "distribution over words given their past\n",
    "contexts.\n",
    "- There‚Äôs lots of data for this! (In English)\n",
    "\n",
    "Pretraining through language modeling:\n",
    "- Train a neural network to perform language\n",
    "modeling on a large amount of text.\n",
    "- Save the network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/pre_training_00.png\" width='98%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # The Pretraining / Finetuning Paradigm\n",
    " \n",
    " See Pretraining as serving for a **smart parameter initialization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Pretraining\n",
    "**Step 1: Pretrain (on language modeling)**\n",
    "Lots of text; learn general things!<br>\n",
    "<div align='center'><img src=\"figs/pre_training_00.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Finetuning Paradigm\n",
    "\n",
    "**Step 2: Finetune (on your task)**\n",
    "Not many labels; adapt to the task!<br>\n",
    "<div align='center'><img src=\"figs/pre_training_02.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Three ways to pre-train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pretraining for three types of architectures\n",
    "\n",
    "\n",
    "## 1) Decoder Only\n",
    "\n",
    "Language models! What we have seen so far. Nice to generate from yet can **not** condition on future words<br>\n",
    "<div align='center'><img src=\"figs/decoder_only.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2) Encoder Only\n",
    "\n",
    "Gets **bidirectional context ‚Äì can condition on future!** How do we train them to build strong representations?<br>\n",
    "\n",
    "<div align='center'><img src=\"figs/encoder_only.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3) Encoder-Decoder\n",
    "\n",
    "The best of both worlds: Good parts of decoders and encoders?  What‚Äôs the best way to pretrain them?\n",
    "\n",
    "<div align='center'><img src=\"figs/e2d.png\" width=60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 1) Decoder only - Option 1\n",
    "\n",
    "When using language model pretrained decoders, we can ignore that they were trained to model $p_{\\theta}(w_t | w_1,\\ldots,w_{t-1})$.\n",
    "\n",
    "We can fine-tune them by training a classifier\n",
    "on the last word‚Äôs hidden state.\n",
    "\n",
    "$$ \\mbf{h}_1,\\ldots,\\mbf{h}_t=\\operatorname{decoder}(w_1,\\ldots,w_t)\\\\\n",
    "\\mbf{y} = \\mbf{W}\\mbf{h}_t+\\mbf{b}$$\n",
    "\n",
    "$\\mbf{W},\\mbf{b}$ are randomly initialized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/decoder_finetune.png\" width=60%' ></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 1) Decoder only - Option 2\n",
    "\n",
    "Re-use them as LM $p_{\\theta}(w_t | w_1,\\ldots,w_{t-1})$.\n",
    "\n",
    "This is helpful in tasks where the output is a sequence with a vocabulary like that at\n",
    "pretraining time!\n",
    "\n",
    "- **Dialogue** (context=dialogue history)\n",
    "- **Summarization** (context=document)\n",
    "\n",
    "$$ \\mbf{h}_1,\\ldots,\\mbf{h}_t=\\operatorname{decoder}(w_1,\\ldots,w_t)\\\\\n",
    "w_{t+1} = \\mbf{W}\\mbf{h}_t+\\mbf{b}$$\n",
    "\n",
    "$\\mbf{W},\\mbf{b}$ were pretrained in the language model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/decoder_finetune2.png\" width=60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generative Pretrained Transformer (GPT) [Radford et al., 2018]\n",
    "<br>\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/gpt-decoder-only.svg\" width='30%'></div>\n",
    "\n",
    "    2018‚Äôs GPT was a big success in pretraining a decoder!\n",
    "\n",
    "- Transformer decoder with **12 layers, 117M parameters.**\n",
    "- **768**-dimensional hidden states, **3072**-dimensional feed-forward hidden layers.\n",
    "- Byte-pair encoding with **40,000 merges** (it is not the size of the vocab.)\n",
    "- Trained on **BooksCorpus: over 7000 unique books.**\n",
    "- Contains long spans of contiguous text, for learning **long-distance dependencies.**\n",
    "- The acronym \"GPT\" never showed up in the original paper; it could stand for \"Generative PreTraining\" or \"Generative Pretrained Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generative Pretrained Transformer (GPT) [Radford et al., 2018]\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/GPT_paper.png\" width='70%'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generative Pretrained Transformer (GPT) [Radford et al., 2018]\n",
    "\n",
    "```\n",
    "Premise: The main is in the doorway\n",
    "Hypothesis: The man is near the door\n",
    "\n",
    "labels: entailment/contradictory/neutral\n",
    "```\n",
    "Input: `[START] The main is in the doorway [DELIM] The man is near the door [EXTRACT]]`\n",
    "<div align='center'><img src=\"figs/GPT_experiments.png\" width='65%'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GPT2: increasingly convincing generations [Radford et al., 2018]\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/GPT2_generation.png\" width='80%'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GPT3\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/GPT3_01.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GPT3\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/GPT3_00.png?2\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GPT3\n",
    "\n",
    "GPT3 has **175 billion parameters.** trained on **300B tokens of text**. Context window is **2048 tokens** (a few pages?).\n",
    "\n",
    "Not much different than before\n",
    "\n",
    "> We use the same model and architecture as GPT-2 [RWC+19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GPT3 -  Huge Model, Huge Training set\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/GPT3_02.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GPT3 -  In-context learning\n",
    "\n",
    "We can \"interact\" with pretrained models in two ways:\n",
    "- Sample from the distributions they define (maybe providing a prompt)\n",
    "- Fine-tune them on a task we care about, and take their predictions.\n",
    "\n",
    "**Very large language models seem to perform some kind of learning without gradient steps simply from examples you provide within their contexts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GPT3 -  In-context learning\n",
    "\n",
    "<div align='center'><img src=\"figs/GPT3_03.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GPT3 -  In-context learning\n",
    "\n",
    "<br><div align='center'><img src=\"figs/GPT3_04.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GPT3 -  In-context learning\n",
    "\n",
    "<br><div align='center'><img src=\"figs/GPT3_04.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GPT3 -  In-context learning\n",
    "\n",
    "<br><div align='center'><img src=\"figs/prompt_zeroshot.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2) Encoder Only\n",
    "\n",
    "Gets **bidirectional context ‚Äì can condition on future!** How do we train them to build strong representations?<br>\n",
    "\n",
    "<div align='center'><img src=\"figs/encoder_only.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Masked Language Modeling + BERT (Bidirectional Encoder Representations from Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Masked Language Modeling\n",
    "\n",
    "We have looked at language model pre-training. But **encoders** get **bidirectional context**, so we **can not do language modeling!**\n",
    "\n",
    "**Idea:** replace some fraction of words in the\n",
    "input with a special `[MASK]` token; predict\n",
    "these words.\n",
    "\n",
    "\n",
    "$$ \\mbf{h}_1,\\ldots,\\mbf{h}_t=\\operatorname{encoder}(w_1,\\ldots,w_t)\\\\\n",
    "w_{t+1} = \\mbf{W}\\mbf{h}_t+\\mbf{b}$$\n",
    "\n",
    "**Masked LM:** Only add loss terms from words that are\n",
    "\"masked out.\" If $\\tilde{\\mbf{x}}$ is the masked version of $\\mbf{x}$, we are learning $p_{\\bmf{\\theta}}(\\mbf{x}|\\tilde{\\mbf{x}}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/masked_LM.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BERT: Masked LM\n",
    "\n",
    "[Devlin et al., 2018] proposed the **Masked LM** objective and released the weights of a\n",
    "pretrained Transformer, a model they labeled **BERT**.\n",
    "\n",
    "\n",
    "Some more details about Masked LM for BERT:\n",
    "- Predict a random **15% of (sub)word tokens.**\n",
    "- Replace input word with **[MASK] 80% of the time**\n",
    "- Replace input word with a **random token 10% of the time**\n",
    "- Leave input word **unchanged 10% of the time** (but still predict it!)\n",
    "- Why? Does not let the model get complacent and not build strong representations of non-masked words. (No masks are seen at fine-tuning time!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/BERT_01.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BERT: Masked LM\n",
    "\n",
    "\n",
    "[Devlin et al., 2018] proposed the **Masked LM** objective and released the weights of a\n",
    "pretrained Transformer, a model they labeled **BERT**.\n",
    "\n",
    "\n",
    "Some more details about Masked LM for BERT:\n",
    "- Predict a random **15% of (sub)word tokens.**\n",
    "- Replace input word with **[MASK] 80% of the time**\n",
    "- Replace input word with a **random token 10% of the time**\n",
    "- Leave input word **unchanged 10% of the time** (but still predict it!)\n",
    "- Why? Does not let the model get complacent and not build strong representations of non-masked words. (No masks are seen at fine-tuning time!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/BERT_02.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BERT: Masked LM\n",
    "\n",
    "\n",
    "[Devlin et al., 2018] proposed the **Masked LM** objective and released the weights of a\n",
    "pretrained Transformer, a model they labeled **BERT**.\n",
    "\n",
    "\n",
    "Some more details about Masked LM for BERT:\n",
    "- Predict a random **15% of (sub)word tokens.**\n",
    "- Replace input word with **[MASK] 80% of the time**\n",
    "- Replace input word with a **random token 10% of the time**\n",
    "- Leave input word **unchanged 10% of the time** (but still predict it!)\n",
    "- Why? Does not let the model get complacent and not build strong representations of non-masked words. (No masks are seen at fine-tuning time!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/BERT_03.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BERT: Next Sentence Prediction\n",
    "\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/bert-input.svg\" width='70%' ></div>\n",
    "\n",
    "\n",
    "BERT was trained to predict whether one chunk follows the other or is randomly sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BERT Details\n",
    "\n",
    "Two models were released:\n",
    "- BERT-base: 12 layers, 768-dim hidden states, 12 attention heads, **110 million params.**\n",
    "- BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, **340 million params**.\n",
    "\n",
    "Trained on:\n",
    "- BooksCorpus (800 million words)\n",
    "- English Wikipedia (2,500 million words)\n",
    "\n",
    "Pretraining is expensive and impractical on a single GPU.\n",
    "- BERT was pretrained with 64 TPU chips for a total of 4 days.\n",
    "- (TPUs are special tensor operation acceleration hardware)\n",
    "\n",
    "Finetuning is practical and common on a single GPU **(\"Pretrain once, finetune many times.\")**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BERT: One model many tasks\n",
    "\n",
    "<div align='center'><img src=\"figs/BERT_04.png\" width='70%' ></div>\n",
    "\n",
    "**QQP:** Quora Question Pairs (detect paraphrase questions)\n",
    "**QNLI:** natural language inference over question answering data\n",
    "**SST-2:** sentiment analysis\n",
    "**CoLA:** corpus of linguistic acceptability (detect whether sentences are grammatical.)\n",
    "**STS-B:** semantic textual similarity\n",
    "**MRPC:** Microsoft paraphrase corpus\n",
    "**RTE:** a small natural language inference corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GLUE Benchmarks\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/GLUE_01.png\" width='70%' ></div>\n",
    "\n",
    "https://gluebenchmark.com/leaderboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BERT Extensions\n",
    "\n",
    "There are a of BERT variants like **RoBERTa**, **SpanBERT** etc.\n",
    "\n",
    "Some generally accepted improvements to the BERT pretraining formula:\n",
    "- **RoBERTa:** mainly just train BERT for longer and remove next sentence prediction!\n",
    "- **SpanBERT:** masking contiguous spans of words makes a harder, more useful pretraining task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/roberta.png\" width='70%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3) Encoder-Decoder\n",
    "\n",
    "The best of both worlds: Good parts of decoders and encoders?  What‚Äôs the best way to pretrain them?\n",
    "\n",
    "<div align='center'><img src=\"figs/e2d.png\" width=60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3) Encoder-Decoder Pretraining [T5 Model]\n",
    "\n",
    "For encoder-decoders, we could do something like language modeling, but where a prefix of every input is provided to the encoder and is not predicted.\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "h_1, \\ldots, h_T=\\operatorname{Encoder}\\left(w_1, \\ldots, w_T\\right) \\\\\n",
    "h_{T+1}, \\ldots, h_2=\\operatorname{Decoder}\\left(w_1, \\ldots, w_T, h_1, \\ldots, h_T\\right) \\\\\n",
    "y_i \\sim A h_i+b, i>T\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "\n",
    "The encoder portion benefits from\n",
    "bidirectional context; the decoder portion is\n",
    "used to train the whole model through\n",
    "language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# \n",
    "<div align='center'><img src=\"figs/T5.png\" width=80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fine-tuning approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Full Fine-tuning vs. Parameter-Efficient Fine-tuning\n",
    "\n",
    "\n",
    "- Finetuning every parameter in a pretrained model works well, but is **memory-intensive.**\n",
    "- Lightweight finetuning methods **adapt pretrained models in a constrained way**: may lead to **less overfitting** and/or **more efficient finetuning** and inference.\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/fine-tuning_00.png\" width=70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Parameter-Efficient Fine-tuning: Prefix-Tuning, Prompt tuning\n",
    "\n",
    "Prefix-Tuning **adds a prefix of parameters**, and freezes all pre-trained parameters.\n",
    "\n",
    "The prefix is processed by the model just like real words would be.\n",
    "\n",
    "**Advantage: each element of a batch at inference could run a different tuned model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/fine-tuning_01.png\" width=80%'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Parameter-Efficient Fine-tuning: Low-Rank Tuning\n",
    "\n",
    "Low-Rank Adaptation Learns a low-rank \"diff\" between the pre-trained and fine-tuned weight matrices.\n",
    "\n",
    "_Hypothesis:_ **the fine-tuning updates (diffs) produce changes that live in a low-dimensional subspace.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/fine-tuning_02.png\" width=70%'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How LLMs scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How LLMs scale?\n",
    "\n",
    "<br><div align='center'><img src=\"figs/GPT3_05.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How LLMs scale?\n",
    "\n",
    "<br><div align='center'><img src=\"figs/GPT3_06.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How LLMs scale?\n",
    "\n",
    "<br><div align='center'><img src=\"figs/GPT3_07.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How LLMs scale?\n",
    "\n",
    "<br><div align='center'><img src=\"figs/GPT3_08.png\" width='80%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evolutional Tree of LLM\n",
    "\n",
    "<br><div align='center'><img src=\"figs/tree_of_LLM.jpg\" width='80%' ></div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "key",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "rise": {
   "autolaunch": true,
   "overlay": "<div class='myheader'>Natural Language Processing<img src='../sapienza_logo.png'/></div>",
   "transition": "linear"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Summary",
   "toc_cell": false,
   "toc_position": {
    "height": "47px",
    "left": "1143px",
    "top": "173px",
    "width": "210.344px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
