{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### Multimodal NLP: Contrastive Methods, CLIP\n",
    "\n",
    "<br><br>\n",
    "Prof. Iacopo Masi and Prof. Stefano Faralli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.colheader_justify', 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "#plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "font = {'family' : 'Times',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "\n",
    "# Aux functions\n",
    "\n",
    "def plot_grid(Xs, Ys, axs=None):\n",
    "    ''' Aux function to plot a grid'''\n",
    "    t = np.arange(Xs.size) # define progression of int for indexing colormap\n",
    "    if axs:\n",
    "        axs.plot(0, 0, marker='*', color='r', linestyle='none') #plot origin\n",
    "        axs.scatter(Xs,Ys, c=t, cmap='jet', marker='.') # scatter x vs y\n",
    "        axs.axis('scaled') # axis scaled\n",
    "    else:\n",
    "        plt.plot(0, 0, marker='*', color='r', linestyle='none') #plot origin\n",
    "        plt.scatter(Xs,Ys, c=t, cmap='jet', marker='.') # scatter x vs y\n",
    "        plt.axis('scaled') # axis scaled\n",
    "        \n",
    "def linear_map(A, Xs, Ys):\n",
    "    '''Map src points with A'''\n",
    "    # [NxN,NxN] -> NxNx2 # add 3-rd axis, like adding another layer\n",
    "    src = np.stack((Xs,Ys), axis=Xs.ndim)\n",
    "    # flatten first two dimension\n",
    "    # (NN)x2\n",
    "    src_r = src.reshape(-1,src.shape[-1]) #ask reshape to keep last dimension and adjust the rest\n",
    "    # 2x2 @ 2x(NN)\n",
    "    dst = A @ src_r.T # 2xNN\n",
    "    #(NN)x2 and then reshape as NxNx2\n",
    "    dst = (dst.T).reshape(src.shape)\n",
    "    # Access X and Y\n",
    "    return dst[...,0], dst[...,1]\n",
    "\n",
    "\n",
    "def plot_points(ax, Xs, Ys, col='red', unit=None, linestyle='solid'):\n",
    "    '''Plots points'''\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, which='both')\n",
    "    ax.axhline(y=0, color='gray', linestyle=\"--\")\n",
    "    ax.axvline(x=0, color='gray',  linestyle=\"--\")\n",
    "    ax.plot(Xs, Ys, color=col)\n",
    "    if unit is None:\n",
    "        plotVectors(ax, [[0,1],[1,0]], ['gray']*2, alpha=1, linestyle=linestyle)\n",
    "    else:\n",
    "        plotVectors(ax, unit, [col]*2, alpha=1, linestyle=linestyle)\n",
    "\n",
    "def plotVectors(ax, vecs, cols, alpha=1, linestyle='solid'):\n",
    "    '''Plot set of vectors.'''\n",
    "    for i in range(len(vecs)):\n",
    "        x = np.concatenate([[0,0], vecs[i]])\n",
    "        ax.quiver([x[0]],\n",
    "                   [x[1]],\n",
    "                   [x[2]],\n",
    "                   [x[3]],\n",
    "                   angles='xy', scale_units='xy', scale=1, color=cols[i],\n",
    "                   alpha=alpha, linestyle=linestyle, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## My own latex definitions\n",
    "\n",
    "$$\\def\\mbf#1{\\mathbf{#1}}$$\n",
    "$$\\def\\bmf#1{\\boldsymbol{#1}}$$\n",
    "$$\\def\\bx{\\mbf{x}}$$\n",
    "$$\\def\\bxt#1{\\mbf{x}_{\\text{#1}}}$$\n",
    "$$\\def\\bv{\\mbf{v}}$$\n",
    "$$\\def\\bz{\\mbf{z}}$$\n",
    "$$\\def\\bmu{\\bmf{\\mu}}$$\n",
    "$$\\def\\bsigma{\\bmf{\\Sigma}}$$\n",
    "$$\\def\\Rd#1{\\in \\mathbb{R}^{#1}}$$\n",
    "$$\\def\\chain#1#2{\\frac{\\partial #1}{\\partial #2}}$$\n",
    "$$\\def\\loss{\\mathcal{L}}$$\n",
    "$$\\def\\params{\\bmf{\\theta}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This week lectures\n",
    "## - Generative vs Discriminative Models\n",
    "## - Contrastive Methods\n",
    "## - Multimodal NLP: NLP as supervision for the Visual domain\n",
    "## - CLIP\n",
    "## - unCLIP (Dall-E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This lecture material is taken from\n",
    "\n",
    "ðŸ“˜ **Mainly from research papers**\n",
    "\n",
    "### Contrastive methods\n",
    "\n",
    "- [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf) **SimCLR** (vision)\n",
    "- [Sampling Matters in Deep Embedding Learning](https://openaccess.thecvf.com/content_ICCV_2017/papers/Wu_Sampling_Matters_in_ICCV_2017_paper.pdf) **Contrastive method: geometric/non-probabilistic** (vision)\n",
    "- [Learning Visual Features from Large Weakly Supervised Data](https://arxiv.org/pdf/1511.02251.pdf) **CLIP precursor (vision/nlp)**\n",
    "- [Learning Visual Features from Large Weakly Supervised Data (slides)](https://cs.nyu.edu/~fergus/teaching/vision/9_detection_pt2.pdf) (vision/nlp)\n",
    "- [Contrastive Learning of Medical Visual Representations\n",
    "from Paired Images and Text](https://arxiv.org/pdf/2010.00747.pdf) **CLIP idea is from this (medical field)** (vision/medical)\n",
    "- [Learning Transferable Visual Models From Natural Language Supervision](http://proceedings.mlr.press/v139/radford21a/radford21a.pdf) **CLIP paper** (vision/nlp)\n",
    "- [Hierarchical Text-Conditional\n",
    "Image Generation with CLIP Latents](https://arxiv.org/pdf/2204.06125.pdf) **unCLIP Dalle-E** (vision/nlp)\n",
    "\n",
    "### Non-contrastive methods\n",
    "- [Exploring Simple Siamese Representation Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.pdf) **SimSiam** (vision)\n",
    "- [Bootstrap Your Own Latent\n",
    "A New Approach to Self-Supervised Learning](https://arxiv.org/pdf/2006.07733.pdf) **BYOL** (vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative vs Discriminative Models in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Learning from complex distributions is in common to vision/NLP (curse of dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hide_input": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vision: high-dimensional, continuous data\n",
    "\n",
    "$$ p(\\mbf{x} )$$\n",
    "<br/>\n",
    "<div align='center'><img src=\"figs/noise.png\" width='70%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# NLP: combinatorial/compositional problem of discrete symbols\n",
    "\n",
    "$$ p(w_1, \\ldots, w_t) $$\n",
    "```\n",
    "pot and enough deep secret rat thunder black industry answer death material angle crime probable nation debt organization spade acid soup reward free circle west forward board bone substance parcel south scissors move window hanging needle sticky pipe table old river attack design expert```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <ins>Unknown</ins> density of the data  \n",
    "\n",
    "$p_{\\text{data}(\\bx)}$\n",
    "\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/data_density.png\" width='80%' ></div>\n",
    "\n",
    "<small>Picture from [Yang-Song Blog](https://yang-song.net/blog/2021/score/)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# <ins>Known</ins> data samples dataset\n",
    "\n",
    "$\\{\\mbf{x}_i\\} \\sim p_{\\text{data}(\\bx)}$\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/samples.png\" width='80%' ></div>\n",
    "\n",
    "<small>Picture from [Yang-Song Blog](https://yang-song.net/blog/2021/score/)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generative vs Discriminative Models\n",
    "\n",
    "**Generative models** objective is to learn an approximation of the data density given the data samples (dataset):\n",
    "\n",
    "$$ p_{\\text{data}}(\\bx) \\approx p_{\\theta}(\\bx)$$\n",
    "\n",
    "$$ p_{\\text{data}}(\\bx,y) \\approx p_{\\theta}(\\bx,y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Discriminative models** objective is to learn a decision boundary to \"separate\" data samples to perform classification. \n",
    "\n",
    "$$p(y == \\text{class}~k | \\bx)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generative\n",
    "\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/density_jem.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Discriminative\n",
    "\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/class_density_jem.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generative and Discriminative are Connected\n",
    "\n",
    "Think **\"classes\" as latent factors** in the data--the class label just \"reveals\" the latent factor. <ins>[There could be, there are others]</ins>.\n",
    "Think as \"slicing the data density\" using the latent factor (\"coloring\" the data density).\n",
    "\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/joint_density_jem.png\" width='15%' ></div>\n",
    "\n",
    "$$ p_{\\theta}(\\bx) = \\sum_{y^{\\prime}} p_{\\theta}(\\bx,y^{\\prime}) \\qquad \\text{marginalization}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ p_{\\theta}(\\bx,y) = \\underbrace{p(\\bx|y)}_{\\text{class-cond. density}}p(y) = \\underbrace{p(y|\\bx)}_{\\text{discriminative}}p(\\bx)  \\qquad \\text{product rule}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generative and Discriminative are Connected\n",
    "\n",
    "$$\\underbrace{p(y|\\bx)}_{\\text{discriminative}}p(\\bx) = p_{\\theta}(\\bx,y) =  \\underbrace{p(\\bx|y)}_{\\text{generative}}p(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\underbrace{p(y|\\bx)}_{\\text{discriminative}} = \\frac{p_{\\theta}(\\bx,y)}{p(\\bx)} =  \\frac{p(\\bx|y)p(y)}{p(\\bx)} = \\frac{p(\\bx|y)p(y)}{\\sum_{y^{\\prime}} p(\\bx|y)p(y^{\\prime})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# From Generative go Discriminative\n",
    "If you need to do just  classification (and do not need probabilities): \n",
    "\n",
    "$$\\arg\\max_{y^{\\prime}} p(y^{\\prime}|\\bx)$$\n",
    "Then you can:\n",
    "1. Model/Estimate the class conditional data density $p(\\bx|y)$ _- think of the data density \"restricted to one latent factor, a class\"._\n",
    "2. Estimate how much do you think a latent factor (class) is probable $p(y)$\n",
    "3. Classify as:\n",
    "\n",
    "$$ \\arg\\max_{y^{\\prime}} p(\\bx|y^{\\prime})p(y^{\\prime}) \\qquad \\text{Given that} \\quad p(y|\\bx) \\propto p(\\bx|y)p(y)$$\n",
    "\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/joint_density_jem.png\" width='20%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/hinton_simple_framework/all_densities.png\" width='50%' ></div>\n",
    "\n",
    "<small>Picture from [YOUR CLASSIFIER IS SECRETLY AN ENERGY BASED\n",
    "MODEL AND YOU SHOULD TREAT IT LIKE ONE](https://openreview.net/pdf?id=Hkxzx0NtDB)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where is the connection with NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Where is the connection with NLP?\n",
    "\n",
    "**LM (language modeling)** is about learning $ p(w_1, \\ldots, w_t) $ (thus **generative**) but it is implemented as **a discriminative classifier** that predicts a pmf of $w_{t+1}$ given $w_1, \\ldots, w_t$.\n",
    "\n",
    "$$ p(w_1, \\ldots, w_t)  \\rightarrow \\prod_{i=1}^T p(w_{i+1}|w_1, \\ldots, w_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Do autoregressive models exist in vision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/pixelCNN_00.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/pixelCNN_01.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/pixelCNN_02.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contrastive Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Can you remember in which part of the course we have seen a Contrastive method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Scaling word2vec with...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Negative Sampling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# word2vec with Skip-Gram at a glance\n",
    "\n",
    "... and why it can be seen as a tiny neural net.\n",
    "\n",
    "<div align='center'><img src=\"figs/word2vec_layers.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Skip-gram\n",
    "\n",
    "Instead of doing:\n",
    "\n",
    "1. **Center word vs ground-truth context embedding** $\\longrightarrow \\bmf{\\theta}_{C}[gt]\\cdot\\bmf{\\theta}_{W}[i]^T$\n",
    "2. normalize as a distribution: **all context vs center word** $\\longrightarrow \\sum_{v=1}^{V} \\exp \\big(\\bmf{\\theta}_{C}[v]\\cdot\\bmf{\\theta}_{W}[i]^T\\big)$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w_{t-1},w_{t};\\mbf{\\theta}) = \\underbrace{-\\bmf{\\theta}_{C}[gt]\\cdot\\bmf{\\theta}_{W}[i]^T}_{\\text{similarity center vs context}} + \\underbrace{\\log\\Big(\\sum_{v=1}^{V} \\exp \\big(\\bmf{\\theta}_{C}[v]\\cdot\\bmf{\\theta}_{W}[i]^T\\big)\\Big)}_{\\text{make sure it is a probability}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Two solutions to approximate the denominator\n",
    "\n",
    "\n",
    "1. **Negative sampling (Contrastive method)**\n",
    "2. Hierarchical Softmax (Tree-based solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scaling word2vec with Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/positive.png\" width='35%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/negative.png\" width='55%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Push up positive and push down negatives\n",
    "\n",
    "$$ \\min_{\\params} \\underbrace{-\\log \\sigma \\left(\\params_{C}[gt]^T\\params_W[i]\\right)}_{\\text{push up positive prob.}} - \\underbrace{\\sum_{k=1}^K \\log \\big[ \\sigma\\left(-\\params_{C}[k]^T\\params_W[i]\\right)\\big]}_{\\text{push down negative prob.}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Visualization\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/negative_params.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why we need the negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/why_negatives.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other forms of Contrastive Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Other forms of Contrastive Methods\n",
    "\n",
    "1. ~Contrastive loss implemented with logistic function and negative sampling [word2vec]~ (nlp)\n",
    "2. Contrastive loss implemented with Softmax and large mini-batches (vision)\n",
    "3. Contrastive loss with geometric interpretation and margin (vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contrastive loss implemented with Softmax and large mini-batches \n",
    "## Method name: SimCLR\n",
    "## Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/hinton_simple_framework/title.png\" width='85%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# [ImageNet](https://www.image-net.org/index.php): A [computer] vision dataset\n",
    "<br>\n",
    "\n",
    "> ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images. The project has been instrumental in advancing computer vision and deep learning research. The data is available for free to researchers for non-commercial use.\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/imagenet_banner.jpeg\" width='45%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# [ImageNet](https://www.image-net.org/index.php): A [computer] vision dataset\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/imagenet_samples.png\" width='65%' ></div>\n",
    "\n",
    "<small>Source: Ye, Tengqi. 2018. \"Visual Object Detection from Lifelogs using Visual Non-lifelog Data.\" Researchgate, January. Accessed 2019-06-20. </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Performance: Supervised vs Self-Supervised\n",
    "\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/performance.png\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Take away from the paper\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/takeaway.png\" width='45%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Main Idea\n",
    "\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/idea.png\" width='45%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# SimCLR Contrastive Loss\n",
    "<br><br>\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/loss_1.png\" width='100%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Normalized Temperature-scaled Cross-Entropy Loss\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/loss_2.png\" width='100%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Data Augmentation\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/data_aug.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Data Augmentation: A way to perturb the original data\n",
    "## Create new, meaningful samples that you may find in the <ins>original data distribution</ins>\n",
    "Red box = actually used for training\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/data_aug_2.png?1\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Comparison of loss functions\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/loss_comparison.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Comparison of loss functions\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/loss_comparison_b.png?1\" width='40%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Comparison of loss functions\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/loss_comparison_c.png?1\" width='90%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# SimCLR: main algorithm\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/algo.png?1\" width='35%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results: Linear classifier on top of SimCLR representation\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/exp_a.png\" width='50%' ></div>\n",
    "<small>$\\times 4$ is a \"depth width\" multiplier of 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results: Models trained with few labels\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/exp_b.png\" width='50%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results: Results across datasets\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/hinton_simple_framework/exp_c.png\" width='85%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Other forms of Contrastive Methods\n",
    "\n",
    "1. ~Contrastive loss implemented with logistic function and negative sampling [word2vec]~ (nlp)\n",
    "2. ~Contrastive loss implemented with Softmax and large mini-batches (vision)~\n",
    "3. Contrastive loss with geometric interpretation and margin (vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contrastive loss with geometric interpretation and margins (vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/sampling_matters_smola/title.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Net as an embedding function\n",
    "\n",
    "Let $f_{\\theta}(\\bx_i)$ be an embedding of a high-dimensional data point (in case of images) $\\bx_i \\in \\mathbb{R}^N$ where $f : \\mathbb{R}^N \\mapsto \\mathbb{R}^D$ is a differentiable deep network with parameters $\\theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Geometric idea\n",
    "\n",
    "Our goal is to learn an embedding that **keeps similar data points close, while pushing dissimilar datapoints apart**. Note distance in measured in the embedding space.\n",
    "\n",
    "$$ D_{ij}= \\vert\\vert f(\\bx_i) - f(\\bx_j) \\vert\\vert_2 $$\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/deep_embedding.png\" width='35%' ></div>\n",
    "\n",
    "For any **positive pair** of datapoints:\n",
    "- $y_{ij} = 1$ this distance should be small\n",
    "- $y_{ij} = 0$  it should be large if negative pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Contrastive Loss\n",
    "\n",
    "The contrastive loss directly optimizes this distance by:\n",
    "- encouraging **all positive distances to approach 0**\n",
    "- while **keeping negative distances above a certain threshold** $\\alpha$\n",
    "<br><br>\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/contrastive_loss.png?1\" width='95%' ></div>\n",
    "\n",
    "**where** **$[\\cdot]_{+}=\\max(0,\\cdot)$**\n",
    "\n",
    "Right part of the math reads as if you are already far away of at least $\\alpha$ then **do not pay a penalty.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br><div align='center'><img src=\"figs/word2vec_contrastive.png?1\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Contrastive loss graph (loss in function of distance)\n",
    "The solid blue lines show the loss function for positive pairs, the dotted green for\n",
    "negative pairs. \n",
    "<br><div align='center'><img src=\"figs/sampling_matters_smola/contrastive_loss_graph.png\" width='35%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Contrastive loss graph (in the embedding space)\n",
    "\n",
    "<br><div align='center'><img src=\"figs/sampling_matters_smola/contrastive_loss_graph_2.png\" width='35%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Contrastive loss limitations\n",
    "\n",
    "- One drawback of the contrastive loss is that **we have to select a <ins>constant margin $\\alpha$ for all pairs of negative samples.**</ins> This implies that visually diverse classes are embedded in the same small space as visually similar ones. The embedding space does not allow for distortions.\n",
    "\n",
    "- **Scale Quadratically** with the sampling of points.\n",
    "    - this is why `SimCLR` works better with large batch size $\\rightarrow$ you have more negative training samples to constrain the search space\n",
    "    - `SimCLR` takes negative samples from the large mini-batch (**negative samples are the denominator in SimCLR loss)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Triplet Loss\n",
    "\n",
    "**Idea:** Take relative comparisons between pairs, not just consider the single pairs.\n",
    "\n",
    "- So we have to have **two pairs (a positive pair, and a negative pair).**\n",
    "- Keep **all positives closer to any negatives**\n",
    "\n",
    "This formulation allows the embedding space to be arbitrarily distorted and does not impose a constant margin $\\alpha$.\n",
    "\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/triplet_loss.png?1\" width='35%' ></div>\n",
    "\n",
    "<small>If not wrong, it was \"invented\" at USC</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Two Pairs\n",
    "\n",
    "Although we have two pairs, we have **\"3 actors\"** in total (**3 embeddings**):\n",
    "\n",
    "$$\\bx_p \\quad \\text{positive} \\qquad \\bx_a \\quad \\text{anchor} \\qquad  \\bx_n \\quad \\text{negative}$$\n",
    "\n",
    "- A $\\bx_p$ an embedding paired with another positive embedding $\\bx_a$\n",
    "    - Call $p$ the positive and $a$ (which is another positive), **the anchor.**\n",
    "    - The assumption is that the latent factor of $\\bx_p$ is equal latent factor of $\\bx_a$.\n",
    "- Then select a negative as the pair $\\bx_a$ wrt to another negative point $\\bx_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Triplet Loss\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/good_config_triplet.png\" width='95%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Triplet Loss\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/bad_config_triplet.png\" width='95%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Triplet loss (relative margin)\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/triplet_loss_margin.png\" width='55%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Triplet loss Limitations\n",
    "\n",
    "- Now you have to sample **Triplet!** **<ins>Cubic</ins> sampling complexity**\n",
    "\n",
    "- Moreover, once the network converges, most samples contribute in a minor way as\n",
    "  very few of the negative margins are violated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Hacks:\n",
    "\n",
    "For the contrastive loss, hard negative mining usually offers faster convergence. For the triplet\n",
    "loss, it is less obvious, as hard negative mining often leads to collapsed models, i.e. all images have the same embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Distance distribution in high-dimension\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/distance_high_dim.png\" width='55%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/distance_dist_1.png\" width='90%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/distance_dist_2.png\" width='90%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# We do inverse transform sampling on $q^{-1}$\n",
    "\n",
    "Instead of sampling uniformly and be likely to get $\\sqrt(2)$-away points, we sample uniformly but according to the distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/dist_weight.png\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/dist_weight_fig.png\" width='95%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Double-margin Contrastive Loss\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/double_margin.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Margin-based Contrastive loss\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/margin-based.png\" width='45%' ></div>\n",
    "<small><b>Note:</b> They are learning $\\beta$!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss at a glance\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/sampling_matters_smola/final-plot.png\" width='95%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Toward CLIP (Contrastive Language-Image Pre-Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/learn_from_weak_sup_maaten/title.png\" width='95%' ></div>\n",
    "\n",
    "<small>**Note:** this paper is before Transformers. Circa 2015.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Idea: Instead of Self-Supervised..... \"Text\" Supervised\n",
    "\n",
    "\n",
    "> In this paper, we explore the potential of leveraging massive, weakly labeled image collections for learning good visual features.\n",
    "\n",
    "\n",
    "- We train convolutional networks on a dataset of 100 million\n",
    "Flickr photos and captions, and show that these networks\n",
    "produce features that **perform well in a range of vision problems**\n",
    "-  We also show that the networks appropriately capture **word similarity**, and learn correspondences between different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Image Classification\n",
    "## Note the discriminative model: $p(y|x)$\n",
    "\n",
    "Given $\\bx$, compute a distribution over the labels $p(y|\\bx)$.\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/cnn.png?1\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Problem: Who is going to label this image as \"plane\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Use Supervision from the Web [Weak Supervision]\n",
    "\n",
    "Research question: \n",
    "> Can we learn high quality visual features from scratch without using any fully\n",
    "supervised data?\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/sup_web.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " **Flickr 100M dataset** contains ~100M photos with associated \"captions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weakly Supervised Image classification\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/bow_loss_1.png\" width='85%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weakly Supervised Image classification\n",
    "\n",
    "- We treat each individual word in a photo's caption as **a target for that photo**\n",
    "- That is: a **multi-label learning problem with extremely noise labels**\n",
    "- We map an image $\\bx$ to multiple labels, not a single $y$.\n",
    "\n",
    "<div align='center'><img src=\"figs/clip/bow_loss_2.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Text preprocessing\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/text_prep.png?1\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/clip/supervision_1.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br/><div align='center'><img src=\"figs/clip/supervision_2.png\" width='95%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-label Classification problem\n",
    "\n",
    "**Note:** each word token now is **NOT mutually exclusive** as in standard classification.\n",
    "i.e. if you sum the label vector you do not get unit mass (it is not a probability anymore).\n",
    "Yet you get the number of word token in the caption.\n",
    "\n",
    "<div align='center'><img src=\"figs/clip/supervision_3.png\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss Function\n",
    "\n",
    "Note $N$ is the number of samples and $K$ is how many words you have in the bag.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='center'><img src=\"figs/clip/multi_class.png\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss Function\n",
    "\n",
    "Note $N$ is the number of samples and $K$ is how many words you have in the bag.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='center'><img src=\"figs/clip/binary_class.png\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/learn_from_weak_sup_maaten/embed_2015.png\" width='85%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/title_1.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  Training pairs\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/ConVIRT_samples.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  ConVIRT: Contrastive Visual Representation Learning from Text\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/ConVIRT_img.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  ConVIRT: Contrastive Visual Representation Learning from Text\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/ConVIRT_loss.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  ConVIRT: Contrastive Visual Representation Learning from Text\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/ConVIRT_loss_2.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  ConVIRT: Realization\n",
    "\n",
    "### Visual part: ResNet-50 Encoder\n",
    "\n",
    "Sequential applications of five random transformations: cropping, horizontal flipping, affine transformation, color jittering and Gaussian blur.\n",
    "\n",
    "### NLP part: BERT encoder\n",
    "\n",
    "BERT Encoder followed by a max-pooling layer over all output vectors. \n",
    "We initialize our encoder with the **ClinicalBERT weights** (Alsentzer et al., 2019) pretrained on the MIMIC clinical notes, which achieved state-of-the-art performance on a suite of clinical NLP tasks.\n",
    "\n",
    "We apply a simple uniform sampling of a sentence from the input document $\\mbf{u}$ (i.e.,  $\\mbf{u}$  is a randomly sampled sentence from $\\mbf{u}$  for each minibatch). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The CLIP paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/title_2.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Claim\n",
    "> We\n",
    "demonstrate that a simplified version of ConVIRT trained\n",
    "from scratch, which we call CLIP, for Contrastive Language-\n",
    "Image Pre-training, is an efficient and scalable method of\n",
    "learning from natural language supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "> CLIP learns to perform a wide set of tasks during pre-\n",
    "training including OCR, geo-localization, action recognition, and outperforms the best publicly available ImageNet\n",
    "model while being more computationally efficient. We also\n",
    "find that zero-shot CLIP models are much more robust than\n",
    "equivalent accuracy supervised ImageNet models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  Approach\n",
    "\n",
    "**Learning perception** from the **supervision contained in natural language paired with images.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Huge dataset\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/clip_dataset.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Caption Prediction does not work\n",
    "\n",
    "> Our initial approach, similar to VirTex, jointly trained an\n",
    "image CNN and text transformer from scratch to predict\n",
    "the caption of an image\n",
    "\n",
    "**Note:** BoW is the 2015 paper we reviewed so far.\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/caption_pred_do_not_work.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Details\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/clip/details_01.png\" width='95%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "<div align='center'><img src=\"figs/clip/details_02.png\" width='95%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# CLIP Training and Inference\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/clip_idea.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# CLIP Classification: Text encoder generates classification \"weights\"\n",
    "\n",
    "Text encoder acts as an **on-the-fly generator of classification weights**\n",
    "\n",
    "$$ p(y|\\mbf{x}) \\propto \\mbf{E}_y\\mbf{e}_x$$\n",
    "\n",
    "The image embedding is given $\\mbf{e}_x$. $\\forall$ class $y$, we encode the class in the text $y$ and \"fill\" a column matrix in $\\mbf{E}_y$. Then we classify as:\n",
    "\n",
    "$$\\arg\\max_{y} \\mbf{E}_y\\mbf{e}_x $$\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/clip_idea.png\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# CLIP Training Code\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/clip_training_code.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CLIP Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Zero Short Performance\n",
    "CLIP with transformers to \"encode\" the classifier vs logistic regression trained on ResNet-features.\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/zero-shot.png\" width='35%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Zero Short Performance\n",
    "First,\n",
    "CLIPâ€™s zero-shot classifier is **generated via natural language\n",
    "which allows for visual concepts to be directly specified\n",
    "(â€œcommunicatedâ€)**. By contrast, â€œnormalâ€ supervised learn-\n",
    "ing must infer concepts **indirectly from training examples**.\n",
    "Context-less example-based learning has the drawback that\n",
    "**many different hypotheses can be consistent with the data**,\n",
    "especially in the one-shot case. A single image often con-\n",
    "tains many different visual concepts\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/zero-shot-1.png\" width='35%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Representation Learning: Distributional Shifts\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/clip/distributional-shifts.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# unCLIP (DALLÂ·E 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/dalle2/dalle2.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/dalle2/dalle2_title.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Method\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/dalle2/method_00.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Method\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/dalle2/method_01.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/dalle2/method_02.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/dalle2/method.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Diffusion Models\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/diffusion/Blausen_0315_Diffusion.png\" width='75%' ></div>\n",
    "\n",
    "<small>Based on https://cvpr2022-tutorial-diffusion-models.github.io/</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Map structured data to white noise\n",
    "\n",
    "<div align='center'><img src=\"figs/diffusion/forward.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# From white noise learn $\\mbf{\\theta}$ to go back to structured data\n",
    "\n",
    "<div align='center'><img src=\"figs/diffusion/backward.png\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Learning to transform noise to \"meaningful data\" is generative modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Even GANs map noise to data\n",
    "\n",
    "<div align='center'><img src=\"figs/diffusion/GANs.png\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Data distribution (unknown)\n",
    "\n",
    "$$ q(\\mbf{x}_0) \\quad \\text{but we have samples from the training set} \\quad  \\mbf{x}_0 \\sim {q(\\mbf{x})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ {q(\\mbf{x}}_0) \\longrightarrow \\mathcal{N}(0,\\mbf{I}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Data perturbation process parametrized by $\\beta$\n",
    "\n",
    "Given what we have $\\mbf{x}_0$, we define a distribution over the perturbed output at the next step:\n",
    "\n",
    "$$ \\mbf{x}_1 \\sim  q(\\mbf{x}_1|\\mbf{x}_0) = \\mathcal{N}(\\sqrt{1-\\beta_1}~\\mbf{x}_0,\\beta_1 \\mbf{I})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Read it as generate data centered on $\\mbf{x}_0$ scaled by $\\sqrt(..)$ with variance $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Go recursive\n",
    "\n",
    "$$ \\mbf{x}_t \\sim q(\\mbf{x}_t|\\mbf{x}_{t-1}) =\\mathcal{N}(\\sqrt{1-\\beta_t}~\\mbf{x}_{t-1},\\beta_t \\mbf{I})$$\n",
    "\n",
    "This means we applied the chain with $T$ time steps: $$\\mbf{x}_0 \\rightarrow \\beta_1 \\ldots  \\rightarrow \\beta_T  \\rightarrow q(\\mbf{x}_0,\\ldots,\\mbf{x}_T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Joint Distribution is the marginal times the conditional\n",
    "\n",
    "$$ q(\\mbf{x}_0,\\ldots,\\mbf{x}_T) = q(\\mbf{x}_0)\\prod_{t=1}^T q(\\mbf{x}_t|\\mbf{x}_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward Diffusion Process - Image Domain\n",
    "\n",
    "$$ q(\\mbf{x}_0,\\ldots,\\mbf{x}_T) = q(\\mbf{x}_0)\\prod_{t=1}^T q(\\mbf{x}_t|\\mbf{x}_{t-1})$$\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/diffusion/fwd_image.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward Diffusion Process - Diffusion Kernel\n",
    "\n",
    "We can \"bypass\" time steps and write: $$\\alpha_t = \\prod_{s=1}^t (1-\\beta_t).$$\n",
    "Note $\\beta_t \\approx \\mathtt{1e-2}$\n",
    "$$ q(\\mbf{x}_t|\\mbf{x}_0) = \\mathcal{N}\\big(\\sqrt{\\alpha_t}~\\mbf{x}_{t-1},(1-\\alpha_t) \\mbf{I}\\big) \\quad \\text{diffusion kernel}$$\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/diffusion/diffusion_kernel.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward Diffusion Process - Sampling\n",
    "\n",
    "$$ q(\\mbf{x}_t|\\mbf{x}_0) = \\mathcal{N}\\big(\\sqrt{\\alpha_t}~\\mbf{x}_{t-1},(1-\\alpha_t) \\mbf{I}\\big) \\quad \\text{diffusion kernel}$$\n",
    "\n",
    "Sample $\\epsilon \\sim  \\mathcal{N}\\big(0,\\mbf{I})$ and then $$\\mbf{x}_t = \\sqrt{\\alpha_t}~\\mbf{x}_{t-1} +  {(1-\\alpha_t)}\\epsilon$$\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/diffusion/diffusion_kernel.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward Diffusion Process - $q(\\mbf{x}_t|\\mbf{x}_0) \\approx \\mathcal{N}(0,\\mbf{I})$\n",
    "\n",
    "The $\\beta_t$ value is designed such that $\\alpha_T \\mapsto 0$ and then $q(\\mbf{x}_t|\\mbf{x}_0) \\approx \\mathcal{N}(0,\\mbf{I})$\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/diffusion/diffusion_kernel.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What happens to data distribution in the forward process\n",
    "\n",
    "$$\n",
    "\\underbrace{q\\left(\\mathbf{x}_t\\right)}_{\\begin{array}{c}\n",
    "\\text { Diffused } \\\\\n",
    "\\text { data dist. }\n",
    "\\end{array}}=\\int \\underbrace{q\\left(\\mathbf{x}_0, \\mathbf{x}_t\\right)}_{\\begin{array}{c}\n",
    "\\text { Joint } \\\\\n",
    "\\text { dist. }\n",
    "\\end{array}} d \\mathbf{x}_0=\\int \\underbrace{q\\left(\\mathbf{x}_0\\right)}_{\\begin{array}{c}\n",
    "\\text { Input } \\\\\n",
    "\\text { data dist. }\n",
    "\\end{array}} q \\underbrace{\\left(\\mathbf{x}_t \\mid \\mathbf{x}_0\\right)}_{\\begin{array}{c}\n",
    "\\text { Diffusion } \\\\\n",
    "\\text { kernel }\n",
    "\\end{array}} d \\mathbf{x}_0\n",
    "$$\n",
    "<br>\n",
    "Gaussian Smoothing of the unknown data distribution\n",
    "<div align='center'><img src=\"figs/diffusion/diffuse_data_dist.png\" width='65%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass is ancestor sampling\n",
    "\n",
    "Similar to sampling in GMM (i.e. sample which gaussian and then sample from the gaussian).\n",
    "Here we aim at sampling from $\\mbf{x}_t \\sim q(\\mbf{x}_t)$:\n",
    "\n",
    "1. We sample $\\mbf{x}_0 \\sim q(\\mbf{x})$ (take a data point from training set)\n",
    "2. Given $\\mbf{x}_0$, we sample $\\mbf{x}_t \\sim q(\\mbf{x}_t|\\mbf{x}_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generative Learning by Denoising\n",
    "## Reversing the forward diffusion process\n",
    "\n",
    "The generative distribution will be trained to describe the same trajectory, but in reverse:\n",
    "\n",
    "$$\n",
    "p\\left(\\mbf{x}_{0} \\cdots, \\mbf{x}_{T}\\right)=p\\left(\\mathbf{x}_{T}\\right) \\prod_{t=1}^T p\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)\n",
    "$$\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/diffusion/diffuse_reverse.png\" width='65%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reverse Generation\n",
    "\n",
    "\n",
    "\n",
    "- Sample $\\mbf{x}_{T} \\sim \\mathcal{N}(0;\\mbf{I})$\n",
    "\n",
    "- Iteratively sample $\\mbf{x}_{t-1} \\sim q(\\mbf{x}_{t-1}|\\mbf{x}_{t})$\n",
    "\n",
    "\n",
    "$q(\\mbf{x}_{t-1}|\\mbf{x}_{t})$ true denoising distribution is intractable unless $\\beta_t$ is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/diffusion/diffuse_reverse.png\" width='95%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# We train a model to approximate $q(\\mbf{x}_{t-1}|\\mbf{x}_{t})$\n",
    "\n",
    "\n",
    "If we assume $\\beta_t \\approx 0$ (very small), then the reverse process is also **Gaussian** where <ins>the mean is parametrized by the weights of a neural networks.</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reverse Denoising Process\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/diffusion/reverse_den_learnable.png\" width='95%' ></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "key",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "rise": {
   "autolaunch": true,
   "overlay": "<div class='myheader'>Natural Language Processing<img src='../sapienza_logo.png'/></div>",
   "transition": "linear"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Summary",
   "toc_cell": false,
   "toc_position": {
    "height": "47px",
    "left": "1143px",
    "top": "173px",
    "width": "210.344px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
