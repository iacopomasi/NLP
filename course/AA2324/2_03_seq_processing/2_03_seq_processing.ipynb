{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### Deep Learning for Sequence Processing\n",
    "<br><br>\n",
    "Prof. Iacopo Masi and Prof. Stefano Faralli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.colheader_justify', 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "#plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "font = {'family' : 'Times',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "\n",
    "# Aux functions\n",
    "\n",
    "def plot_grid(Xs, Ys, axs=None):\n",
    "    ''' Aux function to plot a grid'''\n",
    "    t = np.arange(Xs.size) # define progression of int for indexing colormap\n",
    "    if axs:\n",
    "        axs.plot(0, 0, marker='*', color='r', linestyle='none') #plot origin\n",
    "        axs.scatter(Xs,Ys, c=t, cmap='jet', marker='.') # scatter x vs y\n",
    "        axs.axis('scaled') # axis scaled\n",
    "    else:\n",
    "        plt.plot(0, 0, marker='*', color='r', linestyle='none') #plot origin\n",
    "        plt.scatter(Xs,Ys, c=t, cmap='jet', marker='.') # scatter x vs y\n",
    "        plt.axis('scaled') # axis scaled\n",
    "        \n",
    "def linear_map(A, Xs, Ys):\n",
    "    '''Map src points with A'''\n",
    "    # [NxN,NxN] -> NxNx2 # add 3-rd axis, like adding another layer\n",
    "    src = np.stack((Xs,Ys), axis=Xs.ndim)\n",
    "    # flatten first two dimension\n",
    "    # (NN)x2\n",
    "    src_r = src.reshape(-1,src.shape[-1]) #ask reshape to keep last dimension and adjust the rest\n",
    "    # 2x2 @ 2x(NN)\n",
    "    dst = A @ src_r.T # 2xNN\n",
    "    #(NN)x2 and then reshape as NxNx2\n",
    "    dst = (dst.T).reshape(src.shape)\n",
    "    # Access X and Y\n",
    "    return dst[...,0], dst[...,1]\n",
    "\n",
    "\n",
    "def plot_points(ax, Xs, Ys, col='red', unit=None, linestyle='solid'):\n",
    "    '''Plots points'''\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, which='both')\n",
    "    ax.axhline(y=0, color='gray', linestyle=\"--\")\n",
    "    ax.axvline(x=0, color='gray',  linestyle=\"--\")\n",
    "    ax.plot(Xs, Ys, color=col)\n",
    "    if unit is None:\n",
    "        plotVectors(ax, [[0,1],[1,0]], ['gray']*2, alpha=1, linestyle=linestyle)\n",
    "    else:\n",
    "        plotVectors(ax, unit, [col]*2, alpha=1, linestyle=linestyle)\n",
    "\n",
    "def plotVectors(ax, vecs, cols, alpha=1, linestyle='solid'):\n",
    "    '''Plot set of vectors.'''\n",
    "    for i in range(len(vecs)):\n",
    "        x = np.concatenate([[0,0], vecs[i]])\n",
    "        ax.quiver([x[0]],\n",
    "                   [x[1]],\n",
    "                   [x[2]],\n",
    "                   [x[3]],\n",
    "                   angles='xy', scale_units='xy', scale=1, color=cols[i],\n",
    "                   alpha=alpha, linestyle=linestyle, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## My own latex definitions\n",
    "\n",
    "$$\\def\\mbf#1{\\mathbf{#1}}$$\n",
    "$$\\def\\bmf#1{\\boldsymbol{#1}}$$\n",
    "$$\\def\\bx{\\mbf{x}}$$\n",
    "$$\\def\\bxt#1{\\mbf{x}_{\\text{#1}}}$$\n",
    "$$\\def\\bv{\\mbf{v}}$$\n",
    "$$\\def\\bz{\\mbf{z}}$$\n",
    "$$\\def\\bmu{\\bmf{\\mu}}$$\n",
    "$$\\def\\bsigma{\\bmf{\\Sigma}}$$\n",
    "$$\\def\\Rd#1{\\in \\mathbb{R}^{#1}}$$\n",
    "$$\\def\\chain#1#2{\\frac{\\partial #1}{\\partial #2}}$$\n",
    "$$\\def\\loss{\\mathcal{L}}$$\n",
    "$$\\def\\params{\\bmf{\\theta}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's lecture\n",
    "## - Recap on Language Models\n",
    "### - From Feedforward Nets to Recurrent Neural Nets (RNN)\n",
    "### - RNN Application: Parts of Speech (POS) as Sequence Labeling\n",
    "## - RNN Application: Text Generation as Autoregressive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This lecture material is taken from\n",
    "üìò **Chapter 9 Jurafsky Book**\n",
    "\n",
    "üìò **Chapter 6.3 Eisenstein Book**\n",
    "- [Stanford Slide RNN](http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture05-rnnlm.pdf)\n",
    "- [Stanford Lecture RNN](https://www.youtube.com/watch?v=0LixFSa7yts&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&index=6)\n",
    "- [Stanford Notes on Word2Vec](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf)\n",
    "- [Andrej Karpathy Lecture on RNN](https://www.youtube.com/watch?v=yCC09vCHzF8)\n",
    "- [Andrej Karpathy Slides on RNN](http://cs231n.stanford.edu/slides/2022/lecture_10_ruohan.pdf)\n",
    "\n",
    "Another resource with code is [[d2l.ai] RNN](https://d2l.ai/chapter_recurrent-neural-networks/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning for Sequence Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Language Model (LM)\n",
    "\n",
    "**Language Modeling (LM)** is the task of predicting what word comes next:\n",
    "\n",
    "> the students opened their _________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "books, laptops, exams, minds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Language Model: the order matters!\n",
    "\n",
    "> the cat is small\n",
    "\n",
    "| 0   | 1   | 2  | 3     |\n",
    "|-----|-----|----|-------|\n",
    "| the | cat | is | small |\n",
    "\n",
    "$$p(\\text{the}, \\text{cat}, \\text{is}, \\text{small}) = 0.035$$\n",
    "\n",
    ">small the is cat\n",
    "\n",
    "| 0     | 1   | 2   | 3  |\n",
    "|-------|-----|-----|----|\n",
    "| small | the | cat | is |\n",
    "\n",
    "$$p(\\text{small}, \\text{the}, \\text{cat}, \\text{is}) = 0.000001$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Language Model (LM)\n",
    "More formally, given a sequence of words $w_1,\\ldots,w_t$, compute the probabilities distributions over the text words $w_{t+1}$ where the support of the probability is a vocabulary $V$.\n",
    "\n",
    "$$ p(w_{t+1}|w_t,\\ldots,w_1)$$\n",
    "\n",
    "where $w_{t+1}$ can be any word in $V = \\{w_1,\\ldots,_{|V|}\\}$\n",
    "\n",
    "A system that does this is called a **Language Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Language Model (LM)\n",
    "\n",
    "You can also think a LM as a **machinery that assign a probability to a piece of text of T words:**\n",
    "\n",
    "$$ p(w_t,\\ldots,w_1) = \\underbrace{p(w_t,\\ldots,w_2|w_1)}_{\\text{recursion}} \\cdot \\underbrace{p(w_1)}_{\\text{base case}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ = p(w_t,\\ldots,w_2|w_1) \\cdot p(w_1) = p(w_t,\\ldots,w_3|w_1,w_2)\\cdot p(w_2|w_1) \\cdot p(w_1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$= p(w_t,\\ldots,w_3|w_1,w_2)\\cdot p(w_2|w_1) \\cdot p(w_1) = \\prod_{t=2}^N p(w_{t}|w_{t-1},\\ldots,w_1) \\cdot p(w_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LM can be interpreted as an Autoregressive Model\n",
    "\n",
    "Such models that regress the value of a signal **on the previous values of that same signal** are naturally called **autoregressive models.**\n",
    "\n",
    "$$ p(w_{t+1}|w_t,\\ldots,w_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What LM can do\n",
    "\n",
    "$$ p(w_{t+1}|w_t,\\ldots,w_1)$$\n",
    "\n",
    "Language models prove useful for all sorts of reasons. \n",
    "\n",
    "- Evaluate the the **likelihood of sentences**\n",
    "    - For example, we might wish to compare the naturalness of two candidate outputs generated by a machine translation system or by a speech recognition system. \n",
    "- Ability to sample sequences, and even to optimize for the most likely sequences. Act as **Generators**.\n",
    "- LM are not **ONLY generators (samplers!)** yet are statistical tools that models joint density of words where word order matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Language Model: the order matters!\n",
    "\n",
    "> the cat is small\n",
    "\n",
    "| 0   | 1   | 2  | 3     |\n",
    "|-----|-----|----|-------|\n",
    "| the | cat | is | small |\n",
    "\n",
    "$$p(\\text{the}, \\text{cat}, \\text{is}, \\text{small}) = 0.635$$\n",
    "\n",
    ">small the is cat\n",
    "\n",
    "| 0     | 1   | 2   | 3  |\n",
    "|-------|-----|-----|----|\n",
    "| small | the | cat | is |\n",
    "\n",
    "$$p(\\text{the}, \\text{cat}, \\text{is}, \\text{small}) = 0.0034$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A fixed-window Neural LM (Markovian Assumption)\n",
    "\n",
    "$$ p(w_{t+1}|w_t,\\ldots,w_1) \\approx p(w_{t+1}|w_t,\\ldots,w_{t-N})$$\n",
    "\n",
    "The length of the context $N$ impacts the LM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Length of the Context matters! (Markovian Assumption)\n",
    "\n",
    "**Language Modeling (LM)** is the task of predicting what word comes next:\n",
    "\n",
    "> the students opened their _________\n",
    "\n",
    "What is your prediction here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> As the proctor started the clock, the students opened their _________\n",
    "\n",
    "What is your prediction here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A fixed-window Neural LM\n",
    "\n",
    "<br><div align='center'><img src=\"figs/feedfNN.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A fixed-window Neural LM\n",
    "\n",
    "‚úÖ Improvements over n-gram LM \n",
    "- No sparsity problem\n",
    "- No need to store all observed n-grams!\n",
    "\n",
    "‚ùå Remaining problems:\n",
    "- Fixed window is too small\n",
    "- Enlarging window enlarges $\\mbf{W}$\n",
    "- Window can never be large enough!\n",
    "- No symmetry in how the inputs are processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence Modeling till now\n",
    "<div align='center'><img src=\"figs/one_to_one.png\" width='10%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence Modeling: where we want to go\n",
    "\n",
    "<div align='center'><img src=\"figs/one_many_to_one.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence Modeling\n",
    "\n",
    "<div align='center'><img src=\"figs/many_to_one.png?1\" width='20%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNN) for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN: basic principle\n",
    "\n",
    "- We still use Markovian assumption to consider only $N$ previous steps in time\n",
    "- Yet we do not model explicitly $p(w_t \\mid w_{t-1}, \\ldots, w_{t-N+1})$, instead we use a **latent variable model:**\n",
    "\n",
    "$$p(w_t \\mid w_{t-1}, \\ldots, w_1) \\approx p(w_t \\mid \\mbf{h}_{t-1}),$$\n",
    "\n",
    "where $\\mbf{h}_{t-1}$ is a hidden state that **\"summarizes\"** the sequence information up to time step $t-1$ . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN: hidden state\n",
    "\n",
    "$$p(w_t \\mid w_{t-1}, \\ldots, w_1) \\approx p(w_t \\mid \\mbf{h}_{t-1}),$$\n",
    "\n",
    "where $\\mbf{h}_{t-1}$ is a *hidden state* that stores the sequence\n",
    "information up to time step $t-1$. \n",
    "\n",
    "In general, the hidden state at\n",
    "any time step $t$ could be computed based on both the current\n",
    "input $w_{t}$ and the previous hidden state $\\mbf{h}_{t-1}$:\n",
    "\n",
    "$$ \\mbf{h}_t = f(w_{t}, \\mbf{h}_{t-1}) $$\n",
    "\n",
    "Informal: think $\\mbf{h}_{t}$ as a sort of memory of what happened so far up to $t-1$ plus the current input $w_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Hidden layer vs Hidden states\n",
    "\n",
    "Recall that we have discussed hidden layers with hidden units in **window-based Neural LM.** \n",
    "\n",
    "It is noteworthy that hidden layers and hidden states refer to two very different concepts. \n",
    "\n",
    "- Hidden layers are, as explained, layers that are hidden from view on the path from input to output. \n",
    "\n",
    "- Hidden states are technically speaking *inputs* to whatever we do at a given step, and they can only be computed by looking at data at previous time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence Modeling\n",
    "\n",
    "<div align='center'><img src=\"figs/many_to_one_01.png?4\" width='20%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How do we implement $ f(w_{t}, \\mbf{h}_{t-1}) $ ?\n",
    "\n",
    "Assume $\\mbf{e}_t$ is the word embedding of word token at time $t$ and $w_t$ corresponds to the one-hot encoding of the word, as $\\mbf{e}_t=\\bmf{\\theta}w_t$. Then:\n",
    "\n",
    "$$f(w_{t}, \\mbf{h}_{t-1}) \\doteq \\sigma \\big( \\mbf{W}_h\\mbf{h}_{t-1}+ \\mbf{W}_e\\mbf{e}_{t} +\\mbf{b}\\big)$$\n",
    "\n",
    "This formulation is called **Elman Networks [Elman, 1990] or simple recurrent neural networks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How do we implement $ f(w_{t}, \\mbf{h}_{t-1}) $ ?\n",
    "\n",
    "Assume $\\mbf{e}_t$ is the word embedding of word token at time $t$ and $w_t$ corresponds to the one-hot encoding of the word, as $\\mbf{e}_t=\\bmf{\\theta}w_t$. Then:\n",
    "\n",
    "$$f(w_{t}, \\mbf{h}_{t-1}) \\doteq \\sigma \\big( \\underbrace{\\mbf{W}_h\\mbf{h}_{t-1}}_{\\text{from prev. steps}}+ \\underbrace{\\mbf{W}_e\\mbf{e}_{t}}_{\\text{current input}} +\\mbf{b}\\big)$$\n",
    "\n",
    "where $\\sigma(\\cdot)$ is a non-linear activation function such as _hyperbolic tan or sigmoid_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence Modeling: something is missing here\n",
    "## Can you spot what?\n",
    "\n",
    "<div align='center'><img src=\"figs/many_to_one_01.png?4\" width='15%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence Modeling: something is missing here\n",
    "## Can you spot what? The base case: Starting hidden state!\n",
    "\n",
    "<div align='center'><img src=\"figs/many_to_one_02.png?4\" width='20%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sequence Modeling: Something is missing here, can you spot what?\n",
    "\n",
    "You need the \"base case\", the starting hidden state, usually $\\mbf{h}_0$ is set to all zeros.\n",
    "\n",
    "<div align='center'><img src=\"figs/many_to_one_02.png?4\" width='25%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What happens at each time step\n",
    "\n",
    "Note: Bias omitted for simplicity\n",
    "\n",
    "<br><div align='center'><img src=\"figs/many_to_one_03.png?4\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What happens at each time step\n",
    "\n",
    "Note: Bias omitted for simplicity\n",
    "\n",
    "<br><div align='center'><img src=\"figs/many_to_one_04.png?5\" width='50%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The weights are always the same!\n",
    "\n",
    "Note: Bias omitted for simplicity\n",
    "\n",
    "<br><div align='center'><img src=\"figs/many_to_one_05.png?1\" width='50%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# $\\mbf{W}_e$ and $\\mbf{W}_h$ do NOT depend on $t$!\n",
    "\n",
    "$$\\mbf{h}_{t} \\doteq \\sigma \\big( \\underbrace{\\mbf{W}_h\\mbf{h}_{t-1}}_{\\text{from prev. steps}}+ \\underbrace{\\mbf{W}_e\\mbf{e}_{t}}_{\\text{current input}} +\\mbf{b}\\big)$$\n",
    "\n",
    "**Good Scalability:** for space complexity. \n",
    "\n",
    "The model scales w.r.t to the time dimension, so **we can allow our context to be long**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# From the hidden state to an output\n",
    "\n",
    "$$\\mbf{h}_{t} \\doteq \\sigma \\big( \\underbrace{\\mbf{W}_h\\mbf{h}_{t-1}}_{\\text{from prev. steps}}+ \\underbrace{\\mbf{W}_e\\mbf{e}_{t}}_{\\text{current input}} +\\mbf{b}\\big)$$\n",
    "\n",
    "$$ \\mathbf{y}_t= \\operatorname{softmax}(\\mbf{U}\\bmf{h}_t+\\mbf{b}_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Dynamics vs what we really store in memory\n",
    "\n",
    "Therefore, the parameterization cost of an RNN does not grow as the number of time steps increases.\n",
    "\n",
    "<br><div align='center'><img src=\"figs/many_to_one_06.png?1\" width='50%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Implementation detail\n",
    "\n",
    "$$\\mbf{h}_{t} \\doteq \\sigma \\big( \\underbrace{\\mbf{W}_h\\mbf{h}_{t-1}}_{\\text{from prev. steps}}+ \\underbrace{\\mbf{W}_e\\mbf{e}_{t}}_{\\text{current input}} +\\mbf{b}\\big)$$\n",
    "\n",
    "Can be implemented as **fully-connected layer** matrix multiplication using concatenation:\n",
    "\n",
    "$$\\mbf{h}_{t} \\doteq\\sigma \\big([\\mbf{W}_h;\\mbf{W}_e][\\mbf{h}_{t-1};\\mbf{e_t}]+\\mbf{b}\\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Forward pass in pseudo code\n",
    "\n",
    "\n",
    "```python\n",
    "# input x (the sequence of tokens)\n",
    "h[0] = zeros # H is an array of all zeros!\n",
    "for i in range(1, len(x)): # unroll the RNN in time (this is the slow part)\n",
    "    h[i] = sigma(W_h@h[i-1] + W_e@e[i] + b_1)  # W_h and W_e shared across time\n",
    "    y[i] = softmax(U@h[i] + b_2)               # h[i], y[i] are time dependent\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN as a Neural Language Model\n",
    "\n",
    "The RNN as LM models $p(w_t | \\text{the, students, opened ,their})$.\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/rnn_lm.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN as a Neural Language Model\n",
    "\n",
    "The RNN models $p(w_5 | \\text{the, students, opened ,their})$.\n",
    "\n",
    "<div align='center'><img src=\"figs/rnn_glance.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN  Language Model\n",
    "\n",
    "\n",
    "‚úÖ Improvements over window-based LM \n",
    "- Can process any length input\n",
    "- Computation for step $t$ can **(in theory)** use information from many steps back\n",
    "- Bounded model size respect to context size\n",
    "- Same weights applied on every timestep, so there is symmetry in how inputs are processed.\n",
    "\n",
    "‚ùå Remaining problems:\n",
    "- Recurrent computation is slow (unroll in time)\n",
    "- In practice, difficult to access information from many steps back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "run_control": {
     "marked": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/rnn_lm.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training a RNN Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN LM is many-to-many\n",
    "\n",
    "### Sequence to Sequence modeling\n",
    "\n",
    "<div align='center'><img src=\"figs/many_to_many.png\" width='25%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN LM is many-to-many\n",
    "\n",
    "- The most \"stupid\" sequence to sequence modeling is the **identity function of the data**\n",
    "- The simplest yet useless way to generate the data is propagate the data itself\n",
    "- You can think this also as a form of **memorization (overfitting)** compared to learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>\n",
    "<h3>LM : NLP = Generative Models : Vision</h3>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/many_to_many_id.png\" width='35%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN LM is many-to-many\n",
    "\n",
    "### Sequence to Sequence modeling\n",
    "\n",
    "<div align='center'><img src=\"figs/many_to_many.png\" width='25%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrency (Bad Visualization)\n",
    "\n",
    "<div align='center'><img src=\"figs/recurrency.png\" width='10%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrency (Bad Visualization)\n",
    "\n",
    "In theory, recurrency with cycles is a problem for training, but we can show that we can **unfold the RNN over time and still create a DAG.**\n",
    "\n",
    "<br/><div align='center'><img src=\"figs/recurrency.png\" width='10%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Unfolding the recurrency over time\n",
    "\n",
    "Explicitly **unrolling a recurrent network into a feedforward computational graph** eliminates any explicit recurrences, allowing the network weights to be trained directly.\n",
    "\n",
    "**The step of unfolding** is an hyper-param that you have to decide.\n",
    "<div align='center'><img src=\"figs/recurrency_unfold.png\" width='25%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training a RNN Language Model\n",
    "\n",
    "- Get a big corpus of text which is a sequence of words $\\{w_1,\\ldots,w_N\\}$\n",
    "- Feed a window till time $t$ to the RNN-LM, compute probability distribution over $V$ at time $t+1$\n",
    "    - Predict a probability distribution $\\hat{\\mbf{y}}$ over words, given words so far\n",
    "- Loss function is same as word2vec: we compare two discrete distributions with **cross-entropy**\n",
    "$$ \\loss(\\{\\mbf{W}\\}) = CE(\\hat{\\mbf{y}},{\\mbf{y}})= -\\sum_{w\\in V}\\mbf{y}\\log(\\hat{\\mbf{y}}) =-\\log(\\hat{\\mbf{y}})[t+1]$$\n",
    "\n",
    "- **Note:** $[t+1]$ selects the index in $V$ of the next word, at time $t+1$\n",
    "- We use again **self supervision!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training a RNN Language Model\n",
    "\n",
    "\n",
    "<br/><div align='center'><img src=\"figs/rnn_lm_train_01.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training a RNN Language Model\n",
    "\n",
    "\n",
    "<br/><div align='center'><img src=\"figs/rnn_lm_train_02.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training a RNN Language Model\n",
    "\n",
    "\n",
    "<br/><div align='center'><img src=\"figs/rnn_lm_train_04.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Teacher Forcing at training time\n",
    "\n",
    "**When training, <u>the autoregressive capability is turned OFF</u>**. This is is called **teacher forcing**.\n",
    "At time $t=3$ the model predicts `zoom` but the word should have been `their`. \n",
    "\n",
    "As input to time $t=3$ we use the value from the corpus `their` NOT the prediction `zoom`.\n",
    "<div align='center'><img src=\"figs/rnn_lm_train_05.png?1\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training a RNN Language Model\n",
    "\n",
    "- Computing the loss and gradients across the entire corpus $\\{w_1,\\ldots,w_N\\}$ is too expensive\n",
    "    - We would do a simple GD update after seeing the entire corpus!\n",
    "    - So we use an approximation using SGD. \n",
    "    - We consider $\\{w_1,\\ldots,w_N\\}$ as **a sentence or a document but not the entire corpus.**\n",
    "    $$\\frac{1}{T}\\sum_{t=1}^T\\mathcal{L}_t(\\bmf{\\theta}) $$\n",
    "    \n",
    "- SGD allows us to compute loss and gradients for a small chunk of text, and then update the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training a RNN Language Model\n",
    "1. Compute the loss for a sentence (Note: in the implementation is a batch of sentence)\n",
    "2. **Compute the gradients on weights** (Most difficult part to understand üî•)\n",
    "3. Update the weights\n",
    "4. Get another batch of sentences and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How do we get the gradients on the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# We use Back Propagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT)\n",
    "\n",
    "We can approach BPTT:\n",
    "1) using the computational graph _(engineering point of view/intuition)_\n",
    "2) mathematically _(more formal, gives a motivation)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass in RNN\n",
    "<div align='center'><img src=\"figs/rnn_backprop/Slide45.png\" width='70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass in RNN\n",
    "<div align='center'><img src=\"figs/rnn_backprop/Slide46.png\" width='70%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass in RNN\n",
    "<div align='center'><img src=\"figs/rnn_backprop/Slide47.png\" width='70%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass in RNN\n",
    "<div align='center'><img src=\"figs/rnn_backprop/Slide48.png\" width='70%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT) - Backward\n",
    "<div align='center'><img src=\"figs/rnn_backprop/Slide49.png\" width='70%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT) - Backward\n",
    "<div align='center'><img src=\"figs/rnn_backprop/Slide50.png\" width='70%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT) - Backward\n",
    "<div align='center'><img src=\"figs/rnn_backprop/Slide51.png\" width='70%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT) - Backward\n",
    "<div align='center'><img src=\"figs/rnn_backprop/Slide52.png\" width='70%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT) - Backward\n",
    "\n",
    "Eventually we will arrive to **receive gradients on weights.**\n",
    "\n",
    "Remember at each time step: $F_w$ received different inputs yet the **parameters $\\mbf{W}_h$ are the <u>same across time.<u/>**\n",
    "<div align='center'><img src=\"figs/rnn_backprop/Slide54.png\" width='70%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT) - Backward\n",
    "\n",
    "Eventually we will arrive to receive gradients on weights.\n",
    "\n",
    "Remember at each time step: $F_w$ received different inputs yet the **parameters $\\mbf{W}_h$ are the <u>same across time.<u/>**\n",
    "<div align='center'><img src=\"figs/rnn_backprop/Slide55.png\" width=70%' ></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gradients sum at outward branches\n",
    "\n",
    "$\\mbf{h_3}$ is input to <u>two functions</u> \"in the future\": one produce $\\mbf{y}_3$ the other will produce $\\mbf{h}_4$.\n",
    "\n",
    "So $\\mbf{h_3}$ will receive **two** gradients that will <u>be summed together</u>.\n",
    "\n",
    "<div align='center'><img src=\"figs/rnn_backprop/Slide56.png\" width='15%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gradients sum at outward branches\n",
    "\n",
    "$$ \\chain{f\\big(x(t),y(t)\\big)}{t}  = \\chain{f}{x}\\chain{x}{t}+\\chain{f}{y}\\chain{y}{t}$$\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/rnn_backprop/Slide57.png\" width='15%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT)\n",
    "\n",
    "Mathematically: **apply chain rule**. Note: I simplified by NOT showing:\n",
    "- the biases\n",
    "- the weights that transform the input $\\mbf{W}_e$ and the weights that do classification at each output $\\mbf{U}$.\n",
    "- Keep in mind that we have to receive gradients over them because we need to update them as well\n",
    "\n",
    "I do show the weights $\\mbf{W}_h$, because **this is the hardest part of BPTT.**\n",
    "\n",
    "It is the part in which we have the recursion, and we really go \"back in time\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT)\n",
    "\n",
    "Let's first agree on what we have to compute:\n",
    "\n",
    " $$\\chain{\\loss}{\\mbf{W}_h} =~~? \\qquad \\text{we use chain rule to compute it}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note this is a function that maps a matrix $\\mbf{W}_h$ to a scalar value, the loss $\\loss$. \n",
    "\n",
    "This makes sense to compute because the loss $\\loss$ depends on $\\mbf{W}_h$.\n",
    "\n",
    "We want to know what is the \"infinitesimal\" influence on the loss by perturbing a bit of $\\mbf{W}_h$ (informal definition of gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT)\n",
    "\n",
    "Let's first agree on what we have to compute:\n",
    " $$\\chain{\\loss}{\\mbf{W}_h} =~~? \\qquad \\text{we use chain rule to compute it}$$\n",
    " \n",
    " <div align='center'><img src=\"figs/rnn_backprop/Slide58.png\" width=60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT)\n",
    "\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\chain{\\loss}{\\loss}\\chain{\\loss}{\\loss_3}\\chain{\\loss_3}{\\mbf{y}_3}\\quad{...}$$\n",
    " \n",
    " <div align='center'><img src=\"figs/rnn_backprop/Slide58.png\" width=60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT)\n",
    "\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\chain{\\loss}{\\loss}\\chain{\\loss}{\\loss_3}\\chain{\\loss_3}{\\mbf{y}_3}\\chain{\\mbf{y}_3}{\\mbf{h}_3}\\quad{...}$$\n",
    " \n",
    " <div align='center'><img src=\"figs/rnn_backprop/Slide58.png\" width=60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT)\n",
    "\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\underbrace{\\chain{\\loss}{\\loss}}_1\\underbrace{\\chain{\\loss}{\\loss_3}}_{1/T=1/3}\\underbrace{\\chain{\\loss_3}{\\mbf{y}_3}}_{\\mathbb{R}^{1\\times|V|}}\\underbrace{\\chain{\\mbf{y}_3}{\\mbf{h}_3}}_{\\mathbb{R}^{|V|\\times h}}\\quad{...}$$\n",
    " \n",
    " <div align='center'><img src=\"figs/rnn_backprop/Slide58.png\" width=60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT)\n",
    "\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\underbrace{\\chain{\\loss}{\\loss}}_1\\underbrace{\\chain{\\loss}{\\loss_3}}_{1/T=1/3}\\underbrace{\\chain{\\loss_3}{\\mbf{y}_3}}_{\\mathbb{R}^{1\\times|V|}}\\underbrace{\\chain{\\mbf{y}_3}{\\mbf{h}_3}}_{\\mathbb{R}^{|V|\\times h}}\\underbrace{\\chain{\\mbf{h}_3}{\\mbf{W}_h}}_{\\mathbb{R}^{h}\\times(\\mathbb{R}^{h}\\times \\mathbb{R}^{h})}$$\n",
    " \n",
    " <div align='center'><img src=\"figs/rnn_backprop/Slide58.png\" width=60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BPTT\n",
    "\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\chain{\\loss}{\\loss}\\chain{\\loss}{\\loss_3}\\chain{\\loss_3}{\\mbf{y}_3}\\chain{\\mbf{y}_3}{\\mbf{h}_3}\\chain{\\mbf{h}_3}{\\mbf{W}_h}$$\n",
    "\n",
    "The tricky part is $\\chain{\\mbf{h}_3}{\\mbf{W}_h}$\n",
    "\n",
    "$\\mbf{h}_3 =f\\big(\\mbf{h}_2,\\mbf{W}_h,...)$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    " <div align='center'><img src=\"figs/rnn_backprop/Slide58.png\" width=70%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BPTT\n",
    "\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\chain{\\loss}{\\loss}\\chain{\\loss}{\\loss_3}\\chain{\\loss_3}{\\mbf{y}_3}\\chain{\\mbf{y}_3}{\\mbf{h}_3}\\chain{\\mbf{h}_3}{\\mbf{W}_h}$$\n",
    "\n",
    "The tricky part is $\\chain{\\mbf{h}_3}{\\mbf{W}_h}$. Remember that $\\mbf{h}_3 =f\\big(\\mbf{h}_2,\\mbf{W}_h,\\mbf{W}_e,\\mbf{x}_3\\big)$. \n",
    "\n",
    "So $\\mbf{h}_3$ depends on $\\mbf{h}_2$!\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BPTT\n",
    "\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\chain{\\loss}{\\loss}\\chain{\\loss}{\\loss_3}\\chain{\\loss_3}{\\mbf{y}_3}\\chain{\\mbf{y}_3}{\\mbf{h}_3}\\chain{\\mbf{h}_3}{\\mbf{W}_h}$$\n",
    "\n",
    "$$\\mbf{h}_3 =f\\big(\\mbf{h}_2,\\mbf{W}_h,...\\big)$$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BPTT and Vanishing Gradient problem\n",
    "\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\chain{\\loss}{\\loss}\\chain{\\loss}{\\loss_3}\\chain{\\loss_3}{\\mbf{y}_3}\\chain{\\mbf{y}_3}{\\mbf{h}_3}\\chain{\\mbf{h}_3}{\\mbf{W}_h}$$\n",
    "\n",
    "$$\\mbf{h}_3 =f\\big(\\underbrace{f\\big(\\mbf{h}_1,\\mbf{W}_h,...\\big)}_{\\mbf{h}_2},\\mbf{W}_h,...\\big)$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# This is the essence of BPTT\n",
    "\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\chain{\\loss}{\\loss}\\chain{\\loss}{\\loss_3}\\chain{\\loss_3}{\\mbf{y}_3}\\chain{\\mbf{y}_3}{\\mbf{h}_3}\\chain{\\mbf{h}_3}{\\mbf{W}_h}$$\n",
    "\n",
    "$$\\mbf{h}_3 =f\\big(\\underbrace{f\\big(\\mbf{h}_1,\\mbf{W}_h,...\\big)}_{\\mbf{h}_2},\\mbf{W}_h,...\\big)$$\n",
    "\n",
    " \n",
    "<small>[If you want to go deeper d2l.ai on BPTT](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# This is the essence of BPTT\n",
    "Missing part: $$\\chain{\\mbf{h}_3}{\\mbf{W}_h}~=~?$$\n",
    "\n",
    "So far:\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\chain{\\loss}{\\loss}\\chain{\\loss}{\\loss_3}\\chain{\\loss_3}{\\mbf{y}_3}\\chain{\\mbf{y}_3}{\\mbf{h}_3}\\chain{\\mbf{h}_3}{\\mbf{W}_h}$$\n",
    "\n",
    "$$\\mbf{h}_3 =f\\big(\\underbrace{f\\big(\\mbf{h}_1,\\mbf{W}_h,...\\big)}_{\\mbf{h}_2},\\mbf{W}_h,...\\big)$$\n",
    "\n",
    " \n",
    "<small>[If you want to go deeper: d2l.ai on BPTT](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# This is the essence of BPTT\n",
    "$\\def\\restrict#1{\\raise-.5ex\\hbox{\\ensuremath|}_{#1}}$\n",
    "\n",
    "Missing part: $$\\chain{\\mbf{h}_3}{\\mbf{W}_h}\\Big\\vert_{\\text{all time steps}}=\\chain{\\mbf{h}_3}{\\mbf{W}_h}+\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\cdot\\underbrace{\\chain{\\mbf{h}_2}{\\mbf{W}_h}}_{\\text{note: another rec.}}$$\n",
    "\n",
    "So far:\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\chain{\\loss}{\\loss}\\chain{\\loss}{\\loss_3}\\chain{\\loss_3}{\\mbf{y}_3}\\chain{\\mbf{y}_3}{\\mbf{h}_3}\\chain{\\mbf{h}_3}{\\mbf{W}_h}$$\n",
    "\n",
    "$$\\mbf{h}_3 =f\\big(\\underbrace{f\\big(\\mbf{h}_1,\\mbf{W}_h,...\\big)}_{\\mbf{h}_2},\\mbf{W}_h,...\\big)$$\n",
    "\n",
    " \n",
    "<small>[If you want to go deeper: d2l.ai on BPTT](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BPTT\n",
    "\n",
    "$$\\chain{\\mbf{h}_3}{\\mbf{W}_h}\\Big\\vert_{\\text{all time steps}}=\\chain{\\mbf{h}_3}{\\mbf{W}_h}+\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\chain{\\mbf{h}_2}{\\mbf{W}_h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BPTT\n",
    "\n",
    "$$\\chain{\\mbf{h}_3}{\\mbf{W}_h}\\Big\\vert_{\\text{all time steps}}=\\chain{\\mbf{h}_3}{\\mbf{W}_h}+\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\chain{\\mbf{h}_2}{\\mbf{W}_h}$$\n",
    "\n",
    "$$\\chain{\\mbf{h}_3}{\\mbf{W}_h}\\Big\\vert_{\\text{all time steps}}=\\chain{\\mbf{h}_3}{\\mbf{W}_h}+\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\Big[\\chain{\\mbf{h}_2}{\\mbf{W}_h}+\\chain{\\mbf{h}_2}{\\mbf{h}_1}\\chain{\\mbf{h}_1}{\\mbf{W}_h}\\Big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BPTT\n",
    "\n",
    "$$\\chain{\\mbf{h}_3}{\\mbf{W}_h}\\Big\\vert_{\\text{all time steps}}=\\chain{\\mbf{h}_3}{\\mbf{W}_h}+\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\chain{\\mbf{h}_2}{\\mbf{W}_h}$$\n",
    "\n",
    "$$\\chain{\\mbf{h}_3}{\\mbf{W}_h}\\Big\\vert_{\\text{all time steps}}=\\chain{\\mbf{h}_3}{\\mbf{W}_h}+\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\Big[\\chain{\\mbf{h}_2}{\\mbf{W}_h}+\\chain{\\mbf{h}_2}{\\mbf{h}_1}\\chain{\\mbf{h}_1}{\\mbf{W}_h}\\Big]$$\n",
    "\n",
    "\n",
    "$$\\chain{\\mbf{h}_3}{\\mbf{W}_h}\\Big\\vert_{\\text{all time steps}}=\\chain{\\mbf{h}_3}{\\mbf{W}_h}+\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\chain{\\mbf{h}_2}{\\mbf{W}_h}+\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\chain{\\mbf{h}_2}{\\mbf{h}_1}\\chain{\\mbf{h}_1}{\\mbf{W}_h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Back Propagation Through Time (BPTT)\n",
    "\n",
    "$$\\chain{\\loss}{\\mbf{W}_h} = \\underbrace{\\chain{\\loss}{\\loss}}_1\\underbrace{\\chain{\\loss}{\\loss_3}}_{1/T=1/3}\\underbrace{\\chain{\\loss_3}{\\mbf{y}_3}}_{\\mathbb{R}^{1\\times|V|}}\\underbrace{\\chain{\\mbf{y}_3}{\\mbf{h}_3}}_{\\mathbb{R}^{|V|\\times h}}\\underbrace{\\chain{\\mbf{h}_3}{\\mbf{W}_h}}_{\\mathbb{R}^{h}\\times(\\mathbb{R}^{h}\\times \\mathbb{R}^{h})}$$\n",
    " \n",
    " <div align='center'><img src=\"figs/rnn_backprop/Slide58.png\" width=60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BPTT and Vanishing Gradient Problem\n",
    "\n",
    "If the recursive chain is too long, the produce of matrices $\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\chain{\\mbf{h}_2}{\\mbf{h}_1}...$ can make the **gradient to vanish** especially if using tanh activation function.\n",
    "\n",
    "Think as multiply $a\\cdot b\\cdot b\\cdot b\\cdot b\\cdot b ...$ where $0<b<1$ but for matrices. \n",
    "\n",
    "**At the end the norm of the gradient will be so small that will get to zero numerically!**\n",
    "\n",
    "$$\\chain{\\mbf{h}_3}{\\mbf{W}_h}\\Big\\vert_{\\text{all time steps}}=\\chain{\\mbf{h}_3}{\\mbf{W}_h}+\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\chain{\\mbf{h}_2}{\\mbf{W}_h}+\\chain{\\mbf{h}_3}{\\mbf{h}_2}\\chain{\\mbf{h}_2}{\\mbf{h}_1}\\chain{\\mbf{h}_1}{\\mbf{W}_h}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vanishing Gradient and RNN\n",
    "\n",
    "- For this reason RNN **cannot capture large context dependency** in the text and they are bounded to model moderately small sequences of text. \n",
    "\n",
    "- Still better than window based approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vanishing Gradient and NLP\n",
    "\n",
    "- **LM task:** `When she tried to print her tickets, she found that the printer was out of toner.\n",
    "She went to the stationery store to buy more toner. It was very overpriced. After\n",
    "installing the toner into the printer, she finally printed her ________`\n",
    "- To learn from this training example, the RNN-LM needs to model the dependency between **‚Äútickets‚Äù on the 7th step** and the target word ‚Äútickets‚Äù at the end.\n",
    "\n",
    "- But if the gradient is small, the model cannot learn this dependency\n",
    "- So, the model is unable to predict similar long-distance dependencies at test time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vanishing Gradient Workaround: Truncated BPTT\n",
    "\n",
    "- We use SGD compute the gradients over a small windows (i.e. 25 steps) and then update the parameters.\n",
    "- We avoid computing this over longer sequences.\n",
    " <div align='center'><img src=\"https://d2l.ai/_images/truncated-bptt.svg\" width='50%' ></div>\n",
    "\n",
    "_Comparing strategies for computing gradients in RNNs. From top to bottom: randomized truncation, regular truncation, and full computation._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BPTT\n",
    "\n",
    " <div align='center'><img src=\"figs/BPTT_01.png\" width='100%' ></div>\n",
    " \n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Truncated BPTT\n",
    "\n",
    " <div align='center'><img src=\"figs/BPTT_02.png\" width='40%' ></div>\n",
    " \n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Truncated BPTT\n",
    "\n",
    " <div align='center'><img src=\"figs/BPTT_03.png\" width='100%' ></div>\n",
    " \n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Truncated BPTT\n",
    "\n",
    " <div align='center'><img src=\"figs/BPTT_04.png\" width='100%' ></div>\n",
    " \n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Opposite problem: Exploding gradient\n",
    "\n",
    "- The norm of the gradient becomes to big (think as a force that is applied too strongly and abruptly).\n",
    "- Machine precision cannot represent it, if too strong! You spot it if in the loss you encounter `NaN`.\n",
    "- Easier problem than vanishing gradient\n",
    "\n",
    "## Easy work around is to clip the gradient\n",
    "Idea: if gradient is higher than a maximum threshold, then push it down to the maximum threshold\n",
    "```python\n",
    "np.clip(gradient, -5, 5, out=gradient) # clip to mitigate exploding gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gradient Clipping for Exploding Gradient\n",
    "\n",
    "<br/><div align='center'><img src=\"figs/grad_clipping.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OK, so now that we know how to train it how do we generate text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Remember LM is a Autoregressive, Generative model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generation from a LM - \"Generating roll outs\"\n",
    "\n",
    "Equivalent to 1) generate samples from a generative model 2) use the **autoregressive** capability of the LM.\n",
    "\n",
    "Long story short: \n",
    "1. input a base case token $\\leftarrow$ Initialization\n",
    "2. predict at time $t$ (now you have two choice: deterministic or probabilistic)\n",
    "    - Deterministic: take `argmax` over the softmax probabilities\n",
    "    - Probabilistic: we use **inverse transform sampling** to sample a word from the softmax probabilities\n",
    "3. Once you sampled at time $t$, feed as input the sampled word at time $t+1$ and continue\n",
    "4. The LM ends when predicts `<END>`special token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Inverse Transform Sampling\n",
    "\n",
    "1. Given a discrete distribution $p$ of probabilities defined over a vocabulary $V$\n",
    "2. Compute the Cumulative Density Function (CDF) of $p$ \n",
    "3. Sample uniformly in $u \\sim \\mathcal{U}[0,1]$ use $u$ to index inverting the (CDF)\n",
    "4. According to the interval you are in, now select the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Inverse Transform Sampling\n",
    "\n",
    "```python\n",
    "U = np.random.rand(1000) #random sample uniformly from [0, 1]. \n",
    "# u ~ U[0,1]\n",
    "# for each sampled u.\n",
    "# find the first bin for which u > cumsum. \n",
    "# Get the index of that bin as the sample.\n",
    "sampled_idx = np.argmin((U[:, None] > pmf.cumsum()), axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Inverse Transform Sampling even simpler with just Python\n",
    "```python\n",
    "np.random.choice(values_to_sample, p=probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Inverse Transform Sampling \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dist =  np.array([1.18518298, 1.30917493, 1.10973212, 2.24523519, 1.01625606])\n",
    "pmf = dist/dist.sum()\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_figheight(3)\n",
    "fig.set_figwidth(15)\n",
    "# PDF\n",
    "axs[0].stem(pmf, linefmt='b-', markerfmt='bo', basefmt='--')\n",
    "axs[0].set_title('PMF')\n",
    "axs[0].set_xlabel('Vocabulary')\n",
    "axs[0].set_ylabel('Probability Mass Function')\n",
    "axs[0].set_aspect('auto')\n",
    "# CUMSUM\n",
    "axs[1].plot(pmf.cumsum(), 'o--')\n",
    "axs[1].set_title('CDF')\n",
    "axs[1].set_xlabel('Vocabulary')\n",
    "axs[1].set_ylabel('Cumlative Probability')\n",
    "axs[1].set_aspect('auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Inverse Transform Sampling in Action\n",
    "\n",
    "<br/><center>\n",
    "<img width=\"40%\" src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/Inverse_Transform_Sampling_Example.gif\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generation from a LM - Character-level Language Model\n",
    "- The vocabulary $V$ is `[h,e,l,o]` we have learned how to say hello\n",
    "- We can start from 1) <START> special token 2) or given a char as input\n",
    "\n",
    "Long story short: **Predict + sample (if probabilistic) + autoregress**\n",
    "    \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generation from a LM - Character-level Language Model\n",
    "\n",
    "<div align='center'><img src=\"figs/LM_sampling_01.png\" width='40%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generation from a LM - Character-level Language Model\n",
    "\n",
    "<div align='center'><img src=\"figs/LM_sampling_02.png\" width='40%' ></div><small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generation from a LM - Character-level Language Model\n",
    "\n",
    "<div align='center'><img src=\"figs/LM_sampling_03.png\" width='40%' ></div><small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generation from a LM - Character-level Language Model\n",
    "\n",
    "Note that: sampling makes the generation <u>probabilistic</u>. Even if we start from the same char, we may end up with completely different words (this gives variability in the output) yet the words should match the \"distribution\" of the text on which was trained on. \n",
    "<div align='center'><img src=\"figs/LM_sampling_03.png\" width='30%' ></div><small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generation from a LM - It works with words too\n",
    "\n",
    "\n",
    "<br><div align='center'><img src=\"figs/LM_sampling_04.png\" width='50%' ></div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LM can be seen as an Encoder Decoder\n",
    "\n",
    "Useful idea for when we will cover **Transformers and BERT**\n",
    "\n",
    "$$h = \\operatorname{encode}(x) \\quad \\text{we go from x to another more useful representation h}$$\n",
    "\n",
    "$$x^{\\prime}= \\operatorname{decode}(h) \\quad \\text{from h we can go back to an approximation of x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " $$ x^{\\prime}= \\operatorname{decode}\\big(\\operatorname{encode}(x)\\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Encoder Decoder for Vision\n",
    "\n",
    "Here $\\mbf{x}$ is an image. The encoder maps the image to a low dimensional space using convolution.\n",
    "<div align='center'><img src=\"figs/encoder_decoder_img.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LM is an Encoder Decoder for NLP\n",
    "\n",
    "Text is Sequence to Sequence but we can decompose it as:\n",
    "\n",
    "**Key Idea:** We decompose `Sequence to Sequence` =` Many-to-one` + `one-to-Many`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Encoder for NLP\n",
    "\n",
    "**Many to one:** Encode input sequence in a single vector **(e.g. the last hidden state)**\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/encoder_NLP.png\" width='50%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Encoder for NLP\n",
    "\n",
    "**Many to one:** Encode input sequence in a single vector **(e.g. the last hidden state)**\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/decoder_NLP.png\" width='70%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now the fun üéâü•≥\n",
    "## Minimal RNN implementation in numpy in ~100 lines \n",
    "\n",
    "\n",
    "[Code from Karpathy github gist py2.7](https://gist.github.com/karpathy/d4dee566867f8291f086)\n",
    "\n",
    "[Same but updated to py3](https://gist.github.com/karpathy/d4dee566867f8291f086)\n",
    "\n",
    "<small>Copyright and Code by Andrej Karpathy BSD License</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### We will just study this for practicing, in real scenario please use [pytorch](http://pytorch.org)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align='center'><img src=\"figs/rnn_min_code.png\" width='100%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In practical environment: Pytorch or Pytorch Lighting!\n",
    "<div align='center'><img src=\"https://pytorch.org/tutorials/_static/img/thumbnails/cropped/profiler.png\" width='20%' ><img src=\"\n",
    "https://assets.website-files.com/5f7852ccc906cffcb62ebb36/6010f8febaab42205eda56d0_1*JANKKXdopykNVg66IFHVVg.png\" width='20%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN in 100 lines of code\n",
    "\n",
    "It trains a RNN at the **character level** with unfolding period of *25* chars, hidden layer size is 100-D. \n",
    "\n",
    "We will model the input just with one-hot encoding: we will not use word embedding to keep it simple\n",
    "\n",
    "**Requirements:** numpy only\n",
    "\n",
    "<div align='center'><img src=\"https://d2l.ai/_images/rnn-train.svg\" width='30%' ></div>\n",
    "\n",
    "**Six** parts:\n",
    "1. I/O: corpus parsing and vocabulary construction\n",
    "2. Init of the model parameters and hyper-parameters\n",
    "3. Forward pass (input to loss computation)\n",
    "4. Backward pass (most complex part, Truncated BPTT) ü§Ø\n",
    "5. Main Training Loop\n",
    "6. Generation part (sampling) fun part! ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 1. I/O: corpus parsing and vocabulary construction\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 2. Init of the model parameters and hyper-parameters\n",
    "\n",
    "```python\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 3. Forward pass\n",
    "\n",
    "```python\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs, targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 4. Back pass\n",
    "\n",
    "```python\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  #...\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 5a. Main loop\n",
    "\n",
    "```python\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "  # ....    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 5b. Main loop\n",
    "\n",
    "```python\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 6. Generation\n",
    "\n",
    "```python\n",
    "\n",
    "# sample from the model now and then\n",
    "sample_ix = sample(hprev, inputs[0], 200)\n",
    "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "\n",
    "    \n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Details on the implementation of the backward pass\n",
    "[Details on the backward pass: mkffl.github.io/2019/07/08/minimalist-RNN.html](https://mkffl.github.io/2019/07/08/minimalist-RNN.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN learns to generate Sonnets\n",
    "\n",
    "<br><div align='center'><img src='figs/rnn_generation/rnn_generation_01.png' width='60%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN learns to generate Sonnets\n",
    "\n",
    "<br><div align='center'><img src='figs/rnn_generation/rnn_generation_02.png' width='60%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN learns to generate Sonnets\n",
    "\n",
    "<br><div align='center'><img src='figs/rnn_generation/rnn_generation_03.png' width='60%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ...or generate Latex\n",
    "\n",
    "<br><div align='center'><img src='figs/rnn_generation/rnn_generation_04.png' width='60%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ...or generate Latex\n",
    "\n",
    "<br><div align='center'><img src='figs/rnn_generation/rnn_generation_05.png' width='60%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ...or C GNU/Linux Kernel code\n",
    "\n",
    "<br><div align='center'><img src='figs/rnn_generation/rnn_generation_06.png' width='60%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to evaluate a LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LM Evaluation (you already seen it in Part I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Train on `train` split.\n",
    "- Select hyper-params on validation (or dev set)\n",
    "- Test on the `test` split.\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/validation.png\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Set size and partitioning: not a clear definition\n",
    "\n",
    "<br/>\n",
    "\n",
    "| Train              \t| 70% \t| 60% \t| 80% \t|\n",
    "|--------------------\t|-----\t|-----\t|-----\t|\n",
    "| Validation _(dev)_ \t| 20% \t| 20% \t| 10% \t|\n",
    "| Test               \t| 10% \t| 20% \t| 10% \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LM Evaluation Metric: Perplexity (PP)\n",
    "\n",
    "Informally how well you predict next words on a text corpus $W$ you never trained on.\n",
    "**PP the lower the better**.\n",
    "\n",
    "$$ PP(W) \\doteq p_{LM}(w_1, w_2, \\ldots, w_N)^{-\\frac{1}{N}} $$\n",
    "\n",
    "$$ PP(W) = \\prod_{i=1}^N \\big(\\frac{1}{p_{LM}(w_{i+1}|w_{i},\\ldots,w_{1})} \\big)^{1/N} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNNs greatly improved perplexity over prior art\n",
    "\n",
    "Informally how well you predict next words on a text corpus $W$ you never trained on.\n",
    "**PP the lower the better**.\n",
    "\n",
    "<br><div align='center'><img src=\"figs/LM_evaluation_paper.png\" width='70%' ></div>\n",
    "\n",
    "<small>[Efficient softmax approximation for GPUs](https://arxiv.org/pdf/1609.04309.pdf)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applications of Sequence Modeling, Stacked Bi-directional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OPIS for Course Evaluation\n",
    "\n",
    "## OPIS CODE: 6F0T06KF\n",
    "\n",
    "<u>The code and guide are in the google classroom! </u>\n",
    "\n",
    "Guide on how to evaluate with OPIS (Opinion of the students):\n",
    "\n",
    "https://www.uniroma1.it/sites/default/files/field_file_allegati/vademecum_per_studenti_opis_2022_23.pdf üáÆüáπ\n",
    "\n",
    "https://www.uniroma1.it/sites/default/files/field_file_allegati/guided_path_to_access_student_s_opinions_questionnaire_2022_2023.pdf üá∫üá∏\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's lecture\n",
    "## - Recap on RNN\n",
    "### - We will see a few application of RNN in NLP and vision\n",
    "### - Bidirectional RNN and Stacked RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This lecture material is taken from\n",
    "üìò **Chapter 9 Jurafsky Book**\n",
    "\n",
    "üìò **Chapter 6.3 Eisenstein Book**\n",
    "- [Stanford Slide RNN](http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture05-rnnlm.pdf)\n",
    "- [Stanford Lecture RNN](https://www.youtube.com/watch?v=0LixFSa7yts&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&index=6)\n",
    "- [Stanford Notes on Word2Vec](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf)\n",
    "- [Andrej Karpathy Lecture on RNN](https://www.youtube.com/watch?v=yCC09vCHzF8)\n",
    "- [Andrej Karpathy Slides on RNN](http://cs231n.stanford.edu/slides/2022/lecture_10_ruohan.pdf)\n",
    "\n",
    "Another resource with code is [[d2l.ai] RNN](https://d2l.ai/chapter_recurrent-neural-networks/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap on RNN and LM\n",
    "\n",
    "Language Model: A system that predicts the next word\n",
    "\n",
    "Recurrent Neural Network: A family of neural networks that:\n",
    "- Take sequential input of any length\n",
    "- Apply the same weights on each step\n",
    "- Can optionally produce output on each step\n",
    "- Recurrent Neural Network $\\neq$ Language Model\n",
    "\n",
    "We have shown that RNNs are a great way to build a LM (despite some problems)\n",
    "\n",
    "**RNNs are also useful for much more! We see it today!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why should we care about Language Modeling\n",
    "\n",
    "Language Modeling is a **benchmark task** that helps us measure our progress on predicting language use\n",
    "\n",
    "Language Modeling is a subcomponent of many NLP tasks, especially those involving generating text or estimating the probability of text:\n",
    "- Predictive typing\n",
    "- Speech recognition\n",
    "- Handwriting recognition\n",
    "- Spelling/grammar correction\n",
    "- Authorship identification\n",
    "- Machine translation\n",
    "- Summarization\n",
    "- Dialogue\n",
    "\n",
    "Everything else in NLP has now been rebuilt upon Language Modeling: GPT-3 is an LM!\n",
    "Though remember that **NLP $\\neq$ LM**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN can be used for..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN can be used for Sequence Tagging\n",
    "\n",
    "Solve problems such as **part-of-speech (POS) tagging, named entity recognition**\n",
    "\n",
    "You can solve it with RNN as a many-to-many method with supervision at the word level.\n",
    "\n",
    "<br><div align='center'><img src=\"figs/pos_rnn.png\" width='50%' ></div>\n",
    "\n",
    "POS is the process of assigning a part-of-speech to each word in part-of-speech tagging a text. \n",
    "**Tagging is a disambiguation task; words are ambiguous**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Good rule for Deep Learning Researchers\n",
    "\n",
    "**Most Frequent Class Baseline:**\n",
    "\n",
    "> Most Frequent Class Baseline: Always compare a classifier against a baseline at\n",
    "least as good as the most frequent class baseline (assigning each token to the class\n",
    "it occurred most often in the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN can be used for Sentence Classification\n",
    "\n",
    "Solve problems such as **sentiment classification or sentiment analysis**\n",
    "\n",
    "You can solve it with RNN as a many-to-one method with supervision at the word level.\n",
    "\n",
    "<br><div align='center'><img src=\"figs/sentiment_an_rnn.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN can be used for Sentence Classification\n",
    "\n",
    "How to compute **sentence encoding?**\n",
    "\n",
    "<br><div align='center'><img src=\"figs/sentiment_an_rnn.png\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN can be used for Sentence Classification\n",
    "\n",
    "How to compute **sentence encoding?** Simple: take last hidden state vector\n",
    "\n",
    "\n",
    "<br><div align='center'><img src=\"figs/sentiment_an_rnn_01.png?2\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN can be used for Sentence Classification\n",
    "\n",
    "How to compute **sentence encoding?** Simple: do pooling (e.g. average) of all intermediate hidden states.\n",
    "\n",
    "\n",
    "<br><div align='center'><img src=\"figs/sentiment_an_rnn_02.png?2\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN can be used for Encoding for other tasks\n",
    "\n",
    "$$\\mbf{x} = \\operatorname{encoder}(w_1,...w_N)$$\n",
    "\n",
    "\n",
    "<br><div align='center'><img src=\"figs/sentiment_an_rnn_02.png?2\" width='40%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN can be used for Encoding for other tasks\n",
    "\n",
    "$$\\mbf{x} = \\operatorname{encoder}(w_1,...w_N)$$ Useful for: **question answering, machine translation**\n",
    "\n",
    "\n",
    "<br><div align='center'><img src=\"figs/qa_rnn_01.png?2\" width='60%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN can be used for Decoding for other tasks\n",
    "\n",
    "$$\\mbf{x} = \\operatorname{decoder}(\\operatorname{encoder}(\\text{input}),<start>)$$ \n",
    "\n",
    "Useful for: **speech recognition, machine translation, summarization, imagine captioning**\n",
    "\n",
    "This is an example of a **conditional language model**. The LM is conditioned on the speech representation.\n",
    "\n",
    "<br><div align='center'><img src=\"figs/speech_rnn_01.png?2\" width='60%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN as decoder: Image Captioning\n",
    "\n",
    "Given an image $\\mbf{x}$ return a sentence that describe the image. We need:\n",
    "1. Something that gets a gist representation of the image, e.g. $f(\\mbf{x})$ where $f$ can be a ConvNet trained on ImageNet or with Self-Supervision.\n",
    "2. Conditioned on $f(\\mbf{x})$, starting from `<START>` we train an RNN to match the ground-truth sentence.\n",
    "3. So we go $\\mbf{x} \\longrightarrow \\{w_1,...,w_N\\}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN as decoder: Image Captioning\n",
    "<br><div align='center'><img src=\"figs/image_captioning_01.png?2\" width='60%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN as decoder: Image Captioning\n",
    "<br><div align='center'><img src=\"figs/image_captioning_02.png?2\" width='60%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN as decoder: Image Captioning\n",
    "<br><div align='center'><img src=\"figs/image_captioning_03.png?2\" width='60%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN as Encoder: Visual Question Answering\n",
    "<br><div align='center'><img src=\"figs/vqa_01.png\" width='60%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN as Encoder: Visual Question Answering\n",
    "<br><div align='center'><img src=\"figs/vqa_02.png\" width='60%' ></div>\n",
    "<small>picture from Stanford</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stacked Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN so far\n",
    "\n",
    "<br><div align='center'><img src=\"figs/stacked_rnn_01.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Stacked RNN (or Multi-layer RNN)\n",
    "\n",
    "- RNNs are already \"deep\" on one dimension, **the time dimension**---they unroll over many timesteps.\n",
    "\n",
    "- We can also make them ‚Äúdeep‚Äù in **another dimension** (the representation dimension)! We can do so by applying multiple RNNs ‚Äì this is a multi-layer RNN (stacked RNN).\n",
    "\n",
    "- This allows the network to compute more complex representations \n",
    "\n",
    "- The lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Stacked RNN (or Multi-layer RNN)\n",
    "\n",
    "The hidden states from RNN layer $i$ are the inputs to RNN layer $i+1$\n",
    "\n",
    "<br><div align='center'><img src=\"figs/stacked_rnn_02.png\" width='30%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Stacked RNN (or Multi-layer RNN)\n",
    "\n",
    "\n",
    "- Multi-layer or stacked RNNs allow a network to compute more complex representations: they work better than just have one layer of high-dimensional encodings!\n",
    "\n",
    "- High-performing RNNs are usually **multi-layer BUT NOT as deep as convolutional or feed-forward networks**\n",
    "\n",
    "- In a 2017 paper, Britz et al. find that for Neural Machine Translation, 2 to 4 layers is best for the encoder RNN, and 4 layers is best for the decoder RNN:\n",
    "    - Often 2 layers is a lot better than 1, and 3 might be a little better than 2\n",
    "    -  Usually, skip-connections/dense-connections are needed to train deeper RNNs (e.g., 8 layers)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "key",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "30",
  "rise": {
   "autolaunch": true,
   "overlay": "<div class='myheader'>Natural Language Processing</div>",
   "transition": "linear"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Summary",
   "toc_cell": false,
   "toc_position": {
    "height": "47px",
    "left": "1143px",
    "top": "173px",
    "width": "210.344px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
