{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.di.uniroma1.it/sites/all/themes/sapienza_bootstrap/logo.png' width=\"200\"/>  \n",
    "\n",
    "# Part_1_11_Vector Semantics (Sparse)\n",
    "\n",
    "In Natural Language Processing (`NLP`), vector semantics provides a powerful framework for representing words and documents in a numerical form, enabling efficient computation and semantic analysis. Sparse vector representations, such as **Bag of Words (`BoW`)**, **TF-IDF**, and **Pointwise Mutual Information (`PPMI`)**, have been foundational in the evolution of `NLP`. These approaches rely on statistical co-occurrence patterns and word frequency to capture linguistic meaning, forming the basis for more sophisticated methods like dense embeddings and contextualized models.\n",
    "\n",
    "Sparse representations are particularly useful in understanding the core principles of vector semantics and building intuition about the role of word-document relationships in tasks like text classification, clustering, and retrieval systems.\n",
    "\n",
    "### **Objectives:**\n",
    "In this notebook, Parham provides an overview of sparse vector semantics, including the key methods used to represent text data and their significance in `NLP`. Through practical exercises, Parham will demonstrate the implementation of **Bag of Words (`BoW`)** for document representation, **TF-IDF** to highlight significant terms within documents, and **PPMI** to extract meaningful statistical relationships from co-occurrence matrices.\n",
    "\n",
    "### **References:**\n",
    "- [https://www.datacamp.com/tutorial/python-bag-of-words-model](https://www.datacamp.com/tutorial/python-bag-of-words-model)  \n",
    "- [https://spotintelligence.com/2022/12/20/bag-of-words-python](https://spotintelligence.com/2022/12/20/bag-of-words-python/)  \n",
    "- [https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus](https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus)   \n",
    "\n",
    "### **Tutors**:\n",
    "- Professor Stefano Farali\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: Stefano.faralli@uniroma1.it\n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/stefano-faralli-b1183920/) \n",
    "- Professor Iacopo Masi\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: masi@di.uniroma1.it  \n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/iacopomasi/)  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ae/Github-desktop-logo-symbol.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **GitHub**: [GitHub](https://github.com/iacopomasi)  \n",
    "    \n",
    "\n",
    "### **Contributors:**\n",
    "- Parham Membari  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: p.membari96@gmail.com  \n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/p-mem/)  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ae/Github-desktop-logo-symbol.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **GitHub**: [GitHub](https://github.com/parham075)  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ec/Medium_logo_Monogram.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Medium**: [Medium](https://medium.com/@p.membari96)  \n",
    "\n",
    "**Table of Contents:**\n",
    "1. Import Libraries  \n",
    "2. Introduction to Vector Semantics  \n",
    "3. Bag of Words (`BoW`) Representation      \n",
    "4. Term Frequency-Inverse Document Frequency (`TF-IDF`)\n",
    "5. Pointwise Mutual Information (`PMI`)   \n",
    "6. Closing Thoughts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/p/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/p/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tag import pos_tag\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Vector Semantics\n",
    "Vector semantics is a methodology in `NLP` that uses numerical representations to encode the meaning of words, phrases, or entire documents. These numerical representations are essential for enabling machines to process and analyze human language. By transforming textual data into vectors, computational models can perform mathematical operations to assess similarity, context, and relationships between words or documents.\n",
    "\n",
    "### Why Vector Semantics?\n",
    "- **Quantitative Representation**: Text data, being inherently qualitative, is challenging for computers to process directly. Vector semantics bridges this gap by converting text into a mathematical form.\n",
    "- **Efficient Computation**: Mathematical representations allow for quick calculations of similarity, clustering, and classification tasks.\n",
    "- **Foundation for Machine Learning Models**: Many machine learning models rely on vectorized representations of data as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bag of Words (`BoW`) Representation \n",
    "\n",
    "**Bag of Words** (`BoW`) is a simple and widely-used representation of text data in Natural Language Processing. It represents text as a collection of words, ignoring grammar, word order, and context, while preserving the frequency of words.BoW boadly used in tasks such as text classification and sentiment analysis. This is important because machine learning algorithms can‚Äôt process textual data. The process of converting the text to numbers is known as feature extraction or feature encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Understanding Bag of Words with example:\n",
    "Imagine two sentences:\n",
    "1. Document 1: \"Natural Language Processing is amazing.\"\n",
    "2. Document 2: \"Language models are important for NLP.\"\n",
    "\n",
    "\n",
    "The `BoW` model begins by creating a vocabulary, a unique list of all words across the corpus. Each document is then represented as a vector of word frequencies. Table below, represents the Bag of Words vectors:\n",
    "\n",
    "| **Vocabulary** | **Document 1** | **Document 2** |\n",
    "|-----------------|----------------|----------------|\n",
    "| Natural         | 1              | 0              |\n",
    "| Language        | 1              | 1              |\n",
    "| Processing      | 1              | 0              |\n",
    "| is              | 1              | 0              |\n",
    "| amazing         | 1              | 0              |\n",
    "| models          | 0              | 1              |\n",
    "| are             | 0              | 1              |\n",
    "| important       | 0              | 1              |\n",
    "| for             | 0              | 1              |\n",
    "| NLP             | 0              | 1              |\n",
    "\n",
    "Each position in the vector corresponds to a word in the vocabulary, and the value represents its frequency in the document.\n",
    "\n",
    "### How to implement `BoW`\n",
    "The steps involved to create `BoW` are:\n",
    "- Tokenization: Split the text into individual words or tokens.\n",
    "- Preprocessing:\n",
    "    - Convert text to lowercase.\n",
    "    - Remove special characters, punctuation, and numbers.\n",
    "    - Remove stopwords (e.g., \"the\", \"is\", \"and\").\n",
    "- Apply stemming or lemmatization to normalize words.\n",
    "- Vocabulary Creation: Build a unique list of words from the corpus.\n",
    "- Vectorization: Represent each document as a vector of word frequencies based on the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document 1</th>\n",
       "      <th>Document 2</th>\n",
       "      <th>Document 3</th>\n",
       "      <th>Document 4</th>\n",
       "      <th>Document 5</th>\n",
       "      <th>Document 6</th>\n",
       "      <th>Document 7</th>\n",
       "      <th>Document 8</th>\n",
       "      <th>Document 9</th>\n",
       "      <th>Document 10</th>\n",
       "      <th>Document 11</th>\n",
       "      <th>Document 12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ad</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allow</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyz</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anoth</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>around</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unstructur</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Document 1  Document 2  Document 3  Document 4  Document 5  \\\n",
       "ad                   0           0           0           0           0   \n",
       "allow                0           1           0           0           0   \n",
       "analyz               0           0           1           1           0   \n",
       "anoth                0           0           0           0           0   \n",
       "around               0           0           0           0           0   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "type                 0           0           0           0           1   \n",
       "unit                 0           0           0           0           0   \n",
       "unstructur           0           0           1           0           0   \n",
       "word                 1           0           0           1           2   \n",
       "work                 0           1           0           0           0   \n",
       "\n",
       "            Document 6  Document 7  Document 8  Document 9  Document 10  \\\n",
       "ad                   0           0           1           0            0   \n",
       "allow                0           1           0           0            0   \n",
       "analyz               0           0           1           0            1   \n",
       "anoth                0           0           0           0            1   \n",
       "around               0           0           0           0            0   \n",
       "...                ...         ...         ...         ...          ...   \n",
       "type                 0           0           0           0            0   \n",
       "unit                 1           0           0           0            0   \n",
       "unstructur           0           0           0           0            0   \n",
       "word                 0           2           1           0            1   \n",
       "work                 0           0           0           0            0   \n",
       "\n",
       "            Document 11  Document 12  \n",
       "ad                    0            0  \n",
       "allow                 0            0  \n",
       "analyz                0            0  \n",
       "anoth                 0            0  \n",
       "around                1            0  \n",
       "...                 ...          ...  \n",
       "type                  0            0  \n",
       "unit                  0            0  \n",
       "unstructur            0            0  \n",
       "word                  2            0  \n",
       "work                  0            0  \n",
       "\n",
       "[78 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "paragraph = \"\"\"\n",
    "By tokenizing, you can conveniently split up text by word or by sentence. \n",
    "This will allow you to work with smaller pieces of text that are still relatively coherent and meaningful even outside of the context of the rest of the text. \n",
    "It‚Äôs your first step in turning unstructured data into structured data, which is easier to analyze.\n",
    "\n",
    "When you‚Äôre analyzing text, you‚Äôll be tokenizing by word and tokenizing by sentence. \n",
    "Here‚Äôs what both types of tokenization bring to the table:\n",
    "\n",
    "Tokenizing by word: Words are like the atoms of natural language. \n",
    "They‚Äôre the smallest unit of meaning that still makes sense on its own. \n",
    "Tokenizing your text by word allows you to identify words that come up particularly often. \n",
    "For example, if you were analyzing a group of job ads, \n",
    "then you might find that the word ‚ÄúPython‚Äù comes up often. \n",
    "That could suggest high demand for Python knowledge, but you‚Äôd need to look deeper to know more.\n",
    "\n",
    "Tokenizing by sentence: When you tokenize by sentence, \n",
    "you can analyze how those words relate to one another and see more context. \n",
    "Are there a lot of negative words around the word ‚ÄúPython‚Äù because the hiring manager doesn‚Äôt like Python? \n",
    "Are there more terms from the domain of herpetology than the domain of software development, \n",
    "suggesting that you may be dealing with an entirely different kind of python than you were expecting?\n",
    "\n",
    "\"\"\"\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Step 2: Preprocessing each sentence\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Remove special characters, numbers, and punctuations\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # Convert to lowercase\n",
    "    review = review.lower()\n",
    "    # Split into words\n",
    "    review = review.split()\n",
    "    # Remove stopwords and apply stemming\n",
    "    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    # Join the words back into a sentence\n",
    "    review = ' '.join(review)\n",
    "    # Add the cleaned sentence to the corpus\n",
    "    corpus.append(review)\n",
    "\n",
    "# Step 3: Create Bag of Words model\n",
    "cv = CountVectorizer(max_features=1500)  # Limit to top 1500 features (if necessary)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "vocabulary = cv.get_feature_names_out()\n",
    "\n",
    "# Create DataFrame with each sentence as a column\n",
    "bow_df = pd.DataFrame(X.T, index=vocabulary, columns=[f\"Document {i+1}\" for i in range(X.shape[0])])\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Term Frequency-Inverse Document Frequency (`TF-IDF`)  \n",
    "\n",
    "While the Bag of Words model provides a simple representation of text data, it does not account for the importance of words within a document or across multiple documents. **Term Frequency-Inverse Document Frequency (TF-IDF)** improves upon this by assigning a weight to each word that reflects its relevance to a specific document in the corpus.\n",
    "\n",
    "\n",
    "### 4.1 Understanding TF-IDF\n",
    "\n",
    "The TF-IDF score for a term $ t $ in a document $ d $ is calculated as:\n",
    "\\[\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "\\]\n",
    "Where:\n",
    "1. **Term Frequency (TF)**: Measures how frequently a term appears in a document.\n",
    "   \\[\n",
    "   \\text{TF}(t, d) = \\frac{\\text{Number of occurrences of } t \\text{ in } d}{\\text{Total number of terms in } d}\n",
    "   \\]\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: Measures how important a term is by reducing the weight of commonly used words. To avoid division by zero, smoothing is applied:\n",
    "   \\[\n",
    "   \\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents} + 1}{\\text{Number of documents containing } t + 1}\\right) + 1\n",
    "   \\]\n",
    "\n",
    "The result is a weighted score that increases with the frequency of the term in the document but decreases with its frequency across the corpus.\n",
    "\n",
    "### 4.2 Example: Calculating TF-IDF  \n",
    "\n",
    "Let‚Äôs revisit the two documents:  \n",
    "1. Document 1: \"Natural Language Processing is amazing.\"  \n",
    "2. Document 2: \"Language models are important for NLP.\"  \n",
    "\n",
    "The vocabulary is:  \n",
    "`['amazing', 'are', 'for', 'important', 'is', 'language', 'models', 'natural', 'nlp', 'processing']`  \n",
    "\n",
    "We calculate TF, IDF, and TF-IDF for each term:\n",
    "\n",
    "| **Vocabulary** | **TF (Doc 1)**   | **TF (Doc 2)**   | **IDF**                                             | **TF-IDF (Doc 1)**       | **TF-IDF (Doc 2)**       |\n",
    "|-----------------|------------------|------------------|-----------------------------------------------------|--------------------------|--------------------------|\n",
    "| amazing         | $ \\frac{1}{5} = 0.2 $ | $ \\frac{0}{7} = 0.0 $ | $ \\log\\left(\\frac{2+1}{1+1}\\right) + 1 = 1.41 $  | $ 0.2 \\times 1.41 = 0.28 $  | $ 0.0 \\times 1.41 = 0.0 $  |\n",
    "| are             | $ \\frac{0}{5} = 0.0 $ | $ \\frac{1}{7} \\approx 0.14 $ | $ \\log\\left(\\frac{2+1}{1+1}\\right) + 1 = 1.41 $  | $ 0.0 \\times 1.41 = 0.0 $  | $ 0.14 \\times 1.41 \\approx 0.20 $ |\n",
    "| for             | $ \\frac{0}{5} = 0.0 $ | $ \\frac{1}{7} \\approx 0.14 $ | $ \\log\\left(\\frac{2+1}{1+1}\\right) + 1 = 1.41 $  | $ 0.0 \\times 1.41 = 0.0 $  | $ 0.14 \\times 1.41 \\approx 0.20 $ |\n",
    "| important       | $ \\frac{0}{5} = 0.0 $ | $ \\frac{1}{7} \\approx 0.14 $ | $ \\log\\left(\\frac{2+1}{1+1}\\right) + 1 = 1.41 $  | $ 0.0 \\times 1.41 = 0.0 $  | $ 0.14 \\times 1.41 \\approx 0.20 $ |\n",
    "| is              | $ \\frac{1}{5} = 0.2 $ | $ \\frac{0}{7} = 0.0 $ | $ \\log\\left(\\frac{2+1}{1+1}\\right) + 1 = 1.41 $  | $ 0.2 \\times 1.41 = 0.28 $  | $ 0.0 \\times 1.41 = 0.0 $  |\n",
    "| language        | $ \\frac{1}{5} = 0.2 $ | $ \\frac{1}{7} \\approx 0.14 $ | $ \\log\\left(\\frac{2+1}{2+1}\\right) + 1 = 1.00 $  | $ 0.2 \\times 1.00 = 0.20 $  | $ 0.14 \\times 1.00 \\approx 0.14 $ |\n",
    "| models          | $ \\frac{0}{5} = 0.0 $ | $ \\frac{1}{7} \\approx 0.14 $ | $ \\log\\left(\\frac{2+1}{1+1}\\right) + 1 = 1.41 $  | $ 0.0 \\times 1.41 = 0.0 $  | $ 0.14 \\times 1.41 \\approx 0.20 $ |\n",
    "| natural         | $ \\frac{1}{5} = 0.2 $ | $ \\frac{0}{7} = 0.0 $ | $ \\log\\left(\\frac{2+1}{1+1}\\right) + 1 = 1.41 $  | $ 0.2 \\times 1.41 = 0.28 $  | $ 0.0 \\times 1.41 = 0.0 $  |\n",
    "| nlp             | $ \\frac{0}{5} = 0.0 $ | $ \\frac{1}{7} \\approx 0.14 $ | $ \\log\\left(\\frac{2+1}{1+1}\\right) + 1 = 1.41 $  | $ 0.0 \\times 1.41 = 0.0 $  | $ 0.14 \\times 1.41 \\approx 0.20 $ |\n",
    "| processing      | $ \\frac{1}{5} = 0.2 $ | $ \\frac{0}{7} = 0.0 $ | $ \\log\\left(\\frac{2+1}{1+1}\\right) + 1 = 1.41 $  | $ 0.2 \\times 1.41 = 0.28 $  | $ 0.0 \\times 1.41 = 0.0 $  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['amazing', 'are', 'for', 'important', 'is', 'language', 'models', 'natural', 'nlp', 'processing']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "Document 1: [0.4710778123316179, 0.0, 0.0, 0.0, 0.4710778123316179, 0.335175743327926, 0.0, 0.4710778123316179, 0.0, 0.4710778123316179]\n",
      "Document 2: [0.0, 0.42615959880289433, 0.42615959880289433, 0.42615959880289433, 0.0, 0.3032160644503863, 0.42615959880289433, 0.0, 0.42615959880289433, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Step 2: Preprocess the text (lowercase, remove punctuation)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char if char.isalnum() or char.isspace() else ' ' for char in text])\n",
    "    return text.split()\n",
    "\n",
    "# Step 3: Create the vocabulary\n",
    "def build_vocabulary(corpus):\n",
    "    vocabulary = set()\n",
    "    for document in corpus:\n",
    "        vocabulary.update(preprocess_text(document))\n",
    "    return sorted(vocabulary)\n",
    "\n",
    "# Step 4: Calculate Term Frequency (TF)\n",
    "def compute_tf(document, vocabulary):\n",
    "    term_count = Counter(preprocess_text(document))\n",
    "    total_terms = sum(term_count.values())\n",
    "    return [term_count[word] / total_terms for word in vocabulary]\n",
    "\n",
    "# Step 5: Calculate Inverse Document Frequency (IDF) with smoothing\n",
    "def compute_idf(corpus, vocabulary):\n",
    "    num_documents = len(corpus)\n",
    "    idf = []\n",
    "    for word in vocabulary:\n",
    "        doc_count = sum(1 for document in corpus if word in preprocess_text(document))\n",
    "        idf.append(math.log((1 + num_documents) / (1 + doc_count)) + 1)\n",
    "    return idf\n",
    "\n",
    "# Step 6: Compute TF-IDF for each document\n",
    "def compute_tfidf(corpus):\n",
    "    vocabulary = build_vocabulary(corpus)\n",
    "    tf_matrix = [compute_tf(document, vocabulary) for document in corpus]\n",
    "    idf = compute_idf(corpus, vocabulary)\n",
    "    tfidf_matrix = [[tf * idf[idx] for idx, tf in enumerate(tf_doc)] for tf_doc in tf_matrix]\n",
    "    # Normalize each row (document)\n",
    "    tfidf_matrix_normalized = [\n",
    "        [value / norm(doc) if norm(doc) != 0 else 0 for value in doc]\n",
    "        for doc in tfidf_matrix\n",
    "    ]\n",
    "    return vocabulary, tfidf_matrix_normalized\n",
    "\n",
    "# Run the implementation\n",
    "corpus = [\n",
    "    \"Natural Language Processing is amazing.\",\n",
    "    \"Language models are important for NLP.\"\n",
    "]\n",
    "vocabulary, tfidf_matrix = compute_tfidf(corpus)\n",
    "\n",
    "# Step 7: Display the results\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "for idx, doc_tfidf in enumerate(tfidf_matrix):\n",
    "    print(f\"Document {idx + 1}: {doc_tfidf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1**: Representing Text with TF-IDF Using `TfidfVectorizer`  \n",
    "\n",
    "**Objective**  \n",
    "In this exercise, you will use the `TfidfVectorizer` from `sklearn` to calculate and visualize the **TF-IDF scores** for a given corpus.  \n",
    "\n",
    "Given a small corpus of two documents:  \n",
    "1. Document 1: `\"Natural Language Processing is amazing.\"`  \n",
    "2. Document 2: `\"Language models are important for NLP.\"`  \n",
    "\n",
    "**Your Task**  \n",
    "\n",
    "1. Use the `TfidfVectorizer` to calculate the TF-IDF scores for the given corpus.  \n",
    "2. Extract the vocabulary and TF-IDF matrix.   \n",
    "\n",
    "\n",
    "**Expected Output**  \n",
    "\n",
    "- **Vocabulary**: A list of unique words in the corpus.  \n",
    "- **TF-IDF Matrix**: A table where rows represent documents and columns represent words, with values showing the TF-IDF score for each word in each document.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here\n",
    "# Example Corpus\n",
    "corpus = [\n",
    "    \"Natural Language Processing is amazing.\",\n",
    "    \"Language models are important for NLP.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['amazing' 'are' 'for' 'important' 'is' 'language' 'models' 'natural'\n",
      " 'nlp' 'processing']\n",
      "TF-IDF Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>are</th>\n",
       "      <th>for</th>\n",
       "      <th>important</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>models</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.471078</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.471078</td>\n",
       "      <td>0.335176</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.471078</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.471078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303216</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    amazing      are      for  important        is  language   models  \\\n",
       "0  0.471078  0.00000  0.00000    0.00000  0.471078  0.335176  0.00000   \n",
       "1  0.000000  0.42616  0.42616    0.42616  0.000000  0.303216  0.42616   \n",
       "\n",
       "    natural      nlp  processing  \n",
       "0  0.471078  0.00000    0.471078  \n",
       "1  0.000000  0.42616    0.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title üëÄ Solution\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Display the vocabulary and TF-IDF matrix\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\")\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pointwise Mutual Information (`PMI`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Closing Thoughts "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
