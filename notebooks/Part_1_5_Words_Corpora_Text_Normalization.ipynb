{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part_1_5_Words_Corpora_Text_Normalization\n",
    "In Natural Language Processing (NLP), key techniques such as tokenization, stemming, and sentence segmentation are fundamental for transforming raw text into a structured format that can be effectively analyzed by language models. Byte-Pair Encoding (BPE) tokenization helps handle rare words and out-of-vocabulary terms by breaking text into subword units. Stemming, using algorithms like the Porter Stemmer, reduces words to their root forms, ensuring consistency across the text for better analysis. Sentence segmentation is crucial for dividing text into meaningful sentences, allowing for more accurate processing in downstream tasks. These techniques play a vital role in preparing text for various NLP tasks, ensuring that language data is in a normalized and analyzable state.\n",
    "\n",
    "### **Objectives:**\n",
    "By the end of this notebook, Parham will have a thorough understanding of tokenization and its importance in NLP, specifically learning how to implement **Byte-Pair Encoding (BPE)** to handle words and subwords. He will explore the **Porter Stemmer**, gaining insight into how it reduces words to their base forms and why this is essential for text normalization. Additionally, Parham will learn to apply **Sentence Segmentation** to split text into meaningful sentences for deeper analysis. Through hands-on coding exercises, he will gain practical experience using these techniques, utilizing Python libraries like `NLTK` and `SpaCy` to prepare text for NLP tasks.\n",
    "\n",
    "**Table of Contents:** \n",
    "1. Import Libraries\n",
    "2. Tokenization Techniques\n",
    "3. Text Normalization: Porter Stemmer\n",
    "4. Sentence Segmentation\n",
    "5. Closing Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "from loguru import logger\n",
    "import nltk\n",
    "import pandas as pd \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import subprocess\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization Techniques\n",
    "Byte-pair encoding was first introduced in 1994 as a simple data compression technique by iteratively replacing the most frequent pair of bytes in a sequence with a single, unused byte.\n",
    "Imagine Parham is reading a really big book, but some of the words are really long or tricky, and he might not know all of them. To make things easier, Parham can break the long words into smaller, simpler parts or pieces. This way, he can still understand the book without needing to know every single big word.\n",
    "\n",
    "Byte-Pair Encoding (BPE) is a popular technique used in natural language processing for tokenizing text into subword units. The idea is to break down words into smaller, more frequent pieces, allowing models to efficiently handle rare or unknown words. BPE is especially useful in scenarios where the vocabulary is limited but the text contains a large variety of words, including compound or out-of-vocabulary words.\n",
    "\n",
    "The algorithm works by:\n",
    "- Starting with a sequence of individual characters.\n",
    "- Finding the most frequent pair of consecutive characters (or subwords).\n",
    "- Merging this pair into a single token.\n",
    "- Repeating the process until a predefined number of merges or a desired vocabulary size is reached.\n",
    "\n",
    "This process allows models to represent both common words and subwords effectively, making it easier to process any text.\n",
    "\n",
    "In the cell below Parham will an example of BEP tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After merge 1:\n",
      "Best Pair: ('s', '</w>')\n",
      "Updated Vocabulary: {'T o k e n i z a t i o n </w>': 2, 'i s</w>': 2, 't h e </w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k i n g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u e n c e </w>': 1, 't e x t </w>': 2, 'i n t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k e n s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e </w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v e n </w>': 1, 'i n d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t e n </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'i n </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s i n g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n , </w>': 1, 'n a m e d </w>': 1, 'e n t i t y </w>': 1, 'r e c o g n i t i o n , </w>': 1, 'a n d </w>': 1, 's e n t i m e n t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e </w>': 1, 'r e s u l t i n g </w>': 1, 't o k e n s</w>': 2, 'a r e </w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'i n p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a t i o n , </w>': 1, 'w h e r e </w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s e n t a t i o n s</w>': 1, 'f o r </w>': 1, 'm a c h i n e </w>': 1, 'l e a r n i n g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 2:\n",
      "Best Pair: ('e', 'n')\n",
      "Updated Vocabulary: {'T o k en i z a t i o n </w>': 2, 'i s</w>': 2, 't h e </w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k i n g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e </w>': 1, 't e x t </w>': 2, 'i n t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e </w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'i n d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'i n </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s i n g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n , </w>': 1, 'n a m e d </w>': 1, 'en t i t y </w>': 1, 'r e c o g n i t i o n , </w>': 1, 'a n d </w>': 1, 's en t i m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e </w>': 1, 'r e s u l t i n g </w>': 1, 't o k en s</w>': 2, 'a r e </w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'i n p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a t i o n , </w>': 1, 'w h e r e </w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a t i o n s</w>': 1, 'f o r </w>': 1, 'm a c h i n e </w>': 1, 'l e a r n i n g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 3:\n",
      "Best Pair: ('i', 'n')\n",
      "Updated Vocabulary: {'T o k en i z a t i o n </w>': 2, 'i s</w>': 2, 't h e </w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e </w>': 1, 't e x t </w>': 2, 'in t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e </w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n , </w>': 1, 'n a m e d </w>': 1, 'en t i t y </w>': 1, 'r e c o g n i t i o n , </w>': 1, 'a n d </w>': 1, 's en t i m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e </w>': 1, 'r e s u l t in g </w>': 1, 't o k en s</w>': 2, 'a r e </w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a t i o n , </w>': 1, 'w h e r e </w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a t i o n s</w>': 1, 'f o r </w>': 1, 'm a c h in e </w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 4:\n",
      "Best Pair: ('e', '</w>')\n",
      "Updated Vocabulary: {'T o k en i z a t i o n </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n , </w>': 1, 'n a m e d </w>': 1, 'en t i t y </w>': 1, 'r e c o g n i t i o n , </w>': 1, 'a n d </w>': 1, 's en t i m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 't o k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a t i o n , </w>': 1, 'w h e r e</w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a t i o n s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 5:\n",
      "Best Pair: ('t', 'i')\n",
      "Updated Vocabulary: {'T o k en i z a ti o n </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti o n , </w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti o n , </w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 't o k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a ti o n , </w>': 1, 'w h e r e</w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a ti o n s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 6:\n",
      "Best Pair: ('t', 'o')\n",
      "Updated Vocabulary: {'T o k en i z a ti o n </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 'to k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti o n , </w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti o n , </w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c to r i z a ti o n , </w>': 1, 'w h e r e</w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a ti o n s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 7:\n",
      "Best Pair: ('o', 'n')\n",
      "Updated Vocabulary: {'T o k en i z a ti on </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 'to k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti on , </w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti on , </w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c to r i z a ti on , </w>': 1, 'w h e r e</w>': 1, 'c on v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a ti on s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 8:\n",
      "Best Pair: ('a', 'l')\n",
      "Updated Vocabulary: {'T o k en i z a ti on </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m al l e r </w>': 1, 'u n i t s</w>': 1, 'c al l e d </w>': 1, 'to k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u al </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r al </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti on , </w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti on , </w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n al y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c al l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c to r i z a ti on , </w>': 1, 'w h e r e</w>': 1, 'c on v e r t e d </w>': 1, 'n u m e r i c al </w>': 1, 'r e p r e s en t a ti on s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 9:\n",
      "Best Pair: (',', '</w>')\n",
      "Updated Vocabulary: {'T o k en i z a ti on </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m al l e r </w>': 1, 'u n i t s</w>': 1, 'c al l e d </w>': 1, 'to k en s ,</w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s ,</w>': 1, 'p h r a s e s ,</w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u al </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r al </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti on ,</w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti on ,</w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n al y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c al l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s ,</w>': 1, 'v e c to r i z a ti on ,</w>': 1, 'w h e r e</w>': 1, 'c on v e r t e d </w>': 1, 'n u m e r i c al </w>': 1, 'r e p r e s en t a ti on s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 10:\n",
      "Best Pair: ('ti', 'on')\n",
      "Updated Vocabulary: {'T o k en i z a tion </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m al l e r </w>': 1, 'u n i t s</w>': 1, 'c al l e d </w>': 1, 'to k en s ,</w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s ,</w>': 1, 'p h r a s e s ,</w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u al </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r al </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a tion ,</w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i tion ,</w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n al y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c al l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s ,</w>': 1, 'v e c to r i z a tion ,</w>': 1, 'w h e r e</w>': 1, 'c on v e r t e d </w>': 1, 'n u m e r i c al </w>': 1, 'r e p r e s en t a tion s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    Given a vocabulary (dictionary mapping words to frequency counts), returns a \n",
    "    dictionary of tuples representing the frequency count of pairs of characters \n",
    "    in the vocabulary.\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"\n",
    "    Given a pair of characters and a vocabulary, returns a new vocabulary with the \n",
    "    pair of characters merged together wherever they appear.\n",
    "    \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_vocab(data):\n",
    "    \"\"\"\n",
    "    Given a list of strings, returns a dictionary of words mapping to their frequency \n",
    "    count in the data.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(int)\n",
    "    for line in data:\n",
    "        for word in line.split():\n",
    "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "def byte_pair_encoding(data, n):\n",
    "    \"\"\"\n",
    "    Given a list of strings and an integer n, returns a list of n merged pairs\n",
    "    of characters found in the vocabulary of the input data.\n",
    "    \"\"\"\n",
    "    vocab = get_vocab(data)\n",
    "    for i in range(n):\n",
    "        pairs = get_stats(vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        print(f\"\\nAfter merge {i + 1}:\")\n",
    "        print(f\"Best Pair: {best}\")\n",
    "        print(f\"Updated Vocabulary: {vocab}\")\n",
    "    return vocab\n",
    "\n",
    "# Example usage:\n",
    "corpus = '''Tokenization is the process of breaking down \n",
    "a sequence of text into smaller units called tokens,\n",
    "which can be words, phrases, or even individual characters.\n",
    "Tokenization is often the first step in natural languages processing tasks \n",
    "such as text classification, named entity recognition, and sentiment analysis.\n",
    "The resulting tokens are typically used as input to further processing steps,\n",
    "such as vectorization, where the tokens are converted\n",
    "into numerical representations for machine learning models to use.'''\n",
    "data = corpus.split(' ')\n",
    "\n",
    "n = 10\n",
    "bpe_pairs = byte_pair_encoding(data, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    " The algorithm for Byte-Pair Encoding (BPE) in the previous example is a simplified version for educational purposes. While it illustrates the core concept of BPE, it might not be efficient for processing large datasets or in production environments. Here are some reasons why the naive implementation could be problematic with big data.\n",
    " 1. **Computational Complexity Counting Pairs:** The algorithm counts pairs of characters for every iteration, which can become computationally expensive as the number of tokens increases. For large datasets, this might lead to significant slowdowns.\n",
    "Repeated Iteration: The naive implementation involves multiple passes over the data to find the most frequent pairs, which can be inefficient when working with massive corpora.\n",
    "2. **Memory Usage\n",
    "Storage of Vocabulary:** The implementation keeps an entire vocabulary in memory, which can become unmanageable with large datasets. Each unique token needs to be stored, and the size of this vocabulary can grow quickly.\n",
    "3. **Inefficiency in Merging\n",
    "String Manipulation:** Frequent string replacements can be slow in Python. The current method creates new strings for each merge operation, leading to increased overhead.\n",
    "4. **Scalability Issues\n",
    "Lack of Parallelism:** The algorithm does not leverage parallel processing. For big data, parallelizing tasks can significantly reduce processing time.\n",
    "More Efficient Approaches\n",
    "For working with large datasets, here are a few alternative approaches:\n",
    "\n",
    "**Optimized Libraries:**\n",
    "\n",
    "  Use optimized libraries like `sentencepiece` or `subword-nmt`, which are designed for BPE and can handle larger datasets more efficiently. These libraries implement BPE in a way that‚Äôs optimized for speed and memory usage.\n",
    "\n",
    "- **Stream Processing**:\n",
    "\n",
    "  Instead of loading the entire dataset into memory, consider processing it in chunks or streams. This way, you can update your vocabulary and statistics without needing to load everything at once.\n",
    "- **Use Hash Maps for Counting**:\n",
    "\n",
    "  Instead of using lists or arrays, using hash maps (dictionaries) can improve the efficiency of counting pairs and merging them.\n",
    "\n",
    "As an excersise, please implement BPE using `sentencepiece` using the corpus provided in the previous cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/p/miniconda3/envs/nlp/lib/python3.10/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here\n",
    "\n",
    "! pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "with open(\"input.txt\",\"w+\") as f:\n",
    "  f.write(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: ['‚ñÅT', 'oken', 'ization', '‚ñÅ', 'is', '‚ñÅ', 'the', '‚ñÅprocess', '‚ñÅof', '‚ñÅb', 're', 'a', 'k', 'i', 'ng', '‚ñÅ', 'd', 'o', 'w', 'n', '‚ñÅa', '‚ñÅse', 'q', 'u', 'en', 'ce', '‚ñÅof', '‚ñÅ', 'te', 'x', 't', '‚ñÅin', 'to', '‚ñÅs', 'm', 'alle', 'r', '‚ñÅ', 'u', 'nit', 's', '‚ñÅ', 'call', 'ed', '‚ñÅtokens', ',', '‚ñÅwh', 'i', 'ch', '‚ñÅca', 'n', '‚ñÅb', 'e', '‚ñÅw', 'or', 'd', 's', ',', '‚ñÅp', 'h', 'r', 'as', 'es', ',', '‚ñÅ', 'or', '‚ñÅ', 'e', 've', 'n', '‚ñÅin', 'd', 'i', 'v', 'i', 'd', 'u', 'al', '‚ñÅ', 'ch', 'ar', 'ac', 'te', 'rs', '.', '‚ñÅT', 'oken', 'ization', '‚ñÅ', 'is', '‚ñÅof', 't', 'en', '‚ñÅ', 'the', '‚ñÅ', 'fi', 'rs', 't', '‚ñÅstep', '‚ñÅin', '‚ñÅna', 't', 'ur', 'al', '‚ñÅ', 'la', 'ng', 'ua', 'g', 'es', '‚ñÅprocess', 'i', 'ng', '‚ñÅt', 'as', 'k', 's', '‚ñÅ', 'su', 'ch', '‚ñÅ', 'as', '‚ñÅ', 'te', 'x', 't', '‚ñÅc', 'la', 'ssi', 'fi', 'c', 'ation', ',', '‚ñÅna', 'me', 'd', '‚ñÅ', 'enti', 'ty', '‚ñÅre', 'co', 'g', 'ni', 'tion', ',', '‚ñÅan', 'd', '‚ñÅs', 'enti', 'me', 'n', 't', '‚ñÅan', 'a', 'ly', 's', 'is', '.', '‚ñÅT', 'he', '‚ñÅre', 'su', 'l', 'ti', 'ng', '‚ñÅtokens', '‚ñÅa', 're', '‚ñÅ', 'ty', 'p', 'ical', 'ly', '‚ñÅuse', 'd', '‚ñÅ', 'as', '‚ñÅin', 'p', 'u', 't', '‚ñÅto', '‚ñÅf', 'ur', 'the', 'r', '‚ñÅprocess', 'i', 'ng', '‚ñÅstep', 's', ',', '‚ñÅ', 'su', 'ch', '‚ñÅ', 'as', '‚ñÅ', 've', 'ct', 'or', 'ization', ',', '‚ñÅwh', 'e', 're', '‚ñÅ', 'the', '‚ñÅtokens', '‚ñÅa', 're', '‚ñÅc', 'on', 've', 'r', 'te', 'd', '‚ñÅin', 'to', '‚ñÅ', 'n', 'u', 'me', 'r', 'ical', '‚ñÅre', 'p', 're', 'sent', 'ation', 's', '‚ñÅf', 'or', '‚ñÅ', 'ma', 'ch', 'in', 'e', '‚ñÅ', 'l', 'e', 'ar', 'ni', 'ng', '‚ñÅm', 'o', 'd', 'e', 'l', 's', '‚ñÅto', '‚ñÅuse', '.']\n",
      "Decoded: Tokenization is the process of breaking down a sequence of text into smaller units called tokens, which can be words, phrases, or even individual characters. Tokenization is often the first step in natural languages processing tasks such as text classification, named entity recognition, and sentiment analysis. The resulting tokens are typically used as input to further processing steps, such as vectorization, where the tokens are converted into numerical representations for machine learning models to use.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: input.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 100\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: input.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 8 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=511\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=29\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 8 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=250\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 153 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 8\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 61\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 61 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=109 obj=15.6515 num_tokens=218 num_tokens/piece=2\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=99 obj=14.9491 num_tokens=219 num_tokens/piece=2.21212\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "# @title üëÄ Solution\n",
    "\n",
    "# Train a SentencePiece model\n",
    "spm.SentencePieceTrainer.train(input=\"input.txt\", model_prefix='m', vocab_size=100)\n",
    "\n",
    "# Load the model\n",
    "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
    "\n",
    "encoded = sp.encode(corpus, out_type=str)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "# Decode back to text\n",
    "decoded = sp.decode(encoded)\n",
    "print(\"Decoded:\", decoded)\n",
    "os.remove(\"input.txt\")\n",
    "os.remove(\"m.model\")\n",
    "os.remove(\"m.vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Normalization: Porter Stemmer\n",
    "\n",
    "Text normalization is a critical preprocessing step in Natural Language Processing (NLP) that involves transforming text into a standard format. One common method of normalization is stemming, which reduces words to their root forms. In linguistics, a **stem** is a part of a word that is common to all of its inflected variants. \n",
    "\n",
    "For example:\n",
    "- Integrate\n",
    "- Integrated\n",
    "- Integration\n",
    "- Integrating\n",
    "\n",
    "The above words are inflected variants of **Integrat**. Hence, **Integrat** is a stem. To this stem, we can add different suffixes to form various words. The process of reducing such inflected (or sometimes derived) words to their stem is known as stemming. For instance, \"Integrate,\" \"Integrated,\" \"Integration,\" and \"Integrating\" can be reduced to the stem **Integrat**.\n",
    "\n",
    "### The Porter Stemmer Algorithm\n",
    "\n",
    "The Porter Stemmer operates by applying a series of rules to strip suffixes from words. Here‚Äôs a brief overview of its functionality:\n",
    "\n",
    "**Step-wise Process**: The algorithm consists of multiple steps, each aimed at removing specific types of suffixes. The process can be categorized into five main phases, each focusing on a distinct set of morphological rules.\n",
    "\n",
    "**Suffix Removal**: It employs a set of predefined rules to eliminate common suffixes such as \"ing,\" \"ed,\" \"ly,\" etc. For example:\n",
    "\n",
    "- \"running\" ‚Üí \"run\"\n",
    "- \"happily\" ‚Üí \"happi\"\n",
    "- \"better\" ‚Üí \"better\" (the word is left unchanged as it's already a stem)\n",
    "\n",
    "### Consonant-Vowel Rules\n",
    "\n",
    "A consonant is defined as any letter that is not a vowel or the letter **Y** when preceded by a consonant. For instance, in the word **TOY**, the consonants are **T** and **Y**, while in **SYZYGY**, they are **S**, **Z**, and **G**. \n",
    "\n",
    "If a letter is not a consonant, it is classified as a vowel.\n",
    "\n",
    "**Notation**:\n",
    "- A consonant is denoted by **c** and a vowel by **v**.\n",
    "- A sequence of one or more consecutive consonants is denoted by **C**, and a sequence of one or more consecutive vowels is denoted by **V**. \n",
    "\n",
    "Thus, any word or part of a word can be represented in one of the following forms:\n",
    "\n",
    "- CVCV ‚Ä¶ C ‚Üí e.g., collection, management\n",
    "- CVCV ‚Ä¶ V ‚Üí e.g., conclude, revise\n",
    "- VCVC ‚Ä¶ C ‚Üí e.g., entertainment, illumination\n",
    "- VCVC ‚Ä¶ V ‚Üí e.g., illustrate, abundance\n",
    "\n",
    "All of these forms can be succinctly represented as:\n",
    "\n",
    "{C\\}VCVC ... \\{V\\}\n",
    "\n",
    "Here, the brackets (`{}`) indicate the arbitrary presence of consonants or vowels.\n",
    "\n",
    "**Measure of the Word (m)**: \n",
    "\n",
    "The value **m** found in the above expression is referred to as the measure of any word or word part when represented in the form \\([C](VC)^m[V]\\). Examples for different values of **m** include:\n",
    "\n",
    "- **m=0**   ‚Üí   TREE, TR, EE, Y, BY\n",
    "- **m=1**   ‚Üí   TROUBLE, OATS, TREES, IVY\n",
    "- **m=2**   ‚Üí   TROUBLES, PRIVATE, OATEN, ROBBERY\n",
    "\n",
    "<p align=\"center\"><img src=\"https://vijinimallawaarachchi.com/wp-content/uploads/2017/05/stemmer1.png\" alt=\"Stemmer\" width=\"50%\" height=\"10%\" style=\"display: block; margin: 20px auto;\"/></p>\n",
    "\n",
    "### Rules for Suffix Replacement\n",
    "\n",
    "The rules for replacing (or removing) a suffix are articulated in the following formal structure:\n",
    "\n",
    "`(condition) S1 ‚Üí S2`\n",
    "\n",
    "\n",
    "Where:\n",
    "- **S1** represents the suffix subject to replacement or removal.\n",
    "- **S2** denotes the new suffix (which may also be null, indicating complete removal).\n",
    "\n",
    "#### Example Rules\n",
    "1. **(ends with \"ed\" or \"ing\" and has a root word)**: S1 ‚Üí S2  \n",
    "2. **(ends with \"y\" preceded by a consonant)**: S1 ‚Üí S2  \n",
    "3. **(ends with \"ational\", \"tional\", \"enci\", or \"anci\")**: S1 ‚Üí S2  \n",
    "4. **(ends with \"icate\", \"ative\", or \"alize\")**: S1 ‚Üí S2  \n",
    "5. **(ends with \"ment\", \"ness\", or \"ity\")**: S1 ‚Üí S2  \n",
    "6. **(specific irregular forms)**: S1 ‚Üí S2\n",
    "\n",
    "This means that if a word ends with the suffix **S1**, and the stem before **S1** satisfies the given condition, **S1** is replaced by **S2**. The condition is generally expressed in terms of **m** concerning the stem preceding **S1**.\n",
    "\n",
    "**Example**:\n",
    "\\[\n",
    "(m > 1) \\, EMENT \\rightarrow \\text{(S2 is null)}\n",
    "\\]\n",
    "In this instance, **S1** is ‚ÄòEMENT‚Äô and **S2** is null. This would transform **REPLACEMENT** to **REPLAC**, since **REPLAC** is a word part for which **m = 2**.\n",
    "\n",
    "### Conditions\n",
    "\n",
    "The conditions may incorporate the following components:\n",
    "\n",
    "- **S**: the stem ends with S (and similarly for other letters)\n",
    "- **v**: the stem contains a vowel\n",
    "- **d**: the stem ends with a double consonant (e.g., -TT, -SS)\n",
    "- **o**: the stem ends in a CVC pattern, where the second consonant is not W, X, or Y (e.g., -WIL, -HOP)\n",
    "\n",
    "The condition part may also feature expressions using **and**, **or**, and **not**.\n",
    "\n",
    "Examples of Conditions:\n",
    "- **(m > 1 and (*S or *T))**: tests for a stem with **m > 1** ending in **S** or **T**.\n",
    "- **(*d and not (*L or *S or *Z))**: tests for a stem ending with a double consonant, excluding endings with letters **L**, **S**, or **Z**.\n",
    "\n",
    "### How Rules Are Applied\n",
    "\n",
    "In a set of rules written sequentially, only one rule is obeyed, specifically the one with the longest matching **S1** for the given word. For instance, consider the following rules:\n",
    "\n",
    "`SSES ‚Üí SS IES ‚Üí I SS ‚Üí SS S ‚Üí`\n",
    "\n",
    "\n",
    "Here, all conditions are null. The word **CARESSES** maps to **CARESS**, as **SSES** is the longest match for **S1**. Similarly, **CARESS** maps to **CARESS** (since **S1 = \"SS\"**) and **CARES** maps to **CARE** (since **S1 = \"S\"**).\n",
    "\n",
    "### Advantages of Using Porter Stemmer\n",
    "\n",
    "- **Simplicity**: The algorithm is straightforward to implement and does not require extensive computational resources.\n",
    "- **Efficiency**: It significantly reduces the size of the vocabulary, allowing models to run faster and consume less memory.\n",
    "- **Performance**: In many cases, stemming can lead to improved performance in tasks such as search and classification by grouping related words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of Stemmed Words: ['mypi', 'mypyc', 'licens', 'term', 'mit', 'licens', 'reproduc', 'mit', 'licens', 'copyright', 'c', 'jukka', 'lehtosalo', 'contributor', 'copyright', 'c', 'dropbox', 'inc', 'permiss', 'herebi', 'grant', 'free', 'charg', 'person', 'obtain', 'copi', 'softwar', 'associ', 'document', 'file', 'softwar', 'deal', 'softwar', 'without', 'restrict', 'includ', 'without', 'limit', 'right', 'use', 'copi', 'modifi', 'merg', 'publish', 'distribut', 'sublicens', 'sell', 'copi', 'softwar', 'permit', 'person', 'softwar', 'furnish', 'subject', 'follow', 'condit', 'copyright', 'notic', 'permiss', 'notic', 'shall', 'includ', 'copi', 'substanti', 'portion', 'softwar', 'softwar', 'provid', 'without', 'warranti', 'kind', 'express', 'impli', 'includ', 'limit', 'warranti', 'merchant', 'fit', 'particular', 'purpos', 'noninfring', 'event', 'shall', 'author', 'copyright', 'holder', 'liabl', 'claim', 'damag', 'liabil', 'whether', 'action', 'contract', 'tort', 'otherwis', 'aris', 'connect', 'softwar', 'use', 'deal', 'softwar', 'portion', 'mypi', 'mypyc', 'licens', 'differ', 'licens', 'file', 'licens', 'psf', 'licens', 'reproduc', 'python', 'softwar', 'foundat', 'licens', 'version', 'licens', 'agreement', 'python', 'softwar', 'foundat', 'psf', 'individu', 'organ', 'license', 'access', 'otherwis', 'use', 'softwar', 'python', 'sourc', 'binari', 'form', 'associ', 'document', 'subject', 'term', 'condit', 'licens', 'agreement', 'psf', 'herebi', 'grant', 'license', 'nonexclus', 'licens', 'reproduc', 'analyz', 'test', 'perform', 'display', 'publicli', 'prepar', 'deriv', 'work', 'distribut', 'otherwis', 'use', 'python', 'alon', 'deriv', 'version', 'provid', 'howev', 'psf', 'licens', 'agreement', 'psf', 'notic', 'copyright', 'copyright', 'c', 'python', 'softwar', 'foundat', 'right', 'reserv', 'retain', 'python', 'alon', 'deriv', 'version', 'prepar', 'license', 'event', 'license', 'prepar', 'deriv', 'work', 'base', 'incorpor', 'python', 'part', 'thereof', 'want', 'make', 'deriv', 'work', 'avail', 'other', 'provid', 'herein', 'license', 'herebi', 'agre', 'includ', 'work', 'brief', 'summari', 'chang', 'made', 'python', 'psf', 'make', 'python', 'avail', 'license', 'basi', 'psf', 'make', 'represent', 'warranti', 'express', 'impli', 'way', 'exampl', 'limit', 'psf', 'make', 'disclaim', 'represent', 'warranti', 'merchant', 'fit', 'particular', 'purpos', 'use', 'python', 'infring', 'third', 'parti', 'right', 'psf', 'shall', 'liabl', 'license', 'user', 'python', 'incident', 'special', 'consequenti', 'damag', 'loss', 'result', 'modifi', 'distribut', 'otherwis', 'use', 'python', 'deriv', 'thereof', 'even', 'advis', 'possibl', 'thereof', 'licens', 'agreement', 'automat', 'termin', 'upon', 'materi', 'breach', 'term', 'condit', 'noth', 'licens', 'agreement', 'shall', 'deem', 'creat', 'relationship', 'agenc', 'partnership', 'joint', 'ventur', 'psf', 'license', 'licens', 'agreement', 'grant', 'permiss', 'use', 'psf', 'trademark', 'trade', 'name', 'trademark', 'sens', 'endors', 'promot', 'product', 'servic', 'license', 'third', 'parti', 'copi', 'instal', 'otherwis', 'use', 'python', 'license', 'agre', 'bound', 'term', 'condit', 'licens', 'agreement', 'licens', 'agreement', 'python', 'beopen', 'python', 'open', 'sourc', 'licens', 'agreement', 'version', 'licens', 'agreement', 'beopen', 'offic', 'saratoga', 'avenu', 'santa', 'clara', 'ca', 'individu', 'organ', 'license', 'access', 'otherwis', 'use', 'softwar', 'sourc', 'binari', 'form', 'associ', 'document', 'softwar', 'subject', 'term', 'condit', 'beopen', 'python', 'licens', 'agreement', 'beopen', 'herebi', 'grant', 'license', 'licens', 'reproduc', 'analyz', 'test', 'perform', 'display', 'publicli', 'prepar', 'deriv', 'work', 'distribut', 'otherwis', 'use', 'softwar', 'alon', 'deriv', 'version', 'provid', 'howev', 'beopen', 'python', 'licens', 'retain', 'softwar', 'alon', 'deriv', 'version', 'prepar', 'license', 'beopen', 'make', 'softwar', 'avail', 'license', 'basi', 'beopen', 'make', 'represent', 'warranti', 'express', 'impli', 'way', 'exampl', 'limit', 'beopen', 'make', 'disclaim', 'represent', 'warranti', 'merchant', 'fit', 'particular', 'purpos', 'use', 'softwar', 'infring', 'third', 'parti', 'right', 'beopen', 'shall', 'liabl', 'license', 'user', 'softwar', 'incident', 'special', 'consequenti', 'damag', 'loss', 'result', 'use', 'modifi', 'distribut', 'softwar', 'deriv', 'thereof', 'even', 'advis', 'possibl', 'thereof', 'licens', 'agreement', 'automat', 'termin', 'upon', 'materi', 'breach', 'term', 'condit', 'licens', 'agreement', 'shall', 'govern', 'interpret', 'respect', 'law', 'state', 'california', 'exclud', 'conflict', 'law', 'provis', 'noth', 'licens', 'agreement', 'shall', 'deem', 'creat', 'relationship', 'agenc', 'partnership', 'joint', 'ventur', 'beopen', 'license', 'licens', 'agreement', 'grant', 'permiss', 'use', 'beopen', 'trademark', 'trade', 'name', 'trademark', 'sens', 'endors', 'promot', 'product', 'servic', 'license', 'third', 'parti', 'except', 'beopen', 'python', 'logo', 'avail', 'http', 'may', 'use', 'accord', 'permiss', 'grant', 'web', 'page', 'copi', 'instal', 'otherwis', 'use', 'softwar', 'license', 'agre', 'bound', 'term', 'condit', 'licens', 'agreement', 'cnri', 'licens', 'agreement', 'python', 'licens', 'agreement', 'corpor', 'nation', 'research', 'initi', 'offic', 'preston', 'white', 'drive', 'reston', 'va', 'cnri', 'individu', 'organ', 'license', 'access', 'otherwis', 'use', 'python', 'softwar', 'sourc', 'binari', 'form', 'associ', 'document', 'subject', 'term', 'condit', 'licens', 'agreement', 'cnri', 'herebi', 'grant', 'license', 'nonexclus', 'licens', 'reproduc', 'analyz', 'test', 'perform', 'display', 'publicli', 'prepar', 'deriv', 'work', 'distribut', 'otherwis', 'use', 'python', 'alon', 'deriv', 'version', 'provid', 'howev', 'licens', 'agreement', 'cnri', 'notic', 'copyright', 'copyright', 'c', 'corpor', 'nation', 'research', 'initi', 'right', 'reserv', 'retain', 'python', 'alon', 'deriv', 'version', 'prepar', 'license', 'altern', 'lieu', 'cnri', 'licens', 'agreement', 'license', 'may', 'substitut', 'follow', 'text', 'omit', 'quot', 'python', 'made', 'avail', 'subject', 'term', 'condit', 'cnri', 'licens', 'agreement', 'agreement', 'togeth', 'python', 'may', 'locat', 'internet', 'use', 'follow', 'uniqu', 'persist', 'identifi', 'known', 'handl', 'agreement', 'may', 'also', 'obtain', 'proxi', 'server', 'internet', 'use', 'follow', 'url', 'http', 'event', 'license', 'prepar', 'deriv', 'work', 'base', 'incorpor', 'python', 'part', 'thereof', 'want', 'make', 'deriv', 'work', 'avail', 'other', 'provid', 'herein', 'license', 'herebi', 'agre', 'includ', 'work', 'brief', 'summari', 'chang', 'made', 'python', 'cnri', 'make', 'python', 'avail', 'license', 'basi', 'cnri', 'make', 'represent', 'warranti', 'express', 'impli', 'way', 'exampl', 'limit', 'cnri', 'make', 'disclaim', 'represent', 'warranti', 'merchant', 'fit', 'particular', 'purpos', 'use', 'python', 'infring', 'third', 'parti', 'right', 'cnri', 'shall', 'liabl', 'license', 'user', 'python', 'incident', 'special', 'consequenti', 'damag', 'loss', 'result', 'modifi', 'distribut', 'otherwis', 'use', 'python', 'deriv', 'thereof', 'even', 'advis', 'possibl', 'thereof', 'licens', 'agreement', 'automat', 'termin', 'upon', 'materi', 'breach', 'term', 'condit', 'licens', 'agreement', 'shall', 'govern', 'feder', 'intellectu', 'properti', 'law', 'unit', 'state', 'includ', 'without', 'limit', 'feder', 'copyright', 'law', 'extent', 'feder', 'law', 'appli', 'law', 'commonwealth', 'virginia', 'exclud', 'virginia', 'conflict', 'law', 'provis', 'notwithstand', 'forego', 'regard', 'deriv', 'work', 'base', 'python', 'incorpor', 'materi', 'previous', 'distribut', 'gnu', 'gener', 'public', 'licens', 'gpl', 'law', 'commonwealth', 'virginia', 'shall', 'govern', 'licens', 'agreement', 'issu', 'aris', 'respect', 'paragraph', 'licens', 'agreement', 'noth', 'licens', 'agreement', 'shall', 'deem', 'creat', 'relationship', 'agenc', 'partnership', 'joint', 'ventur', 'cnri', 'license', 'licens', 'agreement', 'grant', 'permiss', 'use', 'cnri', 'trademark', 'trade', 'name', 'trademark', 'sens', 'endors', 'promot', 'product', 'servic', 'license', 'third', 'parti', 'click', 'accept', 'button', 'indic', 'copi', 'instal', 'otherwis', 'use', 'python', 'license', 'agre', 'bound', 'term', 'condit', 'licens', 'agreement', 'accept', 'cwi', 'licens', 'agreement', 'python', 'copyright', 'c', 'sticht', 'mathematisch', 'centrum', 'amsterdam', 'netherland', 'right', 'reserv', 'permiss', 'use', 'copi', 'modifi', 'distribut', 'softwar', 'document', 'purpos', 'without', 'fee', 'herebi', 'grant', 'provid', 'copyright', 'notic', 'appear', 'copi', 'copyright', 'notic', 'permiss', 'notic', 'appear', 'support', 'document', 'name', 'sticht', 'mathematisch', 'centrum', 'cwi', 'use', 'advertis', 'public', 'pertain', 'distribut', 'softwar', 'without', 'specif', 'written', 'prior', 'permiss', 'sticht', 'mathematisch', 'centrum', 'disclaim', 'warranti', 'regard', 'softwar', 'includ', 'impli', 'warranti', 'merchant', 'fit', 'event', 'shall', 'sticht', 'mathematisch', 'centrum', 'liabl', 'special', 'indirect', 'consequenti', 'damag', 'damag', 'whatsoev', 'result', 'loss', 'use', 'data', 'profit', 'whether', 'action', 'contract', 'neglig', 'tortiou', 'action', 'aris', 'connect', 'use', 'perform', 'softwar']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/p/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/p/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure NLTK resources are available\n",
    "nltk.download(info_or_id= 'punkt')\n",
    "nltk.download(info_or_id= 'stopwords')\n",
    "# Function to process the corpus in chunks\n",
    "def process_corpus(file_path, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Processes a large text file in chunks, tokenizing, stemming,\n",
    "    and removing stop words from each chunk.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the text file.\n",
    "        chunk_size (int): The number of lines to process at once.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of stemmed words from the entire corpus,\n",
    "              with stop words removed.\n",
    "    \"\"\"\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))  # Set of English stop words\n",
    "    all_stemmed_words = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        while True:\n",
    "            # Read a chunk of lines from the file\n",
    "            lines = [file.readline() for _ in range(chunk_size)]\n",
    "            if not lines or all(line == '' for line in lines):\n",
    "                break\n",
    "            \n",
    "            # Combine the lines into a single string\n",
    "            corpus_chunk = ' '.join(lines)\n",
    "\n",
    "            # Tokenize the text into words\n",
    "            tokens = word_tokenize(corpus_chunk)\n",
    "\n",
    "            # Remove stop words, punctuation, and perform stemming on each token\n",
    "            stemmed_words = [\n",
    "                porter_stemmer.stem(token) \n",
    "                for token in tokens \n",
    "                if token.lower() not in stop_words and token.isalpha()  # Check if token is alphabetic\n",
    "            ]\n",
    "\n",
    "            # Extend the list of all stemmed words\n",
    "            all_stemmed_words.extend(stemmed_words)\n",
    "\n",
    "    return all_stemmed_words\n",
    "\n",
    "# Example usage\n",
    "file_path = './LICENSE'  # Replace with your large corpus file path\n",
    "stemmed_words = process_corpus(file_path)\n",
    "\n",
    "# Display a sample of the stemmed words\n",
    "print(\"Sample of Stemmed Words:\", stemmed_words)  # Show first 20 stemmed words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages of Using Porter Stemmer:\n",
    "\n",
    "**Over-Stemming:** The Porter Stemmer may lead to excessive reduction, where different words that should not be grouped together are erroneously stemmed to the same form. For example, \"university\" and \"universal\" may both be reduced to \"univer.\"\n",
    "\n",
    "**Loss of Meaning:** Since stemming focuses on morphological similarity rather than semantic meaning, it can sometimes result in a loss of contextual information.\n",
    "\n",
    "**Language Dependence:** The rules are tailored specifically for the English language and may not work well for other languages with different morphological structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Closing Thoughts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
