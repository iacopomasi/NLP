{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part_1_5_Words_Corpora_Text_Normalization\n",
    "In Natural Language Processing (NLP), key techniques such as tokenization, stemming, and sentence segmentation are fundamental for transforming raw text into a structured format that can be effectively analyzed by language models. Byte-Pair Encoding (BPE) tokenization helps handle rare words and out-of-vocabulary terms by breaking text into subword units. Stemming, using algorithms like the Porter Stemmer, reduces words to their root forms, ensuring consistency across the text for better analysis. Sentence segmentation is crucial for dividing text into meaningful sentences, allowing for more accurate processing in downstream tasks. These techniques play a vital role in preparing text for various NLP tasks, ensuring that language data is in a normalized and analyzable state.\n",
    "\n",
    "### **Objectives:**\n",
    "By the end of this notebook, Parham will have a thorough understanding of tokenization and its importance in NLP, specifically learning how to implement **Byte-Pair Encoding (BPE)** to handle words and subwords. He will explore the **Porter Stemmer**, gaining insight into how it reduces words to their base forms and why this is essential for text normalization. Additionally, Parham will learn to apply **Sentence Segmentation** to split text into meaningful sentences for deeper analysis. Through hands-on coding exercises, he will gain practical experience using these techniques, utilizing Python libraries like `NLTK` and `SpaCy` to prepare text for NLP tasks.\n",
    "\n",
    "**Table of Contents:** \n",
    "1. Import Libraries\n",
    "2. Tokenization Techniques\n",
    "3. Text Normalization: Porter Stemmer\n",
    "4. Sentence Segmentation\n",
    "5. Closing Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "from loguru import logger\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization Techniques\n",
    "Byte-pair encoding was first introduced in 1994 as a simple data compression technique by iteratively replacing the most frequent pair of bytes in a sequence with a single, unused byte.\n",
    "Imagine Parham is reading a really big book, but some of the words are really long or tricky, and he might not know all of them. To make things easier, Parham can break the long words into smaller, simpler parts or pieces. This way, he can still understand the book without needing to know every single big word.\n",
    "\n",
    "Byte-Pair Encoding (BPE) is a popular technique used in natural language processing for tokenizing text into subword units. The idea is to break down words into smaller, more frequent pieces, allowing models to efficiently handle rare or unknown words. BPE is especially useful in scenarios where the vocabulary is limited but the text contains a large variety of words, including compound or out-of-vocabulary words.\n",
    "\n",
    "The algorithm works by:\n",
    "- Starting with a sequence of individual characters.\n",
    "- Finding the most frequent pair of consecutive characters (or subwords).\n",
    "- Merging this pair into a single token.\n",
    "- Repeating the process until a predefined number of merges or a desired vocabulary size is reached.\n",
    "\n",
    "This process allows models to represent both common words and subwords effectively, making it easier to process any text.\n",
    "\n",
    "In the cell below Parham will an example of BEP tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After merge 1:\n",
      "Best Pair: ('s', '</w>')\n",
      "Updated Vocabulary: {'T o k e n i z a t i o n </w>': 2, 'i s</w>': 2, 't h e </w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k i n g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u e n c e </w>': 1, 't e x t </w>': 2, 'i n t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k e n s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e </w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v e n </w>': 1, 'i n d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t e n </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'i n </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s i n g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n , </w>': 1, 'n a m e d </w>': 1, 'e n t i t y </w>': 1, 'r e c o g n i t i o n , </w>': 1, 'a n d </w>': 1, 's e n t i m e n t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e </w>': 1, 'r e s u l t i n g </w>': 1, 't o k e n s</w>': 2, 'a r e </w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'i n p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a t i o n , </w>': 1, 'w h e r e </w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s e n t a t i o n s</w>': 1, 'f o r </w>': 1, 'm a c h i n e </w>': 1, 'l e a r n i n g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 2:\n",
      "Best Pair: ('e', 'n')\n",
      "Updated Vocabulary: {'T o k en i z a t i o n </w>': 2, 'i s</w>': 2, 't h e </w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k i n g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e </w>': 1, 't e x t </w>': 2, 'i n t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e </w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'i n d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'i n </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s i n g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n , </w>': 1, 'n a m e d </w>': 1, 'en t i t y </w>': 1, 'r e c o g n i t i o n , </w>': 1, 'a n d </w>': 1, 's en t i m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e </w>': 1, 'r e s u l t i n g </w>': 1, 't o k en s</w>': 2, 'a r e </w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'i n p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a t i o n , </w>': 1, 'w h e r e </w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a t i o n s</w>': 1, 'f o r </w>': 1, 'm a c h i n e </w>': 1, 'l e a r n i n g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 3:\n",
      "Best Pair: ('i', 'n')\n",
      "Updated Vocabulary: {'T o k en i z a t i o n </w>': 2, 'i s</w>': 2, 't h e </w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e </w>': 1, 't e x t </w>': 2, 'in t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e </w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n , </w>': 1, 'n a m e d </w>': 1, 'en t i t y </w>': 1, 'r e c o g n i t i o n , </w>': 1, 'a n d </w>': 1, 's en t i m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e </w>': 1, 'r e s u l t in g </w>': 1, 't o k en s</w>': 2, 'a r e </w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a t i o n , </w>': 1, 'w h e r e </w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a t i o n s</w>': 1, 'f o r </w>': 1, 'm a c h in e </w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 4:\n",
      "Best Pair: ('e', '</w>')\n",
      "Updated Vocabulary: {'T o k en i z a t i o n </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n , </w>': 1, 'n a m e d </w>': 1, 'en t i t y </w>': 1, 'r e c o g n i t i o n , </w>': 1, 'a n d </w>': 1, 's en t i m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 't o k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a t i o n , </w>': 1, 'w h e r e</w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a t i o n s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 5:\n",
      "Best Pair: ('t', 'i')\n",
      "Updated Vocabulary: {'T o k en i z a ti o n </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti o n , </w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti o n , </w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 't o k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a ti o n , </w>': 1, 'w h e r e</w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a ti o n s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 6:\n",
      "Best Pair: ('t', 'o')\n",
      "Updated Vocabulary: {'T o k en i z a ti o n </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 'to k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti o n , </w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti o n , </w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c to r i z a ti o n , </w>': 1, 'w h e r e</w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a ti o n s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 7:\n",
      "Best Pair: ('o', 'n')\n",
      "Updated Vocabulary: {'T o k en i z a ti on </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 'to k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti on , </w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti on , </w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c to r i z a ti on , </w>': 1, 'w h e r e</w>': 1, 'c on v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a ti on s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 8:\n",
      "Best Pair: ('a', 'l')\n",
      "Updated Vocabulary: {'T o k en i z a ti on </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m al l e r </w>': 1, 'u n i t s</w>': 1, 'c al l e d </w>': 1, 'to k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u al </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r al </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti on , </w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti on , </w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n al y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c al l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c to r i z a ti on , </w>': 1, 'w h e r e</w>': 1, 'c on v e r t e d </w>': 1, 'n u m e r i c al </w>': 1, 'r e p r e s en t a ti on s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 9:\n",
      "Best Pair: (',', '</w>')\n",
      "Updated Vocabulary: {'T o k en i z a ti on </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m al l e r </w>': 1, 'u n i t s</w>': 1, 'c al l e d </w>': 1, 'to k en s ,</w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s ,</w>': 1, 'p h r a s e s ,</w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u al </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r al </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti on ,</w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti on ,</w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n al y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c al l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s ,</w>': 1, 'v e c to r i z a ti on ,</w>': 1, 'w h e r e</w>': 1, 'c on v e r t e d </w>': 1, 'n u m e r i c al </w>': 1, 'r e p r e s en t a ti on s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 10:\n",
      "Best Pair: ('ti', 'on')\n",
      "Updated Vocabulary: {'T o k en i z a tion </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m al l e r </w>': 1, 'u n i t s</w>': 1, 'c al l e d </w>': 1, 'to k en s ,</w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s ,</w>': 1, 'p h r a s e s ,</w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u al </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r al </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a tion ,</w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i tion ,</w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n al y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c al l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s ,</w>': 1, 'v e c to r i z a tion ,</w>': 1, 'w h e r e</w>': 1, 'c on v e r t e d </w>': 1, 'n u m e r i c al </w>': 1, 'r e p r e s en t a tion s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    Given a vocabulary (dictionary mapping words to frequency counts), returns a \n",
    "    dictionary of tuples representing the frequency count of pairs of characters \n",
    "    in the vocabulary.\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"\n",
    "    Given a pair of characters and a vocabulary, returns a new vocabulary with the \n",
    "    pair of characters merged together wherever they appear.\n",
    "    \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_vocab(data):\n",
    "    \"\"\"\n",
    "    Given a list of strings, returns a dictionary of words mapping to their frequency \n",
    "    count in the data.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(int)\n",
    "    for line in data:\n",
    "        for word in line.split():\n",
    "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "def byte_pair_encoding(data, n):\n",
    "    \"\"\"\n",
    "    Given a list of strings and an integer n, returns a list of n merged pairs\n",
    "    of characters found in the vocabulary of the input data.\n",
    "    \"\"\"\n",
    "    vocab = get_vocab(data)\n",
    "    for i in range(n):\n",
    "        pairs = get_stats(vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        print(f\"\\nAfter merge {i + 1}:\")\n",
    "        print(f\"Best Pair: {best}\")\n",
    "        print(f\"Updated Vocabulary: {vocab}\")\n",
    "    return vocab\n",
    "\n",
    "# Example usage:\n",
    "corpus = '''Tokenization is the process of breaking down \n",
    "a sequence of text into smaller units called tokens,\n",
    "which can be words, phrases, or even individual characters.\n",
    "Tokenization is often the first step in natural languages processing tasks \n",
    "such as text classification, named entity recognition, and sentiment analysis.\n",
    "The resulting tokens are typically used as input to further processing steps,\n",
    "such as vectorization, where the tokens are converted\n",
    "into numerical representations for machine learning models to use.'''\n",
    "data = corpus.split(' ')\n",
    "\n",
    "n = 10\n",
    "bpe_pairs = byte_pair_encoding(data, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    " The algorithm for Byte-Pair Encoding (BPE) in the previous example is a simplified version for educational purposes. While it illustrates the core concept of BPE, it might not be efficient for processing large datasets or in production environments. Here are some reasons why the naive implementation could be problematic with big data.\n",
    " 1. **Computational Complexity Counting Pairs:** The algorithm counts pairs of characters for every iteration, which can become computationally expensive as the number of tokens increases. For large datasets, this might lead to significant slowdowns.\n",
    "Repeated Iteration: The naive implementation involves multiple passes over the data to find the most frequent pairs, which can be inefficient when working with massive corpora.\n",
    "2. **Memory Usage\n",
    "Storage of Vocabulary:** The implementation keeps an entire vocabulary in memory, which can become unmanageable with large datasets. Each unique token needs to be stored, and the size of this vocabulary can grow quickly.\n",
    "3. **Inefficiency in Merging\n",
    "String Manipulation:** Frequent string replacements can be slow in Python. The current method creates new strings for each merge operation, leading to increased overhead.\n",
    "4. **Scalability Issues\n",
    "Lack of Parallelism:** The algorithm does not leverage parallel processing. For big data, parallelizing tasks can significantly reduce processing time.\n",
    "More Efficient Approaches\n",
    "For working with large datasets, here are a few alternative approaches:\n",
    "\n",
    "**Optimized Libraries:**\n",
    "\n",
    "  Use optimized libraries like `sentencepiece` or `subword-nmt`, which are designed for BPE and can handle larger datasets more efficiently. These libraries implement BPE in a way that‚Äôs optimized for speed and memory usage.\n",
    "\n",
    "- **Stream Processing**:\n",
    "\n",
    "  Instead of loading the entire dataset into memory, consider processing it in chunks or streams. This way, you can update your vocabulary and statistics without needing to load everything at once.\n",
    "- **Use Hash Maps for Counting**:\n",
    "\n",
    "  Instead of using lists or arrays, using hash maps (dictionaries) can improve the efficiency of counting pairs and merging them.\n",
    "\n",
    "As an excersise, please implement BPE using `sentencepiece` using the corpus provided in the previous cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/p/miniconda3/envs/nlp/lib/python3.10/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here\n",
    "\n",
    "! pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "with open(\"input.txt\",\"w+\") as f:\n",
    "  f.write(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: ['‚ñÅT', 'oken', 'ization', '‚ñÅ', 'is', '‚ñÅ', 'the', '‚ñÅprocess', '‚ñÅof', '‚ñÅb', 're', 'a', 'k', 'i', 'ng', '‚ñÅ', 'd', 'o', 'w', 'n', '‚ñÅa', '‚ñÅse', 'q', 'u', 'en', 'ce', '‚ñÅof', '‚ñÅ', 'te', 'x', 't', '‚ñÅin', 'to', '‚ñÅs', 'm', 'alle', 'r', '‚ñÅ', 'u', 'nit', 's', '‚ñÅ', 'call', 'ed', '‚ñÅtokens', ',', '‚ñÅwh', 'i', 'ch', '‚ñÅca', 'n', '‚ñÅb', 'e', '‚ñÅw', 'or', 'd', 's', ',', '‚ñÅp', 'h', 'r', 'as', 'es', ',', '‚ñÅ', 'or', '‚ñÅ', 'e', 've', 'n', '‚ñÅin', 'd', 'i', 'v', 'i', 'd', 'u', 'al', '‚ñÅ', 'ch', 'ar', 'ac', 'te', 'rs', '.', '‚ñÅT', 'oken', 'ization', '‚ñÅ', 'is', '‚ñÅof', 't', 'en', '‚ñÅ', 'the', '‚ñÅ', 'fi', 'rs', 't', '‚ñÅstep', '‚ñÅin', '‚ñÅna', 't', 'ur', 'al', '‚ñÅ', 'la', 'ng', 'ua', 'g', 'es', '‚ñÅprocess', 'i', 'ng', '‚ñÅt', 'as', 'k', 's', '‚ñÅ', 'su', 'ch', '‚ñÅ', 'as', '‚ñÅ', 'te', 'x', 't', '‚ñÅc', 'la', 'ssi', 'fi', 'c', 'ation', ',', '‚ñÅna', 'me', 'd', '‚ñÅ', 'enti', 'ty', '‚ñÅre', 'co', 'g', 'ni', 'tion', ',', '‚ñÅan', 'd', '‚ñÅs', 'enti', 'me', 'n', 't', '‚ñÅan', 'a', 'ly', 's', 'is', '.', '‚ñÅT', 'he', '‚ñÅre', 'su', 'l', 'ti', 'ng', '‚ñÅtokens', '‚ñÅa', 're', '‚ñÅ', 'ty', 'p', 'ical', 'ly', '‚ñÅuse', 'd', '‚ñÅ', 'as', '‚ñÅin', 'p', 'u', 't', '‚ñÅto', '‚ñÅf', 'ur', 'the', 'r', '‚ñÅprocess', 'i', 'ng', '‚ñÅstep', 's', ',', '‚ñÅ', 'su', 'ch', '‚ñÅ', 'as', '‚ñÅ', 've', 'ct', 'or', 'ization', ',', '‚ñÅwh', 'e', 're', '‚ñÅ', 'the', '‚ñÅtokens', '‚ñÅa', 're', '‚ñÅc', 'on', 've', 'r', 'te', 'd', '‚ñÅin', 'to', '‚ñÅ', 'n', 'u', 'me', 'r', 'ical', '‚ñÅre', 'p', 're', 'sent', 'ation', 's', '‚ñÅf', 'or', '‚ñÅ', 'ma', 'ch', 'in', 'e', '‚ñÅ', 'l', 'e', 'ar', 'ni', 'ng', '‚ñÅm', 'o', 'd', 'e', 'l', 's', '‚ñÅto', '‚ñÅuse', '.']\n",
      "Decoded: Tokenization is the process of breaking down a sequence of text into smaller units called tokens, which can be words, phrases, or even individual characters. Tokenization is often the first step in natural languages processing tasks such as text classification, named entity recognition, and sentiment analysis. The resulting tokens are typically used as input to further processing steps, such as vectorization, where the tokens are converted into numerical representations for machine learning models to use.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: input.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 100\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: input.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 8 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=511\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=29\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 8 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=250\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 153 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 8\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 61\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 61 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=109 obj=15.6515 num_tokens=218 num_tokens/piece=2\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=99 obj=14.9491 num_tokens=219 num_tokens/piece=2.21212\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "# @title üëÄ Solution\n",
    "\n",
    "# Train a SentencePiece model\n",
    "spm.SentencePieceTrainer.train(input=\"input.txt\", model_prefix='m', vocab_size=100)\n",
    "\n",
    "# Load the model\n",
    "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
    "\n",
    "encoded = sp.encode(corpus, out_type=str)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "# Decode back to text\n",
    "decoded = sp.decode(encoded)\n",
    "print(\"Decoded:\", decoded)\n",
    "os.remove(\"input.txt\")\n",
    "os.remove(\"m.model\")\n",
    "os.remove(\"m.vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Normalization: Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Closing Thoughts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
