{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.di.uniroma1.it/sites/all/themes/sapienza_bootstrap/logo.png' width=\"200\"/>  \n",
    "\n",
    "# Part_1_8_Part_of_Speech_Tagging  \n",
    "\n",
    "In Natural Language Processing (`NLP`), tagging is a crucial process for annotating text with meaningful labels that aid in linguistic and semantic analysis. Among these, **Part-of-Speech (`POS`) tagging** plays a foundational role in identifying the grammatical roles of words in a sentence, such as noun, verb, adjective, or adverb. This understanding is critical for tasks like syntactic parsing, named entity recognition, machine translation, and text-to-speech systems.  \n",
    "\n",
    "`POS` tagging methods have evolved from rule-based systems to sophisticated algorithms like **Hidden Markov Models (`HMMs`)** and **Conditional Random Fields (CRFs)**, which leverage statistical properties for better contextual analysis. More recently, **neural network-based models** have introduced significant advancements, enabling state-of-the-art performance by leveraging word embeddings and deep learning architectures.  \n",
    "\n",
    "### **Objectives:**  \n",
    "In this notebook, Parham provides an overview of Part-of-Speech tagging, its significance in `NLP`, and the algorithms behind it, including Hidden Markov Models (`HMMs`) and neural networks. Through practical exercises, Parham will train a neural network for `POS` tagging and use `NLTK` to implement the Stanford `POS` Tagger.  \n",
    "\n",
    "### **References:**  \n",
    "- [https://www.nltk.org/book/ch05.html](https://www.nltk.org/book/ch05.html)  \n",
    "- [https://web.stanford.edu/~jurafsky/slp3/old_oct19/8.pdf](https://web.stanford.edu/~jurafsky/slp3/old_oct19/8.pdf)  \n",
    "- [https://www.linguisticsweb.org/doku.php?id=linguisticsweb:tutorials:linguistics_tutorials:automaticannotation:stanford_pos_tagger_python](https://www.linguisticsweb.org/doku.php?id=linguisticsweb:tutorials:linguistics_tutorials:automaticannotation:stanford_pos_tagger_python)  \n",
    "- [https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "- [https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/core-mathematics/pure-maths/matrices/eigenvalues-and-eigenvectors.html](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/core-mathematics/pure-maths/matrices/eigenvalues-and-eigenvectors.html)\n",
    "\n",
    "### **Contributors:**  \n",
    "- Parham Membari  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: p.membari96@gmail.com  \n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/p-mem/)  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ae/Github-desktop-logo-symbol.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **GitHub**: [GitHub](https://github.com/parham075)  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ec/Medium_logo_Monogram.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Medium**: [Medium](https://medium.com/@p.membari96)  \n",
    "\n",
    "**Table of Contents:**  \n",
    "1. Import Libraries\n",
    "2. Introduction to Tagging in NLP  \n",
    "3. Classical algorithms Behind `POS` Tagging (Rule-Based, HMM)  \n",
    "4. Fine tunning of a Neural Network for `POS` Tagging  \n",
    "5. Using NLTK to Handle Stanford POS Tagger  \n",
    "6. Closing Thoughts  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Tagging in NLP  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n Natural Language Processing (NLP), **tagging** involves assigning meaningful labels to elements of text, such as words, phrases, or sentences. These labels capture linguistic or semantic information that is essential for various NLP applications. For example:  \n",
    "- **Part-of-Speech (POS) Tagging:** Assigns grammatical roles (e.g., noun, verb, adjective).  \n",
    "\n",
    "- **Named Entity Recognition (NER):** Identifies proper nouns like names, locations, or organizations.  \n",
    "\n",
    "- **Semantic Role Labeling (SRL):** Describes the roles words play in the semantic structure of a sentence.  \n",
    "\n",
    "Each tagging approach serves a unique purpose, contributing to tasks like text parsing, translation, summarization, and information extraction. Techniques for tagging range from traditional rule-based systems to modern neural network-based methods:  \n",
    "- **Rule-Based Tagging:** Relies on linguistic rules and patterns. It works well for predictable structures but struggles with ambiguity and language variability.  \n",
    "- **Statistical Tagging:** Algorithms like Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) use probabilistic methods to predict tags based on contextual patterns in labeled data.  \n",
    "- **Neural Network-Based Tagging:** Leverages word embeddings and deep learning architectures like BiLSTMs and Transformers to achieve state-of-the-art performance by capturing complex patterns in language.  \n",
    "\n",
    "### 2.1. Part-of-Speech Tagging: A Closer Look  \n",
    "\n",
    "Among these approaches, **Part-of-Speech (POS) tagging** is a foundational task in NLP. It identifies the grammatical role of each word in a sentence, helping to structure raw text for downstream tasks. Consider the sentence:  \n",
    "\n",
    "_\"Computer Science department of Sapienza University of Rome is intellectually lively and reputed for its research outcome.\"_  \n",
    "\n",
    "POS tagging identifies:  \n",
    "- Computer      ‚Üí Proper Noun (NNP)  \n",
    "- Science       ‚Üí Proper Noun (NNP)  \n",
    "- department    ‚Üí Noun (NN)  \n",
    "- of            ‚Üí Preposition (IN)  \n",
    "- Sapienza      ‚Üí Proper Noun (NNP)  \n",
    "- University    ‚Üí Proper Noun (NNP)  \n",
    "- of            ‚Üí Preposition (IN)  \n",
    "- Rome          ‚Üí Proper Noun (NNP)  \n",
    "- is            ‚Üí Verb (VBZ)  \n",
    "- intellectually ‚Üí Adverb (RB)  \n",
    "- lively        ‚Üí Adjective (JJ)  \n",
    "- and           ‚Üí Coordinating Conjunction (CC)  \n",
    "- reputed       ‚Üí Verb, Past Participle (VBN)  \n",
    "- for           ‚Üí Preposition (IN)  \n",
    "- its           ‚Üí Possessive Pronoun (PRP$)  \n",
    "- research      ‚Üí Noun (NN)  \n",
    "- outcome       ‚Üí Noun (NN)  \n",
    "\n",
    "> Note: for more identifiers please check this [documentation](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "\n",
    "By providing information about grammatical structure, this tagging helps machines understand not just individual words, but also the connections between them within a sentence.\n",
    "\n",
    "### 2.2. Two classes of words: **Open** vs. **Closed**:\n",
    "- Closed class words\n",
    "    - Relatively fixed membership\n",
    "    - Usually function words: short, frequent words with grammatical function\n",
    "    - determiners: a, an, the\n",
    "    - pronouns: she, he, I\n",
    "    - prepositions: on, under, over, near, by, ‚Ä¶\n",
    "- Open class words\n",
    "    - Usually content words: Nouns, Verbs, Adjectives, Adverbs\n",
    "    - Plus interjections: oh, ouch, uh-huh, yes, hello\n",
    "    - New nouns and verbs like iPhone or to fax\n",
    "\n",
    "\n",
    "\n",
    "### 2.3. Why Part-of-Speech Tagging?  \n",
    "\n",
    "Here‚Äôs why POS tagging is so valuable:  \n",
    "\n",
    "- **Supports Other NLP Tasks**: POS tagging provides crucial insights for tasks like syntactic parsing, sentiment analysis, and text-to-speech systems.  \n",
    "- **Parsing**: Knowing POS tags can improve syntactic parsing accuracy, which is vital for machine translation and language understanding.  \n",
    "- **Machine Translation (MT)**: POS tags help reordering structures, such as adjectives and nouns, when translating between languages like Spanish and English.  \n",
    "- **Sentiment Analysis**: Distinguishing adjectives or verbs can reveal sentiment or emotional tone in text.  \n",
    "- **Text-to-Speech**: Pronunciation ambiguity, as seen with words like *lead* or *object*, can be resolved using POS tags.  \n",
    "- **Linguistic Analysis**: POS tagging aids in studying linguistic evolution, identifying meaning shifts, and creating new words.  \n",
    "\n",
    "In short, POS tagging acts as a bridge, enabling both practical NLP tasks and linguistic research to benefit from accurate syntactic understanding.  \n",
    "\n",
    "\n",
    "### 2.4. How Difficult is POS Tagging in English?  \n",
    "\n",
    "Although English `POS` tagging has achieved high accuracy, it is not without challenges. Ambiguity is a major issue:  \n",
    "\n",
    "- About **15% of word types** in English are ambiguous (e.g., *back* can be a noun, verb, adjective, or adverb).  \n",
    "- However, **85% of word types are unambiguous** (e.g., *Sapienza* is always a proper noun, and *intellectually* is always an adverb).  \n",
    "- The ambiguous 15% are highly frequent in text, meaning **~60% of word tokens** in actual usage are ambiguous.  \n",
    "\n",
    "Here are examples of how the word *back* varies based on context:  \n",
    "\n",
    "- **Adjective (ADJ)**: _Earnings growth took a **back** seat._  \n",
    "- **Noun (NOUN)**: _A small building in the **back**._  \n",
    "- **Verb (VERB)**: _A clear majority of senators **back** the bill._  \n",
    "- **Particle (PART)**: _Enable the country to buy **back** debt._  \n",
    "- **Adverb (ADV)**: _I was twenty-one **back** then._  \n",
    "\n",
    "\n",
    "### 2.5. POS Tagging Performance  \n",
    "\n",
    "How accurate is POS tagging? Modern methods have achieved impressive results:  \n",
    "\n",
    "- **Tagging Accuracy**: About **97%**, which hasn't changed much in the last decade. Hidden Markov Models (HMMs), Conditional Random Fields (CRFs), and neural network-based approaches like BERT perform similarly.  \n",
    "- **Baseline Accuracy**: Even a \"stupid\" baseline, such as tagging every word with its most frequent tag or unknown words as nouns, achieves **92%** accuracy.  \n",
    "\n",
    "The high accuracy is partly because many words are unambiguous. However, improving the remaining 3% can be difficult due to rare and ambiguous cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Classical Algorithms Behind `POS` Tagging (Rule-Based, HMM)**  \n",
    "\n",
    "Several algorithms are used to perform Part-of-Speech (POS) tagging, each with its strengths and ideal use cases. Let's explore three key approaches: **Rule-Based Tagging**, and **Statistical Methods (e.g., Hidden Markov Models)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1. Rule-Based POS Tagging**  \n",
    "\n",
    "This approach relies on a set of predefined linguistic rules and dictionaries:  \n",
    "\n",
    "- **How it Works**:  \n",
    "  - Uses lexicons (dictionaries) where words are tagged with their possible parts of speech.  \n",
    "  - Applies hand-crafted rules to disambiguate between possible tags based on word context.  \n",
    "  - For example:  \n",
    "    - If a word is preceded by a determiner like \"the,\" it is likely a noun.  \n",
    "    - If a word ends with \"ly,\" it is likely an adverb.  \n",
    "\n",
    "- **Advantages**:  \n",
    "  - Effective for languages with well-defined grammar rules.  \n",
    "  - Requires no training data.  \n",
    "\n",
    "- **Limitations**:  \n",
    "  - Hand-crafting rules is labor-intensive and language-specific.  \n",
    "  - Struggles with ambiguous words or phrases outside the rule set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2. Statistical Methods: Hidden Markov Models (HMM)**  \n",
    "\n",
    "HMMs are a probabilistic approach that leverages the likelihood of word sequences:  \n",
    "\n",
    "- **How it Works**:  \n",
    "  - Treats POS tagging as a sequence labeling problem.  \n",
    "  - Uses the probabilities of tags given a word (**emission probabilities**) and probabilities of transitioning from one tag to another (**transition probabilities**).  \n",
    "  - Finds the most likely sequence of tags using the **Viterbi algorithm**.  \n",
    "\n",
    "- **Advantages**:  \n",
    "  - Handles ambiguous words well by considering context.  \n",
    "  - Requires annotated training data but is more adaptable than rule-based methods.  \n",
    "\n",
    "- **Limitations**:  \n",
    "  - Assumes independence between words, which limits performance.  \n",
    "  - Simpler models compared to modern neural networks. \n",
    "  \n",
    "\n",
    "#### **3.3. Illustrating HMM with an Example: Markov Chain in a Restaurant**  \n",
    "\n",
    "Suppose there is a restaurant that serves only three dishes each day (Pizza üçï, Hamburger üçî, and Hotdog üå≠). We want to calculate the probability of the next dish served tomorrow. The following diagram represents the Markov chain for this restaurant, showing the transition probabilities between dishes:  \n",
    "\n",
    "<p align=\"center\"><img src=\"../imgs/restaurant_Markov.png\" alt=\"Markov Chain\" width=\"40%\" height=\"40%\" style=\"display: block; margin: 20px auto;\"/></p>  \n",
    "\n",
    "The diagram can be summarized as the following transition matrix `A`:  \n",
    "\n",
    "  ```python\n",
    "  A = [   \n",
    "      # üçî   üçï   üå≠\n",
    "      [0.2, 0.6, 0.2],  # üçî\n",
    "      [0.3, 0.0, 0.7],  # üçï\n",
    "      [0.5, 0.0, 0.5]   # üå≠\n",
    "  ]\n",
    "  ```\n",
    "\n",
    "- **Key Property of Markov Chains**:  \n",
    "  The future state depends only on the current state. Mathematically:  \n",
    "  $$\n",
    "  P(X_{n+1} = x | X_1 = x_1, ..., X_n = x_n) \\approx P(X_{n+1} = x | X_n = x_n)\n",
    "  $$  \n",
    "\n",
    "  Given the restaurant has served the following sequence of dishes:  \n",
    "  üçï ‚Üí üçî ‚Üí üçï ‚Üí ?  \n",
    "\n",
    "  The probabilities of serving each dish tomorrow are:  \n",
    "  $$ P(X_4 = üçï | X_3 = üçï) = 0.0 $$  \n",
    "  $$ P(X_4 = üçî | X_3 = üçï) = 0.3 $$  \n",
    "  $$ P(X_4 = üå≠ | X_3 = üçï) = 0.7 $$  \n",
    "\n",
    "  Thus, the highest probability is for a **Hotdog üå≠** to be served tomorrow.  \n",
    "\n",
    "\n",
    "#### **3.4. Long-Term Behavior of the Markov Chain**  \n",
    "\n",
    "Now consider a random walk of dishes over 10 steps:  \n",
    "üçï ‚Üí üå≠ ‚Üí üå≠ ‚Üí üçî ‚Üí üçï ‚Üí üçî ‚Üí üçî ‚Üí üçî ‚Üí üå≠ ‚Üí üçï ‚Üí ?  \n",
    "\n",
    "After 10 steps, the probabilities of each dish can be calculated as:  \n",
    "$$ P(üçï) = \\frac{\\text{Occurrences of üçï}}{\\text{Total steps}} = \\frac{3}{10} = 0.3 $$  \n",
    "$$ P(üçî) = \\frac{\\text{Occurrences of üçî}}{\\text{Total steps}} = \\frac{4}{10} = 0.4 $$  \n",
    "$$ P(üå≠) = \\frac{\\text{Occurrences of üå≠}}{\\text{Total steps}} = \\frac{3}{10} = 0.3 $$  \n",
    "\n",
    "Do these probabilities converge to specific values, or will they continue to fluctuate? Let's find out using a Python script.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-term probabilities:\n",
      "üçî: 0.342\n",
      "üçï: 0.212\n",
      "üå≠: 0.446\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Transition matrix\n",
    "A = np.array([\n",
    "    [0.2, 0.6, 0.2],  # üçî\n",
    "    [0.3, 0.0, 0.7],  # üçï\n",
    "    [0.5, 0.0, 0.5]   # üå≠\n",
    "])\n",
    "\n",
    "dishes = [\"üçî\", \"üçï\", \"üå≠\"]\n",
    "num_steps = 1000\n",
    "state = 1  # Start with üçï\n",
    "states = []\n",
    "\n",
    "for _ in range(num_steps):\n",
    "    states.append(state)\n",
    "    state = np.random.choice([0, 1, 2], p=A[state])\n",
    "\n",
    "# Calculate empirical probabilities\n",
    "counts = np.bincount(states, minlength=len(dishes))\n",
    "probabilities = counts / num_steps\n",
    "\n",
    "print(\"Long-term probabilities:\")\n",
    "for dish, prob in zip(dishes, probabilities):\n",
    "    print(f\"{dish}: {prob:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems they will converge to a certain point which calls stationary distribution or the equilibrium state. \n",
    ">To know more about why we have this behaviour, please take a look at this [documentation](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/core-mathematics/pure-maths/matrices/eigenvalues-and-eigenvectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≠: 0.334\n",
      "üçî: 0.333\n",
      "üçï: 0.333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Transition matrix\n",
    "A = np.array([\n",
    "    [0.2, 0.6, 0.2],  # üçî\n",
    "    [0.3, 0.0, 0.7],  # üçï\n",
    "    [0.5, 0.0, 0.5]   # üå≠\n",
    "])\n",
    "\n",
    "dishes = [\"üçî\", \"üçï\", \"üå≠\"]\n",
    "current_state = 1  # Start with üçï\n",
    "num_steps = 1000\n",
    "predictions = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Get probabilities for the next day\n",
    "    next_day_probs = A[current_state]\n",
    "    \n",
    "    # Predict the most likely state for tomorrow\n",
    "    predicted_state = np.argmax(next_day_probs)\n",
    "    predictions.append(dishes[predicted_state])\n",
    "    \n",
    "    # Update current state for the next iteration\n",
    "    current_state = predicted_state\n",
    "\n",
    "# Calculate frequencies of each dish\n",
    "from collections import Counter\n",
    "freq = Counter(predictions)\n",
    "for dish, count in freq.items():\n",
    "    print(f\"{dish}: {count / num_steps:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 1:**: Predicting the Sequence of Dishes Using Markov Chains**\n",
    "\n",
    "**Objective**  \n",
    "In this assignment, you will implement a Markov Chain model to simulate the prediction of dishes served in a restaurant over 50 days. You will calculate the sequence of probable dishes and their long-term probabilities. \n",
    "The restaurant serves three dishes: üçî (Hamburger), üçï (Pizza), and üå≠ (Hotdog). The transitions between these dishes are governed by the following Markov Chain transition matrix \\( A \\):\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "0.2 & 0.6 & 0.2 \\\\\n",
    "0.3 & 0.0 & 0.7 \\\\\n",
    "0.5 & 0.0 & 0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**Your Task**  \n",
    "\n",
    "1. **Simulate the Sequence**  \n",
    "   - Start with a given dish (e.g., üçï for Day 1).  \n",
    "   - Use the transition matrix to predict the dish for the next day, iterating for 50 days.  \n",
    "   - Print the sequence of dishes over the 50 days.\n",
    "\n",
    "2. **Calculate Probabilities**  \n",
    "   - At the end of the simulation, calculate the long-term probabilities of each dish:\n",
    "     $$\n",
    "     P(\\text{Dish}) = \\frac{\\text{Occurrences of Dish}}{\\text{Total Steps}}\n",
    "     $$\n",
    "   - Print the probabilities for üçî, üçï, and üå≠.\n",
    "\n",
    "**Additional hints and Guidelines**  \n",
    "\n",
    "1. **Transition Matrix**  \n",
    "   Use the matrix `A` to calculate the probabilities of transitioning from the current dish to each possible next dish.  \n",
    "\n",
    "2. **Choose the Next Dish**  \n",
    "   Use the probabilities from the transition matrix to determine the next dish. Obviously, it make sense if you choose the next dish based on the maximum transition probability.  \n",
    "\n",
    "3. **Implementation Steps**  \n",
    "   - Start with a given dish.  \n",
    "   - Use the row corresponding to the current dish in the transition matrix `A` to calculate probabilities for the next day.  \n",
    "   - Update the current dish and repeat the process for 50 days.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have: Hotdog üå≠ for the day 50th\n",
      "\n",
      "Sequence of Dishes Over 50 Days:\n",
      "\n",
      "Pizza üçï -> Hotdog üå≠ -> Hamburger üçî -> Pizza üçï -> Hotdog üå≠ ->\n",
      "Hamburger üçî -> Pizza üçï -> Hotdog üå≠ -> Hamburger üçî -> Pizza üçï ->\n",
      "Hotdog üå≠ -> Hamburger üçî -> Pizza üçï -> Hotdog üå≠ -> Hamburger üçî ->\n",
      "Pizza üçï -> Hotdog üå≠ -> Hamburger üçî -> Pizza üçï -> Hotdog üå≠ ->\n",
      "Hamburger üçî -> Pizza üçï -> Hotdog üå≠ -> Hamburger üçî -> Pizza üçï ->\n",
      "Hotdog üå≠ -> Hamburger üçî -> Pizza üçï -> Hotdog üå≠ -> Hamburger üçî ->\n",
      "Pizza üçï -> Hotdog üå≠ -> Hamburger üçî -> Pizza üçï -> Hotdog üå≠ ->\n",
      "Hamburger üçî -> Pizza üçï -> Hotdog üå≠ -> Hamburger üçî -> Pizza üçï ->\n",
      "Hotdog üå≠ -> Hamburger üçî -> Pizza üçï -> Hotdog üå≠ -> Hamburger üçî ->\n",
      "Pizza üçï -> Hotdog üå≠ -> Hamburger üçî -> Pizza üçï -> Hotdog üå≠ ->\n",
      "\n",
      "Probabilities After 50 Days:\n",
      "Hamburger üçî: 0.32\n",
      "Pizza üçï: 0.34\n",
      "Hotdog üå≠: 0.34\n"
     ]
    }
   ],
   "source": [
    "# @title üëÄ Solution\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Transition matrix A\n",
    "transition_matrix = np.array([\n",
    "    [0.2, 0.6, 0.2],  # Probabilities for Hamburger\n",
    "    [0.3, 0.0, 0.7],  # Probabilities for Pizza\n",
    "    [0.5, 0.0, 0.5]   # Probabilities for Hotdog\n",
    "])\n",
    "\n",
    "dishes = [\"Hamburger üçî\", \"Pizza üçï\", \"Hotdog üå≠\"]\n",
    "\n",
    "def simulate_markov_chain(initial_dish, days):\n",
    "    current_dish = dishes.index(initial_dish)\n",
    "    sequence = [initial_dish]\n",
    "    for _ in range(days - 1):\n",
    "        # Get the probabilities for the next dish based on the current dish\n",
    "        probabilities = transition_matrix[current_dish]\n",
    "        # Choose the next dish based on the maximum transition probability\n",
    "        next_dish = np.argmax(probabilities)\n",
    "        # Append the name of the next dish to the sequence\n",
    "        sequence.append(dishes[next_dish])\n",
    "        # Update the current dish to the index of the next dish\n",
    "        current_dish = next_dish\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def calculate_probabilities(sequence):\n",
    "    total_steps = len(sequence)\n",
    "    probabilities = {dish: sequence.count(dish) / total_steps for dish in dishes}\n",
    "    return probabilities\n",
    "\n",
    "# Simulate for 50 days\n",
    "initial_dish = \"Pizza üçï\"\n",
    "days = 50\n",
    "sequence = simulate_markov_chain(initial_dish, days)\n",
    "probabilities = calculate_probabilities(sequence)\n",
    "\n",
    "# Output results\n",
    "print(f\"We have: {sequence[-1]} for the day 50th\\n\")\n",
    "print(\"Sequence of Dishes Over 50 Days:\\n\")\n",
    "steps_to_show = 5\n",
    "for day in range(0, days, steps_to_show):\n",
    "    print(\" -> \".join(sequence[day:day+steps_to_show]), \"->\")\n",
    "\n",
    "print(\"\\nProbabilities After 50 Days:\")\n",
    "for dish, prob in probabilities.items():\n",
    "    print(f\"{dish}: {prob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine tunning of a Neural Network for `POS` Tagging  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using NLTK to Handle Stanford POS Tagger  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Closing Thoughts  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
